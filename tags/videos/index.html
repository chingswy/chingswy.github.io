<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>videos | Qing Shuai | 帅青</title> <meta name="author" content="Qing Shuai"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="chingswy.github.io/tags/videos/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Qing Shuai | 帅青</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <i class="fas fa-hashtag fa-sm"></i> videos </h1> <p class="post-description"> Papers with tag videos </p> </header> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="2210.03659" class="col-sm-8"> <div class="title">Spatio-temporal Tendency Reasoning for Human Body Pose and Shape Estimation from Videos</div> <div class="author"> Boyang Zhang, SuPing Wu, Hu Cao, Kehua Ma, Pan Li, and Lei Lin</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.03659.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>In this paper, we present a spatio-temporal tendency reasoning (STR) networkfor recovering human body pose and shape from videos. Previous approaches havefocused on how to extend 3D human datasets and temporal-based learning topromote accuracy and temporal smoothing. Different from them, our STR aims tolearn accurate and natural motion sequences in an unconstrained environmentthrough temporal and spatial tendency and to fully excavate the spatio-temporalfeatures of existing video data. To this end, our STR learns the representationof features in the temporal and spatial dimensions respectively, to concentrateon a more robust representation of spatio-temporal features. More specifically,for efficient temporal modeling, we first propose a temporal tendency reasoning(TTR) module. TTR constructs a time-dimensional hierarchical residualconnection representation within a video sequence to effectively reasontemporal sequences’ tendencies and retain effective dissemination of humaninformation. Meanwhile, for enhancing the spatial representation, we design aspatial tendency enhancing (STE) module to further learns to excite spatiallytime-frequency domain sensitive features in human motion informationrepresentations. Finally, we introduce integration strategies to integrate andrefine the spatio-temporal feature representations. Extensive experimentalfindings on large-scale publically available datasets reveal that our STRremains competitive with the state-of-the-art on three datasets. Our code areavailable at https://github.com/Changboyang/STR.git.</p> </div> <div class="qing hidden"> <p>使用了integration的策略来提升性能</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">2210.03659</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Spatio-temporal Tendency Reasoning for Human Body Pose and Shape Estimation from Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Boyang and Wu, SuPing and Cao, Hu and Ma, Kehua and Li, Pan and Lin, Lei}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{SMPL, videos}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/smpl"> <i class="fas fa-hashtag fa-sm"></i> SMPL</a>  <a href="/tags/videos"> <i class="fas fa-hashtag fa-sm"></i> videos</a>  </div> </div> </li></ol> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="Streaming_RF" class="col-sm-8"> <div class="title">Streaming Radiance Fields for 3D Video Synthesis</div> <div class="author"> Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping Tan</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.14831.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present an explicit-grid based method for efficiently reconstructingstreaming radiance fields for novel view synthesis of real world dynamicscenes. Instead of training a single model that combines all the frames, weformulate the dynamic modeling problem with an incremental learning paradigm inwhich per-frame model difference is trained to complement the adaption of abase model on the current frame. By exploiting the simple yet effective tuningstrategy with narrow bands, the proposed method realizes a feasible frameworkfor handling video sequences on-the-fly with high training efficiency. Thestorage overhead induced by using explicit grid representations can besignificantly reduced through the use of model difference based compression. Wealso introduce an efficient strategy to further accelerate model optimizationfor each frame. Experiments on challenging video sequences demonstrate that ourapproach is capable of achieving a training speed of 15 seconds per-frame withcompetitive rendering quality, which attains 1000 \times speedup over thestate-of-the-art implicit methods. Code is available athttps://github.com/AlgoHunt/StreamRF.</p> </div> <div class="sida hidden"> <p>提升训练速度. 提出一个explicit-grid based method, 将3d videos表示为a based model and per-frame model difference.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Streaming_RF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Streaming Radiance Fields for 3D Video Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Lingzhi and Shen, Zhen and Wang, Zhongshu and Shen, Li and Tan, Ping}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{neural-3d-videos}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{提升训练速度. 提出一个explicit-grid based method, 将3d videos表示为a based model and per-frame model difference.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/neural-3d-videos"> <i class="fas fa-hashtag fa-sm"></i> neural-3d-videos</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="NeRFPlayer" class="col-sm-8"> <div class="title">NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields</div> <div class="author"> Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and <a href="https://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Andreas Geiger</a> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.15947.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Visually exploring in a real-world 4D spatiotemporal space freely in VR hasbeen a long-term quest. The task is especially appealing when only a few oreven single RGB cameras are used for capturing the dynamic scene. To this end,we present an efficient framework capable of fast reconstruction, compactmodeling, and streamable rendering. First, we propose to decompose the 4Dspatiotemporal space according to temporal characteristics. Points in the 4Dspace are associated with probabilities of belonging to three categories:static, deforming, and new areas. Each area is represented and regularized by aseparate neural field. Second, we propose a hybrid representations basedfeature streaming scheme for efficiently modeling the neural fields. Ourapproach, coined NeRFPlayer, is evaluated on dynamic scenes captured by singlehand-held cameras and multi-camera arrays, achieving comparable or superiorrendering performance in terms of quality and speed comparable to recentstate-of-the-art methods, achieving reconstruction in 10 seconds per frame andreal-time rendering.</p> </div> <div class="sida hidden"> <p>用一个hybrid representation-based feature streaming scheme表示动态场景.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NeRFPlayer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Song, Liangchen and Chen, Anpei and Li, Zhong and Chen, Zhang and Chen, Lele and Yuan, Junsong and Xu, Yi and Geiger, Andreas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{neural-3d-videos}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{用一个hybrid representation-based  feature streaming scheme表示动态场景.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/neural-3d-videos"> <i class="fas fa-hashtag fa-sm"></i> neural-3d-videos</a>  </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2020</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2019</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2018</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2017</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Qing Shuai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </div></body> </html>