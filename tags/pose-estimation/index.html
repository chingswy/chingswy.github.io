<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>pose-estimation | Qing Shuai | 帅青</title> <meta name="author" content="Qing Shuai"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="chingswy.github.io/tags/pose-estimation/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Qing Shuai | 帅青</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <i class="fas fa-hashtag fa-sm"></i> pose-estimation </h1> <p class="post-description"> Papers with tag pose-estimation </p> </header> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="JRDB-Pose" class="col-sm-8"> <div class="title">JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking</div> <div class="author"> Edward Vendrow, Duy Tho Le, and Hamid Rezatofighi</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.11940.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Autonomous robotic systems operating in human environments must understandtheir surroundings to make accurate and safe decisions. In crowded human sceneswith close-up human-robot interaction and robot navigation, a deepunderstanding requires reasoning about human motion and body dynamics over timewith human body pose estimation and tracking. However, existing datasets eitherdo not provide pose annotations or include scene types unrelated to roboticapplications. Many datasets also lack the diversity of poses and occlusionsfound in crowded human scenes. To address this limitation we introduceJRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimationand tracking using videos captured from a social navigation robot. The datasetcontains challenge scenes with crowded indoor and outdoor locations and adiverse range of scales and occlusion types. JRDB-Pose provides human poseannotations with per-keypoint occlusion labels and track IDs consistent acrossthe scene. A public evaluation server is made available for fair evaluation ona held-out test set. JRDB-Pose is available at https://jrdb.erc.monash.edu/ .</p> </div> <div class="qing hidden"> <p>使用的是全景相机，而不是普通多视角相机</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">JRDB-Pose</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vendrow, Edward and Le, Duy Tho and Rezatofighi, Hamid}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{dataset, multi-person, pose-estimation, tracking}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/dataset"> <i class="fas fa-hashtag fa-sm"></i> dataset</a>  <a href="/tags/multi-person"> <i class="fas fa-hashtag fa-sm"></i> multi-person</a>  <a href="/tags/pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> pose-estimation</a>  <a href="/tags/tracking"> <i class="fas fa-hashtag fa-sm"></i> tracking</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="2210.15121" class="col-sm-8"> <div class="title">Bootstrapping Human Optical Flow and Pose</div> <div class="author"> Aritro Roy Arko, James J. Little, and Kwang Moo Yi</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.15121.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose a bootstrapping framework to enhance human optical flow and pose.We show that, for videos involving humans in scenes, we can improve both theoptical flow and the pose estimation quality of humans by considering the twotasks at the same time. We enhance optical flow estimates by fine-tuning themto fit the human pose estimates and vice versa. In more detail, we optimize thepose and optical flow networks to, at inference time, agree with each other. Weshow that this results in state-of-the-art results on the Human 3.6M and 3DPoses in the Wild datasets, as well as a human-related subset of the Sinteldataset, both in terms of pose estimation accuracy and the optical flowaccuracy at human joint locations. Code available athttps://github.com/ubc-vision/bootstrapping-human-optical-flow-and-pose</p> </div> <div class="qing hidden"> <p>使用pose来增强光流</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">2210.15121</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bootstrapping Human Optical Flow and Pose}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arko, Aritro Roy and Little, James J. and Yi, Kwang Moo}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, optical-flow, pose-estimation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/optical-flow"> <i class="fas fa-hashtag fa-sm"></i> optical-flow</a>  <a href="/tags/pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> pose-estimation</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="2210.04514" class="col-sm-8"> <div class="title">Self-Supervised 3D Human Pose Estimation in Static Video Via Neural Rendering</div> <div class="author"> Luca Schmidtke, Benjamin Hou, Athanasios Vlontzos, and Bernhard Kainz</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.04514.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Inferring 3D human pose from 2D images is a challenging and long-standingproblem in the field of computer vision with many applications including motioncapture, virtual reality, surveillance or gait analysis for sports andmedicine. We present preliminary results for a method to estimate 3D pose from2D video containing a single person and a static background without the needfor any manual landmark annotations. We achieve this by formulating a simpleyet effective self-supervision task: our model is required to reconstruct arandom frame of a video given a frame from another timepoint and a renderedimage of a transformed human shape template. Crucially for optimisation, ourray casting based rendering pipeline is fully differentiable, enabling end toend training solely based on the reconstruction task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">2210.04514</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Supervised 3D Human Pose Estimation in Static Video Via Neural Rendering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schmidtke, Luca and Hou, Benjamin and Vlontzos, Athanasios and Kainz, Bernhard}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{pose-estimation, neural-rendering, self-supervised}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> pose-estimation</a>  <a href="/tags/neural-rendering"> <i class="fas fa-hashtag fa-sm"></i> neural-rendering</a>  <a href="/tags/self-supervised"> <i class="fas fa-hashtag fa-sm"></i> self-supervised</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="AdaptivePose++" class="col-sm-8"> <div class="title">AdaptivePose++: A Powerful Single-Stage Network for Multi-Person Pose Regression</div> <div class="author"> Yabo Xiao, Xiaojuan Wang, Dongdong Yu, Kai Su, Lei Jin, Mei Song, Shuicheng Yan, and Jian Zhao</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.04014.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Multi-person pose estimation generally follows top-down and bottom-upparadigms. Both of them use an extra stage (\boldsymbole.g., humandetection in top-down paradigm or grouping process in bottom-up paradigm) tobuild the relationship between the human instance and corresponding keypoints,thus leading to the high computation cost and redundant two-stage pipeline. Toaddress the above issue, we propose to represent the human parts as adaptivepoints and introduce a fine-grained body representation method. The novel bodyrepresentation is able to sufficiently encode the diverse pose information andeffectively model the relationship between the human instance and correspondingkeypoints in a single-forward pass. With the proposed body representation, wefurther deliver a compact single-stage multi-person pose regression network,termed as AdaptivePose. During inference, our proposed network only needs asingle-step decode operation to form the multi-person pose without complexpost-processes and refinements. We employ AdaptivePose for both 2D/3Dmulti-person pose estimation tasks to verify the effectiveness of AdaptivePose.Without any bells and whistles, we achieve the most competitive performance onMS COCO and CrowdPose in terms of accuracy and speed. Furthermore, theoutstanding performance on MuCo-3DHP and MuPoTS-3D further demonstrates theeffectiveness and generalizability on 3D scenes. Code is available athttps://github.com/buptxyb666/AdaptivePose.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">AdaptivePose++</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AdaptivePose++: A Powerful Single-Stage Network for Multi-Person Pose Regression}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xiao, Yabo and Wang, Xiaojuan and Yu, Dongdong and Su, Kai and Jin, Lei and Song, Mei and Yan, Shuicheng and Zhao, Jian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{multi-person, monocular, pose-estimation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/multi-person"> <i class="fas fa-hashtag fa-sm"></i> multi-person</a>  <a href="/tags/monocular"> <i class="fas fa-hashtag fa-sm"></i> monocular</a>  <a href="/tags/pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> pose-estimation</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="SmartMocap" class="col-sm-8"> <div class="title">SmartMocap: Joint Estimation of Human and Camera Motion using Uncalibrated RGB Cameras</div> <div class="author"> Nitin Saini, Chun-hao P. Huang, <a href="https://ps.is.tuebingen.mpg.de/person/black" target="_blank" rel="noopener noreferrer">Michael J. Black</a>, and Aamir Ahmad</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2209.13906.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Markerless human motion capture (mocap) from multiple RGB cameras is a widelystudied problem. Existing methods either need calibrated cameras or calibratethem relative to a static camera, which acts as the reference frame for themocap system. The calibration step has to be done a priori for every capturesession, which is a tedious process, and re-calibration is required whenevercameras are intentionally or accidentally moved. In this paper, we propose amocap method which uses multiple static and moving extrinsically uncalibratedRGB cameras. The key components of our method are as follows. First, since thecameras and the subject can move freely, we select the ground plane as a commonreference to represent both the body and the camera motions unlike existingmethods which represent bodies in the camera coordinate. Second, we learn aprobability distribution of short human motion sequences (\sim1sec) relativeto the ground plane and leverage it to disambiguate between the camera andhuman motion. Third, we use this distribution as a motion prior in a novelmulti-stage optimization approach to fit the SMPL human body model and thecamera poses to the human body keypoints on the images. Finally, we show thatour method can work on a variety of datasets ranging from aerial cameras tosmartphones. It also gives more accurate results compared to thestate-of-the-art on the task of monocular human mocap with a static camera. Ourcode is available for research purposes onhttps://github.com/robot-perception-group/SmartMocap.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SmartMocap</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SmartMocap: Joint Estimation of Human and Camera Motion using Uncalibrated RGB Cameras}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saini, Nitin and Huang, Chun-hao P. and Black, Michael J. and Ahmad, Aamir}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{mocap, pose-estimation, uncalibrated}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/mocap"> <i class="fas fa-hashtag fa-sm"></i> mocap</a>  <a href="/tags/pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> pose-estimation</a>  <a href="/tags/uncalibrated"> <i class="fas fa-hashtag fa-sm"></i> uncalibrated</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Regularizing_Vector_Embedding.png"> </div> <div id="Regularizing_Vector_Embedding" class="col-sm-8"> <div class="title">Regularizing Vector Embedding in Bottom-Up Human Pose Estimation</div> <div class="author"> </div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660105.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="qing hidden"> <p>使用scale来提升embedding</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Regularizing_Vector_Embedding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Regularizing Vector Embedding in Bottom-Up Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-pose-estimation, bottom-up}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> human-pose-estimation</a>  <a href="/tags/bottom-up"> <i class="fas fa-hashtag fa-sm"></i> bottom-up</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Mobius_GCN.png"> </div> <div id="Mobius_GCN" class="col-sm-8"> <div class="title">3D Human Pose Estimation Using Möbius Graph Convolutional Networks</div> <div class="author"> Niloofar Azizi, Horst Possegger, Emanuele Rodolà, and Horst Bischof</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2203.10554.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>3D human pose estimation is fundamental to understanding human behavior.Recently, promising results have been achieved by graph convolutional networks(GCNs), which achieve state-of-the-art performance and provide ratherlight-weight architectures. However, a major limitation of GCNs is theirinability to encode all the transformations between joints explicitly. Toaddress this issue, we propose a novel spectral GCN using the Möbiustransformation (MöbiusGCN). In particular, this allows us to directly andexplicitly encode the transformation between joints, resulting in asignificantly more compact representation. Compared to even the lightestarchitectures so far, our novel approach requires 90-98% fewer parameters, i.e.our lightest MöbiusGCN uses only 0.042M trainable parameters. Besides thedrastic parameter reduction, explicitly encoding the transformation of jointsalso enables us to achieve state-of-the-art results. We evaluate our approachon the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP,demonstrating both state-of-the-art results and the generalization capabilitiesof MöbiusGCN.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Mobius_GCN</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{3D Human Pose Estimation Using Möbius Graph Convolutional Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azizi, Niloofar and Possegger, Horst and Rodolà, Emanuele and Bischof, Horst}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-pose-estimation, gcn}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> human-pose-estimation</a>  <a href="/tags/gcn"> <i class="fas fa-hashtag fa-sm"></i> gcn</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Occulusion_reasoning.png"> </div> <div id="Occulusion_reasoning" class="col-sm-8"> <div class="title">Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation</div> <div class="author"> Qihao Liu, Yi Zhang, Song Bai, and Alan Yuille</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2208.00090.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Occlusion poses a great threat to monocular multi-person 3D human poseestimation due to large variability in terms of the shape, appearance, andposition of occluders. While existing methods try to handle occlusion with posepriors/constraints, data augmentation, or implicit reasoning, they still failto generalize to unseen poses or occlusion cases and may make large mistakeswhen multiple people are present. Inspired by the remarkable ability of humansto infer occluded joints from visible cues, we develop a method to explicitlymodel this process that significantly improves bottom-up multi-person humanpose estimation with or without occlusions. First, we split the task into twosubtasks: visible keypoints detection and occluded keypoints reasoning, andpropose a Deeply Supervised Encoder Distillation (DSED) network to solve thesecond one. To train our model, we propose a Skeleton-guided human ShapeFitting (SSF) approach to generate pseudo occlusion labels on the existingdatasets, enabling explicit occlusion reasoning. Experiments show thatexplicitly learning from occlusions improves human pose estimation. Inaddition, exploiting feature-level information of visible joints allows us toreason about occluded joints more accurately. Our method outperforms both thestate-of-the-art top-down and bottom-up methods on several benchmarks.</p> </div> <div class="qing hidden"> <p>估计被遮挡住的数据然后再进行association</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Occulusion_reasoning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Qihao and Zhang, Yi and Bai, Song and Yuille, Alan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-pose-estimation, 1vmp}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> human-pose-estimation</a>  <a href="/tags/1vmp"> <i class="fas fa-hashtag fa-sm"></i> 1vmp</a>  </div> </div> </li> </ol> <ol class="bibliography"></ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/SimCC.png"> </div> <div id="SimCC" class="col-sm-8"> <div class="title">SimCC: a Simple Coordinate Classification Perspective for Human Pose Estimation</div> <div class="author"> Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, and Shu-Tao Xia</div> <div class="periodical"> <em>In ECCV</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2107.03332.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>The 2D heatmap-based approaches have dominated Human Pose Estimation (HPE)for years due to high performance. However, the long-standing quantizationerror problem in the 2D heatmap-based methods leads to several well-knowndrawbacks: 1) The performance for the low-resolution inputs is limited; 2) Toimprove the feature map resolution for higher localization precision, multiplecostly upsampling layers are required; 3) Extra post-processing is adopted toreduce the quantization error. To address these issues, we aim to explore abrand new scheme, called \textitSimCC, which reformulates HPE as twoclassification tasks for horizontal and vertical coordinates. The proposedSimCC uniformly divides each pixel into several bins, thus achieving\emphsub-pixel localization precision and low quantization error. Benefitingfrom that, SimCC can omit additional refinement post-processing and excludeupsampling layers under certain settings, resulting in a more simple andeffective pipeline for HPE. Extensive experiments conducted over COCO,CrowdPose, and MPII datasets show that SimCC outperforms heatmap-basedcounterparts, especially in low-resolution settings by a large margin.</p> </div> <div class="qing hidden"> <p>从坐标分类的角度来看2D人体姿态估计问题</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SimCC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SimCC: a Simple Coordinate Classification Perspective for Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Yanjie and Yang, Sen and Liu, Peidong and Zhang, Shoukui and Wang, Yunxiao and Wang, Zhicheng and Yang, Wankou and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-pose-estimation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> human-pose-estimation</a>  </div> </div> </li></ol> <ol class="bibliography"></ol> <h2 class="year">2020</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2019</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2018</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2017</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Qing Shuai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </div></body> </html>