<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>view-synthesis | Qing Shuai | 帅青</title> <meta name="author" content="Qing Shuai"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="chingswy.github.io/tags/view-synthesis/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Qing Shuai | 帅青</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis </h1> <p class="post-description"> Papers with tag view-synthesis </p> </header> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="HDHumans" class="col-sm-8"> <div class="title">HDHumans: A Hybrid Approach for High-fidelity Digital Humans</div> <div class="author"> Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-Moll, <a href="https://zollhoefer.com/" target="_blank" rel="noopener noreferrer">Michael Zollhoefer</a>, and <a href="https://vcai.mpi-inf.mpg.de/VCAI_Projects.html" target="_blank" rel="noopener noreferrer">Christian Theobalt</a> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.12003.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Photo-real digital human avatars are of enormous importance in graphics, asthey enable immersive communication over the globe, improve gaming andentertainment experiences, and can be particularly beneficial for AR and VRsettings. However, current avatar generation approaches either fall short inhigh-fidelity novel view synthesis, generalization to novel motions,reproduction of loose clothing, or they cannot render characters at the highresolution offered by modern displays. To this end, we propose HDHumans, whichis the first method for HD human character synthesis that jointly produces anaccurate and temporally coherent 3D deforming surface and highlyphoto-realistic images of arbitrary novel views and of motions not seen attraining time. At the technical core, our method tightly integrates a classicaldeforming character template with neural radiance fields (NeRF). Our method iscarefully designed to achieve a synergy between classical surface deformationand NeRF. First, the template guides the NeRF, which allows synthesizing novelviews of a highly dynamic and articulated character and even enables thesynthesis of novel motions. Second, we also leverage the dense pointcloudsresulting from NeRF to further improve the deforming surface via 3D-to-3Dsupervision. We outperform the state of the art quantitatively andqualitatively in terms of synthesis quality and resolution, as well as thequality of 3D surface reconstruction.</p> </div> <div class="qing hidden"> <p>DeepCap的拓展</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HDHumans</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HDHumans: A Hybrid Approach for High-fidelity Digital Humans}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Habermann, Marc and Liu, Lingjie and Xu, Weipeng and Pons-Moll, Gerard and Zollhoefer, Michael and Theobalt, Christian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-synthesis, view-synthesis, human-modeling, human-performance-capture}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-synthesis"> <i class="fas fa-hashtag fa-sm"></i> human-synthesis</a>  <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  <a href="/tags/human-modeling"> <i class="fas fa-hashtag fa-sm"></i> human-modeling</a>  <a href="/tags/human-performance-capture"> <i class="fas fa-hashtag fa-sm"></i> human-performance-capture</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/NDF.png"> </div> <div id="NDF" class="col-sm-8"> <div class="title">NDF: Neural Deformable Fields for Dynamic Human Modelling</div> <div class="author"> Ruiqi Zhang, and Jie Chen</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2207.09193.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose Neural Deformable Fields (NDF), a new representation for dynamichuman digitization from a multi-view video. Recent works proposed to representa dynamic human body with shared canonical neural radiance fields which linksto the observation space with deformation fields estimations. However, thelearned canonical representation is static and the current design of thedeformation fields is not able to represent large movements or detailedgeometry changes. In this paper, we propose to learn a neural deformable fieldwrapped around a fitted parametric body model to represent the dynamic human.The NDF is spatially aligned by the underlying reference surface. A neuralnetwork is then learned to map pose to the dynamics of NDF. The proposed NDFrepresentation can synthesize the digitized performer with novel views andnovel poses with a detailed and reasonable dynamic appearance. Experiments showthat our method significantly outperforms recent human synthesis methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NDF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NDF: Neural Deformable Fields for Dynamic Human Modelling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Ruiqi and Chen, Jie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  </div> </div> </li> </ol> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="boosting_pc_render" class="col-sm-8"> <div class="title">Boosting Point Clouds Rendering via Radiance Mapping</div> <div class="author"> Xiaoyang Huang, Yi Zhang, Bingbing Ni, Teng Li, Kai Chen, and Wenjun Zhang</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.15107.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Recent years we have witnessed rapid development in NeRF-based imagerendering due to its high quality. However, point clouds rendering is somehowless explored. Compared to NeRF-based rendering which suffers from densespatial sampling, point clouds rendering is naturally less computationintensive, which enables its deployment in mobile computing device. In thiswork, we focus on boosting the image quality of point clouds rendering with acompact model design. We first analyze the adaption of the volume renderingformulation on point clouds. Based on the analysis, we simplify the NeRFrepresentation to a spatial mapping function which only requires singleevaluation per pixel. Further, motivated by ray marching, we rectify the thenoisy raw point clouds to the estimated intersection between rays and surfacesas queried coordinates, which could avoid spatial frequency collapse andneighbor point disturbance. Composed of rasterization, spatial mapping and therefinement stages, our method achieves the state-of-the-art performance onpoint clouds rendering, outperforming prior works by notable margins, with asmaller model size. We obtain a PSNR of 31.74 on NeRF-Synthetic, 25.88 onScanNet and 30.81 on DTU. Code and data would be released soon.</p> </div> <div class="sida hidden"> <p>基于point cloud做rendering, 实现每条ray一次evaluation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">boosting_pc_render</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Boosting Point Clouds Rendering via Radiance Mapping}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Xiaoyang and Zhang, Yi and Ni, Bingbing and Li, Teng and Chen, Kai and Zhang, Wenjun}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis, point-cloud}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{基于point cloud做rendering, 实现每条ray一次evaluation.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  <a href="/tags/point-cloud"> <i class="fas fa-hashtag fa-sm"></i> point-cloud</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="VIINTER" class="col-sm-8"> <div class="title">VIINTER: View Interpolation with Implicit Neural Representations of Images</div> <div class="author"> Brandon Yushan Feng, Susmija Jabbireddy, and Amitabh Varshney</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.00722.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present VIINTER, a method for view interpolation by interpolating theimplicit neural representation (INR) of the captured images. We leverage thelearned code vector associated with each image and interpolate between thesecodes to achieve viewpoint transitions. We propose several techniques thatsignificantly enhance the interpolation quality. VIINTER signifies a new way toachieve view interpolation without constructing 3D structure, estimating cameraposes, or computing pixel correspondence. We validate the effectiveness ofVIINTER on several multi-view scenes with different types of camera layout andscene composition. As the development of INR of images (as opposed to surfaceor volume) has centered around tasks like image fitting and super-resolution,with VIINTER, we show its capability for view interpolation and offer apromising outlook on using INR for image manipulation tasks.</p> </div> <div class="sida hidden"> <p>用INR和latent vectors记录多张图片, 通过插值latent vectors实现view interpolation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">VIINTER</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VIINTER: View Interpolation with Implicit Neural Representations of Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Feng, Brandon Yushan and Jabbireddy, Susmija and Varshney, Amitabh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{用INR和latent vectors记录多张图片, 通过插值latent vectors实现view interpolation.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/ParticleNeRF.png"> </div> <div id="ParticleNeRF" class="col-sm-8"> <div class="title">ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields in Dynamic Scenes</div> <div class="author"> Jad Abou-Chakra, Feras Dayoub, and Niko Sünderhauf</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.04041.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Neural Radiance Fields (NeRFs) learn implicit representations of - typicallystatic - environments from images. Our paper extends NeRFs to handle dynamicscenes in an online fashion. We propose ParticleNeRF that adapts to changes inthe geometry of the environment as they occur, learning a new up-to-daterepresentation every 350 ms. ParticleNeRF can represent the current state ofdynamic environments with much higher fidelity as other NeRF frameworks. Toachieve this, we introduce a new particle-based parametric encoding, whichallows the intermediate NeRF features - now coupled to particles in space - tomove with the dynamic geometry. This is possible by backpropagating thephotometric reconstruction loss into the position of the particles. Theposition gradients are interpreted as particle velocities and integrated intopositions using a position-based dynamics (PBS) physics system. Introducing PBSinto the NeRF formulation allows us to add collision constraints to theparticle motion and creates future opportunities to add other movement priorsinto the system, such as rigid and deformable body</p> </div> <div class="sida hidden"> <p>提出了particle-based parametric encoding, 将features anchor在dynamic geometry, 实现view synthesis of dynamic scenes. 用position-based dynamics physics system表示dynamic geometry.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ParticleNeRF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields in Dynamic Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Abou-Chakra, Jad and Dayoub, Feras and Sünderhauf, Niko}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis, dynamic-scene}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{提出了particle-based parametric encoding, 将features anchor在dynamic geometry, 实现view synthesis of dynamic scenes. 用position-based dynamics physics system表示dynamic geometry.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  <a href="/tags/dynamic-scene"> <i class="fas fa-hashtag fa-sm"></i> dynamic-scene</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Common_Pets.png"> </div> <div id="Common_Pets" class="col-sm-8"> <div class="title">Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories</div> <div class="author"> Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ignacio Rocco, Natalia Neverova, Andrea Vedaldi, and David Novotny</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.03889.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Obtaining photorealistic reconstructions of objects from sparse views isinherently ambiguous and can only be achieved by learning suitablereconstruction priors. Earlier works on sparse rigid object reconstructionsuccessfully learned such priors from large datasets such as CO3D. In thispaper, we extend this approach to dynamic objects. We use cats and dogs as arepresentative example and introduce Common Pets in 3D (CoP3D), a collection ofcrowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of thefirst large-scale datasets for benchmarking non-rigid 3D reconstruction "in thewild". We also propose Tracker-NeRF, a method for learning 4D reconstructionfrom our dataset. At test time, given a small number of video frames of anunseen object, Tracker-NeRF predicts the trajectories of its 3D points andgenerates new views, interpolating viewpoint and time. Results on CoP3D revealsignificantly better non-rigid new-view synthesis performance than existingbaselines.</p> </div> <div class="sida hidden"> <p>提出了一个deformable objects的数据集, 用于训练一个网络, 从几帧图像中重建出deformable objects.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Common_Pets</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sinha, Samarth and Shapovalov, Roman and Reizenstein, Jeremy and Rocco, Ignacio and Neverova, Natalia and Vedaldi, Andrea and Novotny, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{monocular-video, deformable-objects, view-synthesis}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{提出了一个deformable objects的数据集, 用于训练一个网络, 从几帧图像中重建出deformable objects.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/monocular-video"> <i class="fas fa-hashtag fa-sm"></i> monocular-video</a>  <a href="/tags/deformable-objects"> <i class="fas fa-hashtag fa-sm"></i> deformable-objects</a>  <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"></ol> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Temporal-MPI.png"> </div> <div id="Temporal-MPI" class="col-sm-8"> <div class="title">Temporal-MPI: Enabling Multi-Plane Images for Dynamic Scene Modelling via Temporal Basis Learning</div> <div class="author"> Wenpeng Xing, and Jie Chen</div> <div class="periodical"> <em>In </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.10533.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Novel view synthesis of static scenes has achieved remarkable advancements inproducing photo-realistic results. However, key challenges remain for immersiverendering of dynamic scenes. One of the seminal image-based rendering method,the multi-plane image (MPI), produces high novel-view synthesis quality forstatic scenes. But modelling dynamic contents by MPI is not studied. In thispaper, we propose a novel Temporal-MPI representation which is able to encodethe rich 3D and dynamic variation information throughout the entire video ascompact temporal basis and coefficients jointly learned. Time-instance MPI forrendering can be generated efficiently using mini-seconds by linearcombinations of temporal basis and coefficients from Temporal-MPI. Thusnovel-views at arbitrary time-instance will be able to be rendered viaTemporal-MPI in real-time with high visual quality. Our method is trained andevaluated on Nvidia Dynamic Scene Dataset. We show that our proposed Temporal-MPI is much faster and more compact compared with other state-of-the-artdynamic scene modelling methods.</p> </div> <div class="sida hidden"> <p>构建一组MPI basis和coefficients, 通过linear combinations得到每一帧的MPI, 实现dynamic view synthesis.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Temporal-MPI</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Temporal-MPI: Enabling Multi-Plane Images for Dynamic Scene Modelling via Temporal Basis Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xing, Wenpeng and Chen, Jie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis, dynamic-scene}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{构建一组MPI basis和coefficients, 通过linear combinations得到每一帧的MPI, 实现dynamic view synthesis.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  <a href="/tags/dynamic-scene"> <i class="fas fa-hashtag fa-sm"></i> dynamic-scene</a>  </div> </div> </li></ol> <h2 class="year">2020</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2019</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2018</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2017</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Qing Shuai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </div></body> </html>