<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>1p | Qing Shuai | 帅青</title> <meta name="author" content="Qing Shuai"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="chingswy.github.io/tags/1p/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Qing Shuai | 帅青</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <i class="fas fa-hashtag fa-sm"></i> 1p </h1> <p class="post-description"> Papers with tag 1p </p> </header> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="lrmesh" class="col-sm-8"> <div class="title">Learnable human mesh triangulation for 3D human pose and shape estimation</div> <div class="author"> Sungho Chun, Sungbum Park, and Ju Yong Chang</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2208.11251" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Compared to joint position, the accuracy of joint rotation and shapeestimation has received relatively little attention in the skinned multi-personlinear model (SMPL)-based human mesh reconstruction from multi-view images. Thework in this field is broadly classified into two categories. The firstapproach performs joint estimation and then produces SMPL parameters by fittingSMPL to resultant joints. The second approach regresses SMPL parametersdirectly from the input images through a convolutional neural network(CNN)-based model. However, these approaches suffer from the lack ofinformation for resolving the ambiguity of joint rotation and shapereconstruction and the difficulty of network learning. To solve theaforementioned problems, we propose a two-stage method. The proposed methodfirst estimates the coordinates of mesh vertices through a CNN-based model frominput images, and acquires SMPL parameters by fitting the SMPL model to theestimated vertices. Estimated mesh vertices provide sufficient information fordetermining joint rotation and shape, and are easier to learn than SMPLparameters. According to experiments using Human3.6M and MPI-INF-3DHP datasets,the proposed method significantly outperforms the previous works in terms ofjoint rotation and shape estimation, and achieves competitive performance interms of joint location estimation.</p> </div> <div class="qing hidden"> <p>每个视角进行可见性判断再进行特征融合，最后接了拟合模块</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lrmesh</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learnable human mesh triangulation for 3D human pose and shape estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chun, Sungho and Park, Sungbum and Chang, Ju Yong}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, SMPL, e2e}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/smpl"> <i class="fas fa-hashtag fa-sm"></i> SMPL</a>  <a href="/tags/e2e"> <i class="fas fa-hashtag fa-sm"></i> e2e</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="2203.15865" class="col-sm-8"> <div class="title">On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation</div> <div class="author"> Soumava Kumar Roy, Leonardo Citraro, Sina Honari, and <a href="https://people.epfl.ch/pascal.fua/bio?lang=en" target="_blank" rel="noopener noreferrer">Pascal Fua</a> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2203.15865" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Supervised approaches to 3D pose estimation from single images are remarkablyeffective when labeled data is abundant. However, as the acquisition ofground-truth 3D labels is labor intensive and time consuming, recent attentionhas shifted towards semi- and weakly-supervised learning. Generating aneffective form of supervision with little annotations still poses majorchallenge in crowded scenes. In this paper we propose to impose multi-viewgeometrical constraints by means of a weighted differentiable triangulation anduse it as a form of self-supervision when no labels are available. We thereforetrain a 2D pose estimator in such a way that its predictions correspond to there-projection of the triangulated 3D pose and train an auxiliary network onthem to produce the final 3D poses. We complement the triangulation with aweighting mechanism that alleviates the impact of noisy predictions caused byself-occlusion or occlusion from other subjects. We demonstrate theeffectiveness of our semi-supervised approach on Human3.6M and MPI-INF-3DHPdatasets, as well as on a new multi-view multi-person dataset that featuresocclusion.</p> </div> <div class="qing hidden"> <p>使用多视角三角化来自监督</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">2203.15865</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Roy, Soumava Kumar and Citraro, Leonardo and Honari, Sina and Fua, Pascal}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose, e2e, self-supervised}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  <a href="/tags/e2e"> <i class="fas fa-hashtag fa-sm"></i> e2e</a>  <a href="/tags/self-supervised"> <i class="fas fa-hashtag fa-sm"></i> self-supervised</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/PPT.png"> </div> <div id="PPT" class="col-sm-8"> <div class="title">PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation</div> <div class="author"> Haoyu Ma, Zhe Wang, Yifei Chen, Deying Kong, Liangjian Chen, Xingwei Liu, Xiangyi Yan, Hao Tang, and Xiaohui Xie</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2209.08194.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Recently, the vision transformer and its variants have played an increasinglyimportant role in both monocular and multi-view human pose estimation.Considering image patches as tokens, transformers can model the globaldependencies within the entire image or across images from other views.However, global attention is computationally expensive. As a consequence, it isdifficult to scale up these transformer-based methods to high-resolutionfeatures and many views. In this paper, we propose the token-Pruned Pose Transformer (PPT) for 2Dhuman pose estimation, which can locate a rough human mask and performsself-attention only within selected tokens. Furthermore, we extend our PPT tomulti-view human pose estimation. Built upon PPT, we propose a new cross-viewfusion strategy, called human area fusion, which considers all human foregroundpixels as corresponding candidates. Experimental results on COCO and MPIIdemonstrate that our PPT can match the accuracy of previous pose transformermethods while reducing the computation. Moreover, experiments on Human 3.6M andSki-Pose demonstrate that our Multi-view PPT can efficiently fuse cues frommultiple views and achieve new state-of-the-art results.</p> </div> <div class="qing hidden"> <p>使用人体区域来做fusion</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PPT</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Haoyu and Wang, Zhe and Chen, Yifei and Kong, Deying and Chen, Liangjian and Liu, Xingwei and Yan, Xiangyi and Tang, Hao and Xie, Xiaohui}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="LVD" class="col-sm-8"> <div class="title">Learned Vertex Descent: A New Direction for 3D Human Model Fitting</div> <div class="author"> Enric Corona, Gerard Pons-Moll, Guillem Alenyà, and Francesc Moreno-Noguer</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.06254.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose a novel optimization-based paradigm for 3D human model fitting onimages and scans. In contrast to existing approaches that directly regress theparameters of a low-dimensional statistical body model (e.g. SMPL) from inputimages, we train an ensemble of per-vertex neural fields network. The networkpredicts, in a distributed manner, the vertex descent direction towards theground truth, based on neural features extracted at the current vertexprojection. At inference, we employ this network, dubbed LVD, within agradient-descent optimization pipeline until its convergence, which typicallyoccurs in a fraction of a second even when initializing all vertices into asingle point. An exhaustive evaluation demonstrates that our approach is ableto capture the underlying body of clothed people with very different bodyshapes, achieving a significant improvement compared to state-of-the-art. LVDis also applicable to 3D model fitting of humans and hands, for which we show asignificant improvement to the SOTA with a much simpler and faster method.</p> </div> <div class="qing hidden"> <p>一种新的姿态估计的框架</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">LVD</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learned Vertex Descent: A New Direction for 3D Human Model Fitting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Corona, Enric and Pons-Moll, Gerard and Alenyà, Guillem and Moreno-Noguer, Francesc}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, monocular, 1p, SMPL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/monocular"> <i class="fas fa-hashtag fa-sm"></i> monocular</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/smpl"> <i class="fas fa-hashtag fa-sm"></i> SMPL</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Structural_Triangulation.png"> </div> <div id="Structural_Triangulation" class="col-sm-8"> <div class="title">Structural Triangulation: A Closed-Form Solution to Constrained 3D Human Pose Estimation</div> <div class="author"> </div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650685.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="qing hidden"> <p>输入相机参数和骨架结构信息，优化的方式获得人体骨架</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Structural_Triangulation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Structural Triangulation: A Closed-Form Solution to Constrained 3D Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{mv1p}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/mv1p"> <i class="fas fa-hashtag fa-sm"></i> mv1p</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/PPT.png"> </div> <div id="PPU" class="col-sm-8"> <div class="title">PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation</div> <div class="author"> Haoyu Ma, Zhe Wang, Yifei Chen, Deying Kong, Liangjian Chen, Xingwei Liu, Xiangyi Yan, Hao Tang, and Xiaohui Xie</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2209.08194.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Recently, the vision transformer and its variants have played an increasinglyimportant role in both monocular and multi-view human pose estimation.Considering image patches as tokens, transformers can model the globaldependencies within the entire image or across images from other views.However, global attention is computationally expensive. As a consequence, it isdifficult to scale up these transformer-based methods to high-resolutionfeatures and many views. In this paper, we propose the token-Pruned Pose Transformer (PPT) for 2Dhuman pose estimation, which can locate a rough human mask and performsself-attention only within selected tokens. Furthermore, we extend our PPT tomulti-view human pose estimation. Built upon PPT, we propose a new cross-viewfusion strategy, called human area fusion, which considers all human foregroundpixels as corresponding candidates. Experimental results on COCO and MPIIdemonstrate that our PPT can match the accuracy of previous pose transformermethods while reducing the computation. Moreover, experiments on Human 3.6M andSki-Pose demonstrate that our Multi-view PPT can efficiently fuse cues frommultiple views and achieve new state-of-the-art results.</p> </div> <div class="qing hidden"> <p>使用人体区域来做fusion</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PPU</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Haoyu and Wang, Zhe and Chen, Yifei and Kong, Deying and Chen, Liangjian and Liu, Xingwei and Yan, Xiangyi and Tang, Hao and Xie, Xiaohui}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/VisDB.png"> </div> <div id="VisDB" class="col-sm-8"> <div class="title">Learning Visibility for Robust Dense Human Body Estimation</div> <div class="author"> Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, Yang Zhou, and Ming-Hsuan Yang</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2208.10652.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Estimating 3D human pose and shape from 2D images is a crucial yetchallenging task. While prior methods with model-based representations canperform reasonably well on whole-body images, they often fail when parts of thebody are occluded or outside the frame. Moreover, these results usually do notfaithfully capture the human silhouettes due to their limited representationpower of deformable models (e.g., representing only the naked body). Analternative approach is to estimate dense vertices of a predefined templatebody in the image space. Such representations are effective in localizingvertices within an image but cannot handle out-of-frame body parts. In thiswork, we learn dense human body estimation that is robust to partialobservations. We explicitly model the visibility of human joints and verticesin the x, y, and z axes separately. The visibility in x and y axes helpdistinguishing out-of-frame cases, and the visibility in depth axis correspondsto occlusions (either self-occlusions or occlusions by other objects). Weobtain pseudo ground-truths of visibility labels from dense UV correspondencesand train a neural network to predict visibility along with 3D coordinates. Weshow that visibility can serve as 1) an additional signal to resolve depthordering ambiguities of self-occluded vertices and 2) a regularization termwhen fitting a human body model to the predictions. Extensive experiments onmultiple 3D human datasets demonstrate that visibility modeling significantlyimproves the accuracy of human body estimation, especially for partial-bodycases. Our project page with code is at: https://github.com/chhankyao/visdb.</p> </div> <div class="qing hidden"> <p>考虑了遮挡来估计SMPL</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">VisDB</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Visibility for Robust Dense Human Body Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yao, Chun-Han and Yang, Jimei and Ceylan, Duygu and Zhou, Yi and Zhou, Yang and Yang, Ming-Hsuan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{SMPL, 1v1p}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/smpl"> <i class="fas fa-hashtag fa-sm"></i> SMPL</a>  <a href="/tags/1v1p"> <i class="fas fa-hashtag fa-sm"></i> 1v1p</a>  </div> </div> </li> </ol> <ol class="bibliography"></ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="temporal3d" class="col-sm-8"> <div class="title">Learning Temporal 3D Human Pose Estimation with Pseudo-Labels</div> <div class="author"> Arij Bouazizi, Ulrich Kressel, and Vasileios Belagiannis</div> <div class="periodical"> <em>In </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2110.07578.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present a simple, yet effective, approach for self-supervised 3D humanpose estimation. Unlike the prior work, we explore the temporal informationnext to the multi-view self-supervision. During training, we rely ontriangulating 2D body pose estimates of a multiple-view camera system. Atemporal convolutional neural network is trained with the generated 3Dground-truth and the geometric multi-view consistency loss, imposinggeometrical constraints on the predicted 3D body skeleton. During inference,our model receives a sequence of 2D body pose estimates from a single-view topredict the 3D body pose for each of them. An extensive evaluation shows thatour method achieves state-of-the-art performance in the Human3.6M andMPI-INF-3DHP benchmarks. Our code and models are publicly available at\urlhttps://github.com/vru2020/TM_HPE/.</p> </div> <div class="qing hidden"> <p>输入一段序列的2D关键点，输出3Dpose，通过多视角的一致性来监督</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">temporal3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Temporal 3D Human Pose Estimation with Pseudo-Labels}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bouazizi, Arij and Kressel, Ulrich and Belagiannis, Vasileios}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, monocular, 1p, 3dpose, self-supervised}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/monocular"> <i class="fas fa-hashtag fa-sm"></i> monocular</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  <a href="/tags/self-supervised"> <i class="fas fa-hashtag fa-sm"></i> self-supervised</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="GeneralTri" class="col-sm-8"> <div class="title">Generalizable Human Pose Triangulation</div> <div class="author"> Kristijan Bartol, David Bojanić, Tomislav Petković, and Tomislav Pribanić</div> <div class="periodical"> <em>In </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2110.00280.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We address the problem of generalizability for multi-view 3D human poseestimation. The standard approach is to first detect 2D keypoints in images andthen apply triangulation from multiple views. Even though the existing methodsachieve remarkably accurate 3D pose estimation on public benchmarks, most ofthem are limited to a single spatial camera arrangement and their number.Several methods address this limitation but demonstrate significantly degradedperformance on novel views. We propose a stochastic framework for human posetriangulation and demonstrate a superior generalization across different cameraarrangements on two public datasets. In addition, we apply the same approach tothe fundamental matrix estimation problem, showing that the proposed method cansuccessfully apply to other computer vision problems. The stochastic frameworkachieves more than 8.8% improvement on the 3D pose estimation task, compared tothe state-of-the-art, and more than 30% improvement for fundamental matrixestimation, compared to a standard algorithm.</p> </div> <div class="qing hidden"> <p>提出了一个框架来解决泛化的三角化问题</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">GeneralTri</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalizable Human Pose Triangulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bartol, Kristijan and Bojanić, David and Petković, Tomislav and Pribanić, Tomislav}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="2110.05092" class="col-sm-8"> <div class="title">Adaptive Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation</div> <div class="author"> Hui Shuai, Lele Wu, and Qingshan Liu</div> <div class="periodical"> <em>In </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2110.05092.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>This paper proposes a unified framework dubbed Multi-view and Temporal FusingTransformer (MTF-Transformer) to adaptively handle varying view numbers andvideo length without camera calibration in 3D Human Pose Estimation (HPE). Itconsists of Feature Extractor, Multi-view Fusing Transformer (MFT), andTemporal Fusing Transformer (TFT). Feature Extractor estimates 2D pose fromeach image and fuses the prediction according to the confidence. It providespose-focused feature embedding and makes subsequent modules computationallylightweight. MFT fuses the features of a varying number of views with a novelRelative-Attention block. It adaptively measures the implicit relativerelationship between each pair of views and reconstructs more informativefeatures. TFT aggregates the features of the whole sequence and predicts 3Dpose via a transformer. It adaptively deals with the video of arbitrary lengthand fully unitizes the temporal information. The migration of transformersenables our model to learn spatial geometry better and preserve robustness forvarying application scenarios. We report quantitative and qualitative resultson the Human3.6M, TotalCapture, and KTH Multiview Football II. Compared withstate-of-the-art methods with camera parameters, MTF-Transformer obtainscompetitive results and generalizes well to dynamic capture with an arbitrarynumber of unseen views.</p> </div> <div class="qing hidden"> <p>多视角特征融合的transformer以及时序融合的transformer</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">2110.05092</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adaptive Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shuai, Hui and Wu, Lele and Liu, Qingshan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose, transformer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  <a href="/tags/transformer"> <i class="fas fa-hashtag fa-sm"></i> transformer</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="semidense" class="col-sm-8"> <div class="title">Semi-supervised Dense Keypointsusing Unlabeled Multiview Images</div> <div class="author"> Zhixuan Yu, Haozheng Yu, Long Sha, Sujoy Ganguly, and Hyun Soo Park</div> <div class="periodical"> <em>In </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2109.09299.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>This paper presents a new end-to-end semi-supervised framework to learn adense keypoint detector using unlabeled multiview images. A key challenge liesin finding the exact correspondences between the dense keypoints in multipleviews since the inverse of keypoint mapping can be neither analytically derivednor differentiated. This limits applying existing multiview supervisionapproaches on sparse keypoint detection that rely on the exact correspondences.To address this challenge, we derive a new probabilistic epipolar constraintthat encodes the two desired properties. (1) Soft correspondence: we define amatchability, which measures a likelihood of a point matching to the otherimage’s corresponding point, thus relaxing the exact correspondences’requirement. (2) Geometric consistency: every point in the continuouscorrespondence fields must satisfy the multiview consistency collectively. Weformulate a probabilistic epipolar constraint using a weighted average ofepipolar errors through the matchability thereby generalizing thepoint-to-point geometric error to the field-to-field geometric error. Thisgeneralization facilitates learning a geometrically coherent dense keypointdetection model by utilizing a large number of unlabeled multiview images.Additionally, to prevent degenerative cases, we employ a distillation-basedregularization by using a pretrained model. Finally, we design a new neuralnetwork architecture, made of twin networks, that effectively minimizes theprobabilistic epipolar errors of all possible correspondences between two viewimages by building affinity matrices. Our method shows superior performancecompared to existing methods, including non-differentiable bootstrapping interms of keypoint accuracy, multiview consistency, and 3D reconstructionaccuracy.</p> </div> <div class="qing hidden"> <p>提出了软的对应关系通过几何一致性来约束</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">semidense</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semi-supervised Dense Keypointsusing Unlabeled Multiview Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Zhixuan and Yu, Haozheng and Sha, Long and Ganguly, Sujoy and Park, Hyun Soo}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, densepose, semi-supervised}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/densepose"> <i class="fas fa-hashtag fa-sm"></i> densepose</a>  <a href="/tags/semi-supervised"> <i class="fas fa-hashtag fa-sm"></i> semi-supervised</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="FLEX" class="col-sm-8"> <div class="title">FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction</div> <div class="author"> Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, and Daniel Cohen-Or</div> <div class="periodical"> <em>In ECCV</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2105.01937.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://briang13.github.io/FLEX/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The increasing availability of video recordings made by multiple cameras hasoffered new means for mitigating occlusion and depth ambiguities in pose andmotion reconstruction methods. Yet, multi-view algorithms strongly depend oncamera parameters; particularly, the relative transformations between thecameras. Such a dependency becomes a hurdle once shifting to dynamic capture inuncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), anend-to-end extrinsic parameter-free multi-view model. FLEX is extrinsicparameter-free (dubbed ep-free) in the sense that it does not require extrinsiccamera parameters. Our key idea is that the 3D angles between skeletal parts,as well as bone lengths, are invariant to the camera position. Hence, learning3D rotations and bone lengths rather than locations allows predicting commonvalues for all camera views. Our network takes multiple video streams, learnsfused deep features through a novel multi-view fusion layer, and reconstructs asingle consistent skeleton with temporally coherent joint rotations. Wedemonstrate quantitative and qualitative results on three public datasets, andon synthetic multi-person video streams captured by dynamic cameras. We compareour model to state-of-the-art methods that are not ep-free and show that in theabsence of camera parameters, we outperform them by a large margin whileobtaining comparable results when camera parameters are available. Code,trained models, and other materials are available on our project page.</p> </div> <div class="qing hidden"> <p>输入多视角的序列的2D估计，估计脚步接触标签以及骨长、3D旋转，可以不给定相机</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">FLEX</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gordon, Brian and Raab, Sigal and Azov, Guy and Giryes, Raja and Cohen-Or, Daniel}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{3dpose, mv1p}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  <a href="/tags/mv1p"> <i class="fas fa-hashtag fa-sm"></i> mv1p</a>  </div> </div> </li> </ol> <ol class="bibliography"></ol> <h2 class="year">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="AdaFuse" class="col-sm-8"> <div class="title">AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild</div> <div class="author"> Zhe Zhang, Chunyu Wang, Weichao Qiu, Wenhu Qin, and Wenjun Zeng</div> <div class="periodical"> <em>In </em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2010.13302.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/zhezh/adafuse-3d-human-pose" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Occlusion is probably the biggest challenge for human pose estimation in thewild. Typical solutions often rely on intrusive sensors such as IMUs to detectoccluded joints. To make the task truly unconstrained, we present AdaFuse, anadaptive multiview fusion method, which can enhance the features in occludedviews by leveraging those in visible views. The core of AdaFuse is to determinethe point-point correspondence between two views which we solve effectively byexploring the sparsity of the heatmap representation. We also learn an adaptivefusion weight for each camera view to reflect its feature quality in order toreduce the chance that good features are undesirably corrupted by “bad”views. The fusion model is trained end-to-end with the pose estimation network,and can be directly applied to new camera configurations without additionaladaptation. We extensively evaluate the approach on three public datasetsincluding Human3.6M, Total Capture and CMU Panoptic. It outperforms thestate-of-the-arts on all of them. We also create a large scale syntheticdataset Occlusion-Person, which allows us to perform numerical evaluation onthe occluded joints, as it provides occlusion labels for every joint in theimages. The dataset and code are released athttps://github.com/zhezh/adafuse-3d-human-pose.</p> </div> <div class="qing hidden"> <p>同时输入多视角的图像关键点估计heatmap，输出2D关键点</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">AdaFuse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zhe and Wang, Chunyu and Qiu, Weichao and Qin, Wenhu and Zeng, Wenjun}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  </div> </div> </li></ol> <ol class="bibliography"></ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="lrtri" class="col-sm-8"> <div class="title">Learnable Triangulation of Human Pose</div> <div class="author"> Karim Iskakov, Egor Burkov, Victor Lempitsky, and Yury Malkov</div> <div class="periodical"> <em>In </em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/1905.05754" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/karfly/learnable-triangulation-pytorch" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We present two novel solutions for multi-view 3D human pose estimation basedon new learnable triangulation methods that combine 3D information frommultiple 2D views. The first (baseline) solution is a basic differentiablealgebraic triangulation with an addition of confidence weights estimated fromthe input images. The second solution is based on a novel method of volumetricaggregation from intermediate 2D backbone feature maps. The aggregated volumeis then refined via 3D convolutions that produce final 3D joint heatmaps andallow modelling a human pose prior. Crucially, both approaches are end-to-enddifferentiable, which allows us to directly optimize the target metric. Wedemonstrate transferability of the solutions across datasets and considerablyimprove the multi-view state of the art on the Human3.6M dataset. Videodemonstration, annotations and additional materials will be posted on ourproject page (https://saic-violet.github.io/learnable-triangulation).</p> </div> <div class="qing hidden"> <p>多个视角的特征反投影到3D空间中通过3D网络获得最终输出</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lrtri</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learnable Triangulation of Human Pose}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Iskakov, Karim and Burkov, Egor and Lempitsky, Victor and Malkov, Yury}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose, e2e}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  <a href="/tags/e2e"> <i class="fas fa-hashtag fa-sm"></i> e2e</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="crossviewfusion" class="col-sm-8"> <div class="title">Cross View Fusion for 3D Human Pose Estimation</div> <div class="author"> Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, and Wenjun Zeng</div> <div class="periodical"> <em>In </em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1909.01203.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present an approach to recover absolute 3D human poses from multi-viewimages by incorporating multi-view geometric priors in our model. It consistsof two separate steps: (1) estimating the 2D poses in multi-view images and (2)recovering the 3D poses from the multi-view 2D poses. First, we introduce across-view fusion scheme into CNN to jointly estimate 2D poses for multipleviews. Consequently, the 2D pose estimation for each view already benefits fromother views. Second, we present a recursive Pictorial Structure Model torecover the 3D pose from the multi-view 2D poses. It gradually improves theaccuracy of 3D pose with affordable computational cost. We test our method ontwo public datasets H36M and Total Capture. The Mean Per Joint Position Errorson the two datasets are 26mm and 29mm, which outperforms the state-of-the-artsremarkably (26mm vs 52mm, 29mm vs 35mm). Our code is released at\urlhttps://github.com/microsoft/multiview-human-pose-estimation-pytorch.</p> </div> <div class="qing hidden"> <p>直接多视角的融合</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">crossviewfusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cross View Fusion for 3D Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qiu, Haibo and Wang, Chunyu and Wang, Jingdong and Wang, Naiyan and Zeng, Wenjun}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  </div> </div> </li> </ol> <ol class="bibliography"></ol> <h2 class="year">2018</h2> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <h2 class="year">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="harvesting" class="col-sm-8"> <div class="title">Harvesting Multiple Views for Marker-less 3D Human Pose Annotations</div> <div class="author"> <a href="https://geopavlakos.github.io/" target="_blank" rel="noopener noreferrer">Georgios Pavlakos</a>, <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a>, Konstantinos G. Derpanis, and Kostas Daniilidis</div> <div class="periodical"> <em>In </em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1704.04793.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Recent advances with Convolutional Networks (ConvNets) have shifted thebottleneck for many computer vision tasks to annotated data collection. In thispaper, we present a geometry-driven approach to automatically collectannotations for human pose prediction tasks. Starting from a generic ConvNetfor 2D human pose, and assuming a multi-view setup, we describe an automaticway to collect accurate 3D human pose annotations. We capitalize on constraintsoffered by the 3D geometry of the camera setup and the 3D structure of thehuman body to probabilistically combine per view 2D ConvNet predictions into aglobally optimal 3D pose. This 3D pose is used as the basis for harvestingannotations. The benefit of the annotations produced automatically with ourapproach is demonstrated in two challenging settings: (i) fine-tuning a genericConvNet-based 2D pose predictor to capture the discriminative aspects of asubject’s appearance (i.e.,"personalization"), and (ii) training a ConvNet fromscratch for single view 3D human pose prediction without leveraging 3D posegroundtruth. The proposed multi-view pose estimator achieves state-of-the-artresults on standard benchmarks, demonstrating the effectiveness of our methodin exploiting the available multi-view information.</p> </div> <div class="qing hidden"> <p>多视角的heatmap生成了3D特征，然后使用3Dpictorial获取骨架位置</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">harvesting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Harvesting Multiple Views for Marker-less 3D Human Pose Annotations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pavlakos, Georgios and Zhou, Xiaowei and Derpanis, Konstantinos G. and Daniilidis, Kostas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  </div> </div> </li></ol> <ol class="bibliography"></ol> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Qing Shuai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </div></body> </html>