<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Paper Reading of Sida | Qing Shuai | 帅青</title> <meta name="author" content="Qing Shuai"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="chingswy.github.io/reading_sida/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Qing Shuai | 帅青</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Reading of Sida</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/NeRFSLAM.png"> </div> <div id="NeRFSLAM" class="col-sm-8"> <div class="title">NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields</div> <div class="author"> Antoni Rosinol, John J. Leonard, and Luca Carlone</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.13641.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose a novel geometric and photometric 3D mapping pipeline for accurateand real-time scene reconstruction from monocular images. To achieve this, weleverage recent advances in dense monocular SLAM and real-time hierarchicalvolumetric neural radiance fields. Our insight is that dense monocular SLAMprovides the right information to fit a neural radiance field of the scene inreal-time, by providing accurate pose estimates and depth-maps with associateduncertainty. With our proposed uncertainty-based depth loss, we achieve notonly good photometric accuracy, but also great geometric accuracy. In fact, ourproposed pipeline achieves better geometric and photometric accuracy thancompeting approaches (up to 179% better PSNR and 86% better L1 depth), whileworking in real-time and using only monocular images.</p> </div> <div class="sida hidden"> <p>Real-time dense monocular SLAM. 将monocular SLAM和volumetric nerf结合. SLAM提供camera poses, depth maps和uncertainty用于volumetric nerf的训练.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NeRFSLAM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rosinol, Antoni and Leonard, John J. and Carlone, Luca}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Real-time dense monocular SLAM. 将monocular SLAM和volumetric nerf结合.  SLAM提供camera poses, depth maps和uncertainty用于volumetric nerf的训练.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="MPviaIK" class="col-sm-8"> <div class="title">Multi-Person 3D Pose and Shape Estimation via Inverse Kinematics and Refinement</div> <div class="author"> Junuk Cha, Muhammad Saqlain, GeonU Kim, Mingyu Shin, and Seungryul Baek</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.13529.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Estimating 3D poses and shapes in the form of meshes from monocular RGBimages is challenging. Obviously, it is more difficult than estimating 3D posesonly in the form of skeletons or heatmaps. When interacting persons areinvolved, the 3D mesh reconstruction becomes more challenging due to theambiguity introduced by person-to-person occlusions. To tackle the challenges,we propose a coarse-to-fine pipeline that benefits from 1) inverse kinematicsfrom the occlusion-robust 3D skeleton estimation and 2) Transformer-basedrelation-aware refinement techniques. In our pipeline, we first obtainocclusion-robust 3D skeletons for multiple persons from an RGB image. Then, weapply inverse kinematics to convert the estimated skeletons to deformable 3Dmesh parameters. Finally, we apply the Transformer-based mesh refinement thatrefines the obtained mesh parameters considering intra- and inter-personrelations of 3D meshes. Via extensive experiments, we demonstrate theeffectiveness of our method, outperforming state-of-the-arts on 3DPW, MuPoTSand AGORA datasets.</p> </div> <div class="sida hidden"> <p>Multi-person human mesh recovery from monocular images. 预测multi-person skeleton, 然后用inverse kinematics得到SMPL parameters.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MPviaIK</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-Person 3D Pose and Shape Estimation via Inverse Kinematics and Refinement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cha, Junuk and Saqlain, Muhammad and Kim, GeonU and Shin, Mingyu and Baek, Seungryul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Multi-person human mesh recovery from monocular images. 预测multi-person skeleton, 然后用inverse kinematics得到SMPL parameters.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="SUPR" class="col-sm-8"> <div class="title">SUPR: A Sparse Unified Part-Based Human Representation</div> <div class="author"> Ahmed A. A. Osman, Timo Bolkart, Dimitrios Tzionas, and <a href="https://ps.is.tuebingen.mpg.de/person/black" target="_blank" rel="noopener noreferrer">Michael J. Black</a> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.13861.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Statistical 3D shape models of the head, hands, and fullbody are widely usedin computer vision and graphics. Despite their wide use, we show that existingmodels of the head and hands fail to capture the full range of motion for theseparts. Moreover, existing work largely ignores the feet, which are crucial formodeling human movement and have applications in biomechanics, animation, andthe footwear industry. The problem is that previous body part models aretrained using 3D scans that are isolated to the individual parts. Such datadoes not capture the full range of motion for such parts, e.g. the motion ofhead relative to the neck. Our observation is that full-body scans provideimportant information about the motion of the body parts. Consequently, wepropose a new learning scheme that jointly trains a full-body model andspecific part models using a federated dataset of full-body and body-partscans. Specifically, we train an expressive human body model called SUPR(Sparse Unified Part-Based Human Representation), where each joint strictlyinfluences a sparse set of model vertices. The factorized representationenables separating SUPR into an entire suite of body part models. Note that thefeet have received little attention and existing 3D body models have highlyunder-actuated feet. Using novel 4D scans of feet, we train a model with anextended kinematic tree that captures the range of motion of the toes.Additionally, feet deform due to ground contact. To model this, we include anovel non-linear deformation function that predicts foot deformationconditioned on the foot pose, shape, and ground contact. We train SUPR on anunprecedented number of scans: 1.2 million body, head, hand and foot scans. Wequantitatively compare SUPR and the separated body parts and find that oursuite of models generalizes better than existing models. SUPR is available athttp://supr.is.tue.mpg.de</p> </div> <div class="qing hidden"> <p>基于part的人体模型</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SUPR</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SUPR: A Sparse Unified Part-Based Human Representation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Osman, Ahmed A. A. and Bolkart, Timo and Tzionas, Dimitrios and Black, Michael J.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, human-part, human-representation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/human-part"> <i class="fas fa-hashtag fa-sm"></i> human-part</a>  <a href="/tags/human-representation"> <i class="fas fa-hashtag fa-sm"></i> human-representation</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="DeXtreme" class="col-sm-8"> <div class="title">DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality</div> <div class="author"> Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Yashraj Narang, Jean-Francois Lafleche, Dieter Fox, Gavriel State' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.13702.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Recent work has demonstrated the ability of deep reinforcement learning (RL)algorithms to learn complex robotic behaviours in simulation, including in thedomain of multi-fingered manipulation. However, such models can be challengingto transfer to the real world due to the gap between simulation and reality. Inthis paper, we present our techniques to train a) a policy that can performrobust dexterous manipulation on an anthropomorphic robot hand and b) a robustpose estimator suitable for providing reliable real-time information on thestate of the object being manipulated. Our policies are trained to adapt to awide range of conditions in simulation. Consequently, our vision-based policiessignificantly outperform the best vision policies in the literature on the samereorientation task and are competitive with policies that are given privilegedstate information via motion capture systems. Our work reaffirms thepossibilities of sim-to-real transfer for dexterous manipulation in diversekinds of hardware and simulator setups, and in our case, with the Allegro Handand Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities forresearchers to achieve such results with commonly-available, affordable robothands and cameras. Videos of the resulting policy and supplementaryinformation, including experiments and demos, can be found at\urlhttps://dextreme.org/</p> </div> <div class="sida hidden"> <p>Dexterous manipulation. 在大量simulation environments下训练学习policy, 实现sim-to-real transfer.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DeXtreme</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Handa, Ankur and Allshire, Arthur and Makoviychuk, Viktor and Petrenko, Aleksei and Singh, Ritvik and Liu, Jingzhou and Makoviichuk, Denys and Wyk, Karl Van and Zhurkevich, Alexander and Sundaralingam, Balakumar and Narang, Yashraj and Lafleche, Jean-Francois and Fox, Dieter and State, Gavriel}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Dexterous manipulation. 在大量simulation environments下训练学习policy, 实现sim-to-real transfer.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="Motion_Policy_Networks" class="col-sm-8"> <div class="title">Motion Policy Networks</div> <div class="author"> Adam Fishman, Adithyavairan Murali, Clemens Eppner, Bryan Peele, Byron Boots, and Dieter Fox</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.12209.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Collision-free motion generation in unknown environments is a core buildingblock for robot manipulation. Generating such motions is challenging due tomultiple objectives; not only should the solutions be optimal, the motiongenerator itself must be fast enough for real-time performance and reliableenough for practical deployment. A wide variety of methods have been proposedranging from local controllers to global planners, often being combined tooffset their shortcomings. We present an end-to-end neural model called MotionPolicy Networks (M\piNets) to generate collision-free, smooth motion fromjust a single depth camera observation. M\piNets are trained on over 3million motion planning problems in over 500,000 environments. Our experimentsshow that M\piNets are significantly faster than global planners whileexhibiting the reactivity needed to deal with dynamic scenes. They are 46%better than prior neural planners and more robust than local control policies.Despite being only trained in simulation, M\piNets transfer well to the realrobot with noisy partial point clouds. Code and data are publicly available athttps://mpinets.github.io.</p> </div> <div class="sida hidden"> <p>Collision-free motion generation in unknown environments. 提出一个simulation datasets, 训练一个motion policy networks, 从single depth observation预测smooth motion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Motion_Policy_Networks</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Motion Policy Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fishman, Adam and Murali, Adithyavairan and Eppner, Clemens and Peele, Bryan and Boots, Byron and Fox, Dieter}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Collision-free motion generation in unknown environments. 提出一个simulation datasets, 训练一个motion policy networks, 从single depth  observation预测smooth motion.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="Streaming_RF" class="col-sm-8"> <div class="title">Streaming Radiance Fields for 3D Video Synthesis</div> <div class="author"> Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping Tan</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.14831.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present an explicit-grid based method for efficiently reconstructingstreaming radiance fields for novel view synthesis of real world dynamicscenes. Instead of training a single model that combines all the frames, weformulate the dynamic modeling problem with an incremental learning paradigm inwhich per-frame model difference is trained to complement the adaption of abase model on the current frame. By exploiting the simple yet effective tuningstrategy with narrow bands, the proposed method realizes a feasible frameworkfor handling video sequences on-the-fly with high training efficiency. Thestorage overhead induced by using explicit grid representations can besignificantly reduced through the use of model difference based compression. Wealso introduce an efficient strategy to further accelerate model optimizationfor each frame. Experiments on challenging video sequences demonstrate that ourapproach is capable of achieving a training speed of 15 seconds per-frame withcompetitive rendering quality, which attains 1000 \times speedup over thestate-of-the-art implicit methods. Code is available athttps://github.com/AlgoHunt/StreamRF.</p> </div> <div class="sida hidden"> <p>提升训练速度. 提出一个explicit-grid based method, 将3d videos表示为a based model and per-frame model difference.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Streaming_RF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Streaming Radiance Fields for 3D Video Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Lingzhi and Shen, Zhen and Wang, Zhongshu and Shen, Li and Tan, Ping}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{neural-3d-videos}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{提升训练速度. 提出一个explicit-grid based method, 将3d videos表示为a based model and per-frame model difference.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/neural-3d-videos"> <i class="fas fa-hashtag fa-sm"></i> neural-3d-videos</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="boosting_pc_render" class="col-sm-8"> <div class="title">Boosting Point Clouds Rendering via Radiance Mapping</div> <div class="author"> Xiaoyang Huang, Yi Zhang, Bingbing Ni, Teng Li, Kai Chen, and Wenjun Zhang</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.15107.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Recent years we have witnessed rapid development in NeRF-based imagerendering due to its high quality. However, point clouds rendering is somehowless explored. Compared to NeRF-based rendering which suffers from densespatial sampling, point clouds rendering is naturally less computationintensive, which enables its deployment in mobile computing device. In thiswork, we focus on boosting the image quality of point clouds rendering with acompact model design. We first analyze the adaption of the volume renderingformulation on point clouds. Based on the analysis, we simplify the NeRFrepresentation to a spatial mapping function which only requires singleevaluation per pixel. Further, motivated by ray marching, we rectify the thenoisy raw point clouds to the estimated intersection between rays and surfacesas queried coordinates, which could avoid spatial frequency collapse andneighbor point disturbance. Composed of rasterization, spatial mapping and therefinement stages, our method achieves the state-of-the-art performance onpoint clouds rendering, outperforming prior works by notable margins, with asmaller model size. We obtain a PSNR of 31.74 on NeRF-Synthetic, 25.88 onScanNet and 30.81 on DTU. Code and data would be released soon.</p> </div> <div class="sida hidden"> <p>基于point cloud做rendering, 实现每条ray一次evaluation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">boosting_pc_render</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Boosting Point Clouds Rendering via Radiance Mapping}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Xiaoyang and Zhang, Yi and Ni, Bingbing and Li, Teng and Chen, Kai and Zhang, Wenjun}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis, point-cloud}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{基于point cloud做rendering, 实现每条ray一次evaluation.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  <a href="/tags/point-cloud"> <i class="fas fa-hashtag fa-sm"></i> point-cloud</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="SAM-RL" class="col-sm-8"> <div class="title">SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering</div> <div class="author"> Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao, and Cewu Lu</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.15185.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Model-based reinforcement learning (MBRL) is recognized with the potential tobe significantly more sample efficient than model-free RL. How an accuratemodel can be developed automatically and efficiently from raw sensory inputs(such as images), especially for complex environments and tasks, is achallenging problem that hinders the broad application of MBRL in the realworld. In this work, we propose a sensing-aware model-based reinforcementlearning system called SAM-RL. Leveraging the differentiable physics-basedsimulation and rendering, SAM-RL automatically updates the model by comparingrendered images with real raw images and produces the policy efficiently. Withthe sensing-aware learning pipeline, SAM-RL allows a robot to select aninformative viewpoint to monitor the task process. We apply our framework toreal-world experiments for accomplishing three manipulation tasks: roboticassembly, tool manipulation, and deformable object manipulation. We demonstratethe effectiveness of SAM-RL via extensive experiments. Supplemental materialsand videos are available on our project webpage athttps://sites.google.com/view/sam-rl.</p> </div> <div class="sida hidden"> <p>基于differentiable rendering更新environment model, 帮助model-based reinforcement learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SAM-RL</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lv, Jun and Feng, Yunhai and Zhang, Cheng and Zhao, Shuang and Shao, Lin and Lu, Cewu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{manipulation, differentiable-rendering}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{基于differentiable rendering更新environment model, 帮助model-based reinforcement learning.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/manipulation"> <i class="fas fa-hashtag fa-sm"></i> manipulation</a>  <a href="/tags/differentiable-rendering"> <i class="fas fa-hashtag fa-sm"></i> differentiable-rendering</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="NeRFPlayer" class="col-sm-8"> <div class="title">NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields</div> <div class="author"> Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and <a href="https://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Andreas Geiger</a> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.15947.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Visually exploring in a real-world 4D spatiotemporal space freely in VR hasbeen a long-term quest. The task is especially appealing when only a few oreven single RGB cameras are used for capturing the dynamic scene. To this end,we present an efficient framework capable of fast reconstruction, compactmodeling, and streamable rendering. First, we propose to decompose the 4Dspatiotemporal space according to temporal characteristics. Points in the 4Dspace are associated with probabilities of belonging to three categories:static, deforming, and new areas. Each area is represented and regularized by aseparate neural field. Second, we propose a hybrid representations basedfeature streaming scheme for efficiently modeling the neural fields. Ourapproach, coined NeRFPlayer, is evaluated on dynamic scenes captured by singlehand-held cameras and multi-camera arrays, achieving comparable or superiorrendering performance in terms of quality and speed comparable to recentstate-of-the-art methods, achieving reconstruction in 10 seconds per frame andreal-time rendering.</p> </div> <div class="sida hidden"> <p>用一个hybrid representation-based feature streaming scheme表示动态场景.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NeRFPlayer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Song, Liangchen and Chen, Anpei and Li, Zhong and Chen, Zhang and Chen, Lele and Yuan, Junsong and Xu, Yi and Geiger, Andreas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{neural-3d-videos}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{用一个hybrid representation-based  feature streaming scheme表示动态场景.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/neural-3d-videos"> <i class="fas fa-hashtag fa-sm"></i> neural-3d-videos</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="ProbNerf" class="col-sm-8"> <div class="title">ProbNeRF: Uncertainty-Aware Inference of 3D Shapes from 2D Images</div> <div class="author"> Matthew D. Hoffman, Tuan Anh Le, Pavel Sountsov, Christopher Suter, Ben Lee, Vikash K. Mansinghka, and Rif A. Saurous</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.17415.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>The problem of inferring object shape from a single 2D image isunderconstrained. Prior knowledge about what objects are plausible can help,but even given such prior knowledge there may still be uncertainty about theshapes of occluded parts of objects. Recently, conditional neural radiancefield (NeRF) models have been developed that can learn to infer good pointestimates of 3D models from single 2D images. The problem of inferringuncertainty estimates for these models has received less attention. In thiswork, we propose probabilistic NeRF (ProbNeRF), a model and inference strategyfor learning probabilistic generative models of 3D objects’ shapes andappearances, and for doing posterior inference to recover those properties from2D images. ProbNeRF is trained as a variational autoencoder, but at test timewe use Hamiltonian Monte Carlo (HMC) for inference. Given one or a few 2Dimages of an object (which may be partially occluded), ProbNeRF is able notonly to accurately model the parts it sees, but also to propose realistic anddiverse hypotheses about the parts it does not see. We show that key to thesuccess of ProbNeRF are (i) a deterministic rendering scheme, (ii) anannealed-HMC strategy, (iii) a hypernetwork-based decoder architecture, and(iv) doing inference over a full set of NeRF weights, rather than just alow-dimensional code.</p> </div> <div class="sida hidden"> <p>Single-view shape reconstruction. 用variational autoencoder训练一个generative model, 用Monte Carlo预测图片的latent vector.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ProbNerf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ProbNeRF: Uncertainty-Aware Inference of 3D Shapes from 2D Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hoffman, Matthew D. and Le, Tuan Anh and Sountsov, Pavel and Suter, Christopher and Lee, Ben and Mansinghka, Vikash K. and Saurous, Rif A.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="gCoRF" class="col-sm-8"> <div class="title">gCoRF: Generative Compositional Radiance Fields</div> <div class="author"> Mallikarjun BR, Ayush Tewari, Xingang Pan, Mohamed Elgharib, and <a href="https://vcai.mpi-inf.mpg.de/VCAI_Projects.html" target="_blank" rel="noopener noreferrer">Christian Theobalt</a> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.17344.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>3D generative models of objects enable photorealistic image synthesis with 3Dcontrol. Existing methods model the scene as a global scene representation,ignoring the compositional aspect of the scene. Compositional reasoning canenable a wide variety of editing applications, in addition to enablinggeneralizable 3D reasoning. In this paper, we present a compositionalgenerative model, where each semantic part of the object is represented as anindependent 3D representation learned from only in-the-wild 2D data. We startwith a global generative model (GAN) and learn to decompose it into differentsemantic parts using supervision from 2D segmentation masks. We then learn tocomposite independently sampled parts in order to create coherent globalscenes. Different parts can be independently sampled while keeping the rest ofthe object fixed. We evaluate our method on a wide variety of objects and partsand demonstrate editing applications.</p> </div> <div class="sida hidden"> <p>Editable 3D-aware generative model. 先学习一个global generative model, 然后通过semantic mask解藕为多个semantic part的generative models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gCoRF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{gCoRF: Generative Compositional Radiance Fields}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{BR, Mallikarjun and Tewari, Ayush and Pan, Xingang and Elgharib, Mohamed and Theobalt, Christian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Editable 3D-aware generative model. 先学习一个global generative model, 然后通过semantic mask解藕为多个semantic part的generative models.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="UmeTrack" class="col-sm-8"> <div class="title">UmeTrack: Unified multi-view end-to-end hand tracking for VR</div> <div class="author"> Shangchen Han, Po-chen Wu, Yubo Zhang, Beibei Liu, Linguang Zhang, Zheng Wang, Weiguang Si, Peizhao Zhang, Yujun Cai, Tomas Hodan, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Randi Cabezas, Luan Tran, Muzaffer Akbay, Tsz-Ho Yu, Cem Keskin, Robert Wang' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.00099.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Real-time tracking of 3D hand pose in world space is a challenging problemand plays an important role in VR interaction. Existing work in this space arelimited to either producing root-relative (versus world space) 3D pose or relyon multiple stages such as generating heatmaps and kinematic optimization toobtain 3D pose. Moreover, the typical VR scenario, which involves multi-viewtracking from wide \acfov cameras is seldom addressed by these methods. Inthis paper, we present a unified end-to-end differentiable framework formulti-view, multi-frame hand tracking that directly predicts 3D hand pose inworld space. We demonstrate the benefits of end-to-end differentiabilty byextending our framework with downstream tasks such as jitter reduction andpinch prediction. To demonstrate the efficacy of our model, we further presenta new large-scale egocentric hand pose dataset that consists of both real andsynthetic data. Experiments show that our system trained on this datasethandles various challenging interactive motions, and has been successfullyapplied to real-time VR applications.</p> </div> <div class="sida hidden"> <p>Multi-view hand pose tracking. 用VR眼镜的相机预测3D hand pose in world space. 用网络预测. 提出了一个数据集.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">UmeTrack</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{UmeTrack: Unified multi-view end-to-end hand tracking for VR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Shangchen and Wu, Po-chen and Zhang, Yubo and Liu, Beibei and Zhang, Linguang and Wang, Zheng and Si, Weiguang and Zhang, Peizhao and Cai, Yujun and Hodan, Tomas and Cabezas, Randi and Tran, Luan and Akbay, Muzaffer and Yu, Tsz-Ho and Keskin, Cem and Wang, Robert}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{hand-pose}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Multi-view hand pose tracking. 用VR眼镜的相机预测3D hand pose in world space. 用网络预测. 提出了一个数据集.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/hand-pose"> <i class="fas fa-hashtag fa-sm"></i> hand-pose</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="VIINTER" class="col-sm-8"> <div class="title">VIINTER: View Interpolation with Implicit Neural Representations of Images</div> <div class="author"> Brandon Yushan Feng, Susmija Jabbireddy, and Amitabh Varshney</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.00722.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present VIINTER, a method for view interpolation by interpolating theimplicit neural representation (INR) of the captured images. We leverage thelearned code vector associated with each image and interpolate between thesecodes to achieve viewpoint transitions. We propose several techniques thatsignificantly enhance the interpolation quality. VIINTER signifies a new way toachieve view interpolation without constructing 3D structure, estimating cameraposes, or computing pixel correspondence. We validate the effectiveness ofVIINTER on several multi-view scenes with different types of camera layout andscene composition. As the development of INR of images (as opposed to surfaceor volume) has centered around tasks like image fitting and super-resolution,with VIINTER, we show its capability for view interpolation and offer apromising outlook on using INR for image manipulation tasks.</p> </div> <div class="sida hidden"> <p>用INR和latent vectors记录多张图片, 通过插值latent vectors实现view interpolation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">VIINTER</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VIINTER: View Interpolation with Implicit Neural Representations of Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Feng, Brandon Yushan and Jabbireddy, Susmija and Varshney, Amitabh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{用INR和latent vectors记录多张图片, 通过插值latent vectors实现view interpolation.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="nerf2nerf" class="col-sm-8"> <div class="title">nerf2nerf: Pairwise Registration of Neural Radiance Fields</div> <div class="author"> Lily Goli, Daniel Rebain, Sara Sabour, Animesh Garg, and Andrea Tagliasacchi</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.01600.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We introduce a technique for pairwise registration of neural fields thatextends classical optimization-based local registration (i.e. ICP) to operateon Neural Radiance Fields (NeRF) – neural 3D scene representations trainedfrom collections of calibrated images. NeRF does not decompose illumination andcolor, so to make registration invariant to illumination, we introduce theconcept of a ”surface field” – a field distilled from a pre-trained NeRFmodel that measures the likelihood of a point being on the surface of anobject. We then cast nerf2nerf registration as a robust optimization thatiteratively seeks a rigid transformation that aligns the surface fields of thetwo scenes. We evaluate the effectiveness of our technique by introducing adataset of pre-trained NeRF scenes – our synthetic scenes enable quantitativeevaluations and comparisons to classical registration techniques, while ourreal scenes demonstrate the validity of our technique in real-world scenarios.Additional results available at: https://nerf2nerf.github.io</p> </div> <div class="sida hidden"> <p>Registration of neural radiance fields. 从nerf中提取surface fields, 然后求解两个surface fields的rigid transformation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nerf2nerf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{nerf2nerf: Pairwise Registration of Neural Radiance Fields}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goli, Lily and Rebain, Daniel and Sabour, Sara and Garg, Animesh and Tagliasacchi, Andrea}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{registration, nerf}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Registration of neural radiance fields. 从nerf中提取surface fields, 然后求解两个surface fields的rigid transformation.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/registration"> <i class="fas fa-hashtag fa-sm"></i> registration</a>  <a href="/tags/nerf"> <i class="fas fa-hashtag fa-sm"></i> nerf</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="FactorMatte" class="col-sm-8"> <div class="title">FactorMatte: Redefining Video Matting for Re-Composition Tasks</div> <div class="author"> Zeqi Gu, Wenqi Xian, Noah Snavely, and Abe Davis</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.02145.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose "factor matting", an alternative formulation of the video mattingproblem in terms of counterfactual video synthesis that is better suited forre-composition tasks. The goal of factor matting is to separate the contents ofvideo into independent components, each visualizing a counterfactual version ofthe scene where contents of other components have been removed. We show thatfactor matting maps well to a more general Bayesian framing of the mattingproblem that accounts for complex conditional interactions between layers.Based on this observation, we present a method for solving the factor mattingproblem that produces useful decompositions even for video with complexcross-layer interactions like splashes, shadows, and reflections. Our method istrained per-video and requires neither pre-training on external large datasets,nor knowledge about the 3D structure of the scene. We conduct extensiveexperiments, and show that our method not only can disentangle scenes withcomplex interactions, but also outperforms top methods on existing tasks suchas classical video matting and background subtraction. In addition, wedemonstrate the benefits of our approach on a range of downstream tasks. Pleaserefer to our project webpage for more details: https://factormatte.github.io</p> </div> <div class="sida hidden"> <p>Factor matting of videos. 将video中的各部分内容decompose出来. 用一个网络预测图片中的各个component. 通过reconstruction loss训练网络. 通过foreground discriminator来regularize training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">FactorMatte</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FactorMatte: Redefining Video Matting for Re-Composition Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Zeqi and Xian, Wenqi and Snavely, Noah and Davis, Abe}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{matting}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Factor matting of videos. 将video中的各部分内容decompose出来. 用一个网络预测图片中的各个component. 通过reconstruction loss训练网络. 通过foreground discriminator来regularize training.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/matting"> <i class="fas fa-hashtag fa-sm"></i> matting</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="TextCraft" class="col-sm-8"> <div class="title">TextCraft: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Text</div> <div class="author"> Aditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman Shayani, Amir Hosein Khasahmadi, Srinath Sridhar, and Daniel Ritchie</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.01427.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Language is one of the primary means by which we describe the 3D world aroundus. While rapid progress has been made in text-to-2D-image synthesis, similarprogress in text-to-3D-shape synthesis has been hindered by the lack of paired(text, shape) data. Moreover, extant methods for text-to-shape generation havelimited shape diversity and fidelity. We introduce TextCraft, a method toaddress these limitations by producing high-fidelity and diverse 3D shapeswithout the need for (text, shape) pairs for training. TextCraft achieves thisby using CLIP and using a multi-resolution approach by first generating in alow-dimensional latent space and then upscaling to a higher resolution,improving the fidelity of the generated shape. To improve shape diversity, weuse a discrete latent space which is modelled using a bidirectional transformerconditioned on the interchangeable image-text embedding space induced by CLIP.Moreover, we present a novel variant of classifier-free guidance, which furtherimproves the accuracy-diversity trade-off. Finally, we perform extensiveexperiments that demonstrate that TextCraft outperforms state-of-the-artbaselines.</p> </div> <div class="sida hidden"> <p>Text-to-shape generation without training on paired data. 在一组3D shape上训练. 训练基于CLIP的image feature生成shape的网络. 从而实现基于CLIP的texture feature生成shape.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TextCraft</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TextCraft: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Text}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sanghi, Aditya and Fu, Rao and Liu, Vivian and Willis, Karl and Shayani, Hooman and Khasahmadi, Amir Hosein and Sridhar, Srinath and Ritchie, Daniel}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{text-to-shape}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{Text-to-shape generation without training on paired data. 在一组3D shape上训练. 训练基于CLIP的image feature生成shape的网络. 从而实现基于CLIP的texture feature生成shape.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/text-to-shape"> <i class="fas fa-hashtag fa-sm"></i> text-to-shape</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/QRF.png"> </div> <div id="QRF" class="col-sm-8"> <div class="title">QRF: Implicit Neural Representations with Quantum Radiance Fields</div> <div class="author"> YuanFu Yang, and Min Sun</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.03418.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Photorealistic rendering of real-world scenes is a tremendous challenge witha wide range of applications, including MR (Mixed Reality), and VR (MixedReality). Neural networks, which have long been investigated in the context ofsolving differential equations, have previously been introduced as implicitrepresentations for Photorealistic rendering. However, realistic renderingusing classic computing is challenging because it requires time-consumingoptical ray marching, and suffer computational bottlenecks due to the curse ofdimensionality. In this paper, we propose Quantum Radiance Fields (QRF), whichintegrate the quantum circuit, quantum activation function, and quantum volumerendering for implicit scene representation. The results indicate that QRF notonly takes advantage of the merits of quantum computing technology such as highspeed, fast convergence, and high parallelism, but also ensure high quality ofvolume rendering.</p> </div> <div class="sida hidden"> <p>通过quantum techniques加速nerf.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">QRF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{QRF: Implicit Neural Representations with Quantum Radiance Fields}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, YuanFu and Sun, Min}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{nerf-speed}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{通过quantum techniques加速nerf.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/nerf-speed"> <i class="fas fa-hashtag fa-sm"></i> nerf-speed</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/ParticleNeRF.png"> </div> <div id="ParticleNeRF" class="col-sm-8"> <div class="title">ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields in Dynamic Scenes</div> <div class="author"> Jad Abou-Chakra, Feras Dayoub, and Niko Sünderhauf</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.04041.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Neural Radiance Fields (NeRFs) learn implicit representations of - typicallystatic - environments from images. Our paper extends NeRFs to handle dynamicscenes in an online fashion. We propose ParticleNeRF that adapts to changes inthe geometry of the environment as they occur, learning a new up-to-daterepresentation every 350 ms. ParticleNeRF can represent the current state ofdynamic environments with much higher fidelity as other NeRF frameworks. Toachieve this, we introduce a new particle-based parametric encoding, whichallows the intermediate NeRF features - now coupled to particles in space - tomove with the dynamic geometry. This is possible by backpropagating thephotometric reconstruction loss into the position of the particles. Theposition gradients are interpreted as particle velocities and integrated intopositions using a position-based dynamics (PBS) physics system. Introducing PBSinto the NeRF formulation allows us to add collision constraints to theparticle motion and creates future opportunities to add other movement priorsinto the system, such as rigid and deformable body</p> </div> <div class="sida hidden"> <p>提出了particle-based parametric encoding, 将features anchor在dynamic geometry, 实现view synthesis of dynamic scenes. 用position-based dynamics physics system表示dynamic geometry.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ParticleNeRF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields in Dynamic Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Abou-Chakra, Jad and Dayoub, Feras and Sünderhauf, Niko}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis, dynamic-scene}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{提出了particle-based parametric encoding, 将features anchor在dynamic geometry, 实现view synthesis of dynamic scenes. 用position-based dynamics physics system表示dynamic geometry.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  <a href="/tags/dynamic-scene"> <i class="fas fa-hashtag fa-sm"></i> dynamic-scene</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="EditableIndoorLight" class="col-sm-8"> <div class="title">Editable Indoor Lighting Estimation</div> <div class="author"> Henrique Weber, Mathieu Garon, and Jean-François Lalonde</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.03928.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present a method for estimating lighting from a single perspective imageof an indoor scene. Previous methods for predicting indoor illumination usuallyfocus on either simple, parametric lighting that lack realism, or on richerrepresentations that are difficult or even impossible to understand or modifyafter prediction. We propose a pipeline that estimates a parametric light thatis easy to edit and allows renderings with strong shadows, alongside with anon-parametric texture with high-frequency information necessary for realisticrendering of specular objects. Once estimated, the predictions obtained withour model are interpretable and can easily be modified by an artist/user with afew mouse clicks. Quantitative and qualitative results show that our approachmakes indoor lighting estimation easier to handle by a casual user, while stillproducing competitive results.</p> </div> <div class="sida hidden"> <p>提出了一种parametric light, 支持realistic rendering和编辑.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">EditableIndoorLight</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Editable Indoor Lighting Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Weber, Henrique and Garon, Mathieu and Lalonde, Jean-François}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{editable, lighting}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{提出了一种parametric light, 支持realistic rendering和编辑.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/editable"> <i class="fas fa-hashtag fa-sm"></i> editable</a>  <a href="/tags/lighting"> <i class="fas fa-hashtag fa-sm"></i> lighting</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Common_Pets.png"> </div> <div id="Common_Pets" class="col-sm-8"> <div class="title">Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories</div> <div class="author"> Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ignacio Rocco, Natalia Neverova, Andrea Vedaldi, and David Novotny</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.03889.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Obtaining photorealistic reconstructions of objects from sparse views isinherently ambiguous and can only be achieved by learning suitablereconstruction priors. Earlier works on sparse rigid object reconstructionsuccessfully learned such priors from large datasets such as CO3D. In thispaper, we extend this approach to dynamic objects. We use cats and dogs as arepresentative example and introduce Common Pets in 3D (CoP3D), a collection ofcrowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of thefirst large-scale datasets for benchmarking non-rigid 3D reconstruction "in thewild". We also propose Tracker-NeRF, a method for learning 4D reconstructionfrom our dataset. At test time, given a small number of video frames of anunseen object, Tracker-NeRF predicts the trajectories of its 3D points andgenerates new views, interpolating viewpoint and time. Results on CoP3D revealsignificantly better non-rigid new-view synthesis performance than existingbaselines.</p> </div> <div class="sida hidden"> <p>提出了一个deformable objects的数据集, 用于训练一个网络, 从几帧图像中重建出deformable objects.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Common_Pets</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sinha, Samarth and Shapovalov, Roman and Reizenstein, Jeremy and Rocco, Ignacio and Neverova, Natalia and Vedaldi, Andrea and Novotny, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{monocular-video, deformable-objects, view-synthesis}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{提出了一个deformable objects的数据集, 用于训练一个网络, 从几帧图像中重建出deformable objects.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/monocular-video"> <i class="fas fa-hashtag fa-sm"></i> monocular-video</a>  <a href="/tags/deformable-objects"> <i class="fas fa-hashtag fa-sm"></i> deformable-objects</a>  <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="Learning_Visual_Locomotion" class="col-sm-8"> <div class="title">Learning Visual Locomotion with Cross-Modal Supervision</div> <div class="author"> Antonio Loquercio, Ashish Kumar, and Jitendra Malik</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.03785.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>In this work, we show how to learn a visual walking policy that only uses amonocular RGB camera and proprioception. Since simulating RGB is hard, wenecessarily have to learn vision in the real world. We start with a blindwalking policy trained in simulation. This policy can traverse some terrains inthe real world but often struggles since it lacks knowledge of the upcominggeometry. This can be resolved with the use of vision. We train a visual modulein the real world to predict the upcoming terrain with our proposed algorithmCross-Modal Supervision (CMS). CMS uses time-shifted proprioception tosupervise vision and allows the policy to continually improve with morereal-world experience. We evaluate our vision-based walking policy over adiverse set of terrains including stairs (up to 19cm high), slippery slopes(inclination of 35 degrees), curbs and tall steps (up to 20cm), and complexdiscrete terrains. We achieve this performance with less than 30 minutes ofreal-world data. Finally, we show that our policy can adapt to shifts in thevisual field with a limited amount of real-world experience. Video results andcode at https://antonilo.github.io/vision_locomotion/.</p> </div> <div class="sida hidden"> <p>在walking robotics中加入了visual module, 学习处理upcoming geometry. 通过proprioception在real world中训练这个visual module.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Learning_Visual_Locomotion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Visual Locomotion with Cross-Modal Supervision}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Loquercio, Antonio and Kumar, Ashish and Malik, Jitendra}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{visual-locomotion}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{在walking robotics中加入了visual module, 学习处理upcoming geometry. 通过proprioception在real world中训练这个visual module.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/visual-locomotion"> <i class="fas fa-hashtag fa-sm"></i> visual-locomotion</a>  </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Temporal-MPI.png"> </div> <div id="Temporal-MPI" class="col-sm-8"> <div class="title">Temporal-MPI: Enabling Multi-Plane Images for Dynamic Scene Modelling via Temporal Basis Learning</div> <div class="author"> Wenpeng Xing, and Jie Chen</div> <div class="periodical"> <em>In </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="sida btn btn-sm z-depth-0" role="button">Sida</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.10533.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Novel view synthesis of static scenes has achieved remarkable advancements inproducing photo-realistic results. However, key challenges remain for immersiverendering of dynamic scenes. One of the seminal image-based rendering method,the multi-plane image (MPI), produces high novel-view synthesis quality forstatic scenes. But modelling dynamic contents by MPI is not studied. In thispaper, we propose a novel Temporal-MPI representation which is able to encodethe rich 3D and dynamic variation information throughout the entire video ascompact temporal basis and coefficients jointly learned. Time-instance MPI forrendering can be generated efficiently using mini-seconds by linearcombinations of temporal basis and coefficients from Temporal-MPI. Thusnovel-views at arbitrary time-instance will be able to be rendered viaTemporal-MPI in real-time with high visual quality. Our method is trained andevaluated on Nvidia Dynamic Scene Dataset. We show that our proposed Temporal-MPI is much faster and more compact compared with other state-of-the-artdynamic scene modelling methods.</p> </div> <div class="sida hidden"> <p>构建一组MPI basis和coefficients, 通过linear combinations得到每一帧的MPI, 实现dynamic view synthesis.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Temporal-MPI</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Temporal-MPI: Enabling Multi-Plane Images for Dynamic Scene Modelling via Temporal Basis Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xing, Wenpeng and Chen, Jie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis, dynamic-scene}</span><span class="p">,</span>
  <span class="na">sida</span> <span class="p">=</span> <span class="s">{构建一组MPI basis和coefficients, 通过linear combinations得到每一帧的MPI, 实现dynamic view synthesis.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  <a href="/tags/dynamic-scene"> <i class="fas fa-hashtag fa-sm"></i> dynamic-scene</a>  </div> </div> </li></ol> <h2 class="year">2020</h2> <ol class="bibliography"></ol> <h2 class="year">2019</h2> <ol class="bibliography"></ol> <h2 class="year">2018</h2> <ol class="bibliography"></ol> <h2 class="year">2017</h2> <ol class="bibliography"></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Qing Shuai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>