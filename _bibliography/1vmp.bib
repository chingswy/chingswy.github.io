
@article{mehta_single-shot_2017,
	title = {Single-Shot Multi-Person 3D Pose Estimation From Monocular {RGB}},
	url = {http://arxiv.org/abs/1712.03453},
	abstract = {We propose a new single-shot method for multiperson 3D pose estimation in general scenes from a monocular {RGB} camera. Our approach uses novel occlusion-robust pose-maps ({ORPM}) which enable full body pose inference even under strong partial occlusions by other people and objects in the scene. {ORPM} outputs a ﬁxed number of maps which encode the 3D joint locations of all people in the scene. Body part associations [8] allow us to infer 3D pose for an arbitrary number of people without explicit bounding box prediction. To train our approach we introduce {MuCo}3DHP, the ﬁrst large scale training data set showing real images of sophisticated multi-person interactions and occlusions. We synthesize a large corpus of multiperson images by compositing images of individual people (with ground truth from mutli-view performance capture). We evaluate our method on our new challenging 3D annotated multi-person test set {MuPoTs}3D where we achieve state-of-the-art performance. To further stimulate research in multi-person 3D pose estimation, we will make our new datasets, and associated code publicly available for research purposes.},
	journaltitle = {{arXiv}:1712.03453 [cs]},
	author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Sridhar, Srinath and Pons-Moll, Gerard and Theobalt, Christian},
	urldate = {2019-09-16},
	date = {2017-12-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.03453},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Mehta et al. - 2017 - Single-Shot Multi-Person 3D Pose Estimation From M.pdf:/Users/fangqi/Zotero/storage/IUWH3SIK/Mehta et al. - 2017 - Single-Shot Multi-Person 3D Pose Estimation From M.pdf:application/pdf},
}

@article{moon_camera_2019,
	title = {Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single {RGB} Image},
	url = {http://arxiv.org/abs/1907.11346},
	abstract = {Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distance-aware top-down approach for 3D multi-person pose estimation from a single {RGB} image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any groundtruth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets. The code is available in https://github.com/mks0601/3DMPPE\_ROOTNET\_RELEASE , https://github.com/mks0601/3DMPPE\_POSENET\_RELEASE.},
	journaltitle = {{arXiv}:1907.11346 [cs]},
	author = {Moon, Gyeongsik and Chang, Ju Yong and Lee, Kyoung Mu},
	urldate = {2019-09-16},
	date = {2019-07-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.11346},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Moon et al. - 2019 - Camera Distance-aware Top-down Approach for 3D Mul.pdf:/Users/fangqi/Zotero/storage/G2HK7VIH/Moon et al. - 2019 - Camera Distance-aware Top-down Approach for 3D Mul.pdf:application/pdf},
}

@article{veges_absolute_2019,
	title = {Absolute Human Pose Estimation with Depth Prediction Network},
	url = {http://arxiv.org/abs/1904.05947},
	abstract = {The common approach to 3D human pose estimation is predicting the body joint coordinates relative to the hip. This works well for a single person but is insufﬁcient in the case of multiple interacting people. Methods predicting absolute coordinates ﬁrst estimate a root-relative pose then calculate the translation via a secondary optimization task. We propose a neural network that predicts joints in a camera centered coordinate system instead of a root-relative one. Unlike previous methods, our network works in a single step without any postprocessing. Our network beats previous methods on the {MuPoTS}3D dataset and achieves state-of-the-art results.},
	journaltitle = {{arXiv}:1904.05947 [cs]},
	author = {Véges, Márton and Lőrincz, András},
	urldate = {2019-09-16},
	date = {2019-04-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.05947},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Véges and Lőrincz - 2019 - Absolute Human Pose Estimation with Depth Predicti.pdf:/Users/fangqi/Zotero/storage/JJTF47F4/Véges and Lőrincz - 2019 - Absolute Human Pose Estimation with Depth Predicti.pdf:application/pdf},
}

@article{zanfir_deep_nodate,
	title = {Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images},
	abstract = {We present {MubyNet} – a feed-forward, multitask, bottom up system for the integrated localization, as well as 3d pose and shape estimation, of multiple people in monocular images. The challenge is the formal modeling of the problem that intrinsically requires discrete and continuous computation, e.g. grouping people vs. predicting 3d pose. The model identiﬁes human body structures (joints and limbs) in images, groups them based on 2d and 3d information fused using learned scoring functions, and optimally aggregates such responses into partial or complete 3d human skeleton hypotheses under kinematic tree constraints, but without knowing in advance the number of people in the scene and their visibility relations. We design a multi-task deep neural network with differentiable stages where the person grouping problem is formulated as an integer program based on learned body part scores parameterized by both 2d and 3d information. This avoids suboptimality resulting from separate 2d and 3d reasoning, with grouping performed based on the combined representation. The ﬁnal stage of 3d pose and shape prediction is based on a learned attention process where information from different human body parts is optimally integrated. State-of-the-art results are obtained in large scale datasets like Human3.6M and Panoptic, and qualitatively by reconstructing the 3d shape and pose of multiple people, under occlusion, in difﬁcult monocular images.},
	pages = {10},
	author = {Zanfir, Andrei and Marinoiu, Elisabeta and Zanfir, Mihai and Popa, Alin-Ionut and Sminchisescu, Cristian},
	langid = {english},
	file = {Zanfir et al. - Deep Network for the Integrated 3D Sensing of Mult.pdf:/Users/fangqi/Zotero/storage/8XQY8N78/Zanfir et al. - Deep Network for the Integrated 3D Sensing of Mult.pdf:application/pdf},
}

@article{rogez_lcr-net++:_2019,
	title = {{LCR}-Net++: Multi-person 2D and 3D Pose Detection in Natural Images},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1803.00455},
	doi = {10.1109/TPAMI.2019.2892985},
	shorttitle = {{LCR}-Net++},
	abstract = {We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D poses of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our Localization-Classiﬁcation-Regression architecture, named {LCR}-Net, contains 3 main components: 1) the pose proposal generator that suggests candidate poses at different locations in the image; 2) a classiﬁer that scores the different pose proposals; and 3) a regressor that reﬁnes pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The ﬁnal pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our method recovers full-body 2D and 3D poses, hallucinating plausible body parts when the persons are partially occluded or truncated by the image boundary. Our approach signiﬁcantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the {MPII} 2D pose benchmark and demonstrates satisfying 3D pose results even for multi-person images.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Rogez, Gregory and Weinzaepfel, Philippe and Schmid, Cordelia},
	urldate = {2019-09-16},
	date = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.00455},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Rogez et al. - 2019 - LCR-Net++ Multi-person 2D and 3D Pose Detection i.pdf:/Users/fangqi/Zotero/storage/XTRBJKLM/Rogez et al. - 2019 - LCR-Net++ Multi-person 2D and 3D Pose Detection i.pdf:application/pdf},
}

@inproceedings{zanfir_monocular_2018,
	location = {Salt Lake City, {UT}, {USA}},
	title = {Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance of Multiple Scene Constraints},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578327/},
	doi = {10.1109/CVPR.2018.00229},
	shorttitle = {Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes},
	abstract = {Human sensing has greatly beneﬁted from recent advances in deep learning, parametric human modeling, and large scale 2d and 3d datasets. However, existing 3d models make strong assumptions about the scene, considering either a single person per image, full views of the person, a simple background or many cameras. In this paper, we leverage state-of-the-art deep multi-task neural networks and parametric human and scene modeling, towards a fully automatic monocular visual sensing system for multiple interacting people, which (i) infers the 2d and 3d pose and shape of multiple people from a single image, relying on detailed semantic representations at both model and image level, to guide a combined optimization with feedforward and feedback components, (ii) automatically integrates scene constraints including ground plane support and simultaneous volume occupancy by multiple people, and (iii) extends the single image model to video by optimally solving the temporal person assignment problem and imposing coherent temporal pose and motion reconstructions while preserving image alignment ﬁdelity. We perform experiments on both single and multi-person datasets, and systematically evaluate each component of the model, showing improved performance and extensive multiple human sensing capability. We also apply our method to images with multiple people, severe occlusions and diverse backgrounds captured in challenging natural scenes, and obtain results of good perceptual quality.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2148--2157},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Zanfir, Andrei and Marinoiu, Elisabeta and Sminchisescu, Cristian},
	urldate = {2019-09-16},
	date = {2018-06},
	langid = {english},
	file = {Zanfir et al. - 2018 - Monocular 3D Pose and Shape Estimation of Multiple.pdf:/Users/fangqi/Zotero/storage/UEF6KAVA/Zanfir et al. - 2018 - Monocular 3D Pose and Shape Estimation of Multiple.pdf:application/pdf},
}

@inproceedings{rogez_lcr-net:_2017,
	location = {Honolulu, {HI}},
	title = {{LCR}-Net: Localization-Classification-Regression for Human Pose},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099617/},
	doi = {10.1109/CVPR.2017.134},
	shorttitle = {{LCR}-Net},
	abstract = {We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D pose of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our architecture, named {LCRNet}, contains 3 main components: 1) the pose proposal generator that suggests potential poses at different locations in the image; 2) a classiﬁer that scores the different pose proposals; and 3) a regressor that reﬁnes pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The ﬁnal pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our approach signiﬁcantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multiperson subsets of the {MPII} 2D pose benchmark.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1216--1224},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Rogez, Gregory and Weinzaepfel, Philippe and Schmid, Cordelia},
	urldate = {2019-09-16},
	date = {2017-07},
	langid = {english},
	file = {Rogez et al. - 2017 - LCR-Net Localization-Classification-Regression fo.pdf:/Users/fangqi/Zotero/storage/3ZZQRMW6/Rogez et al. - 2017 - LCR-Net Localization-Classification-Regression fo.pdf:application/pdf},
}

@article{dabral_multi-person_2019,
	title = {Multi-Person 3D Human Pose Estimation from Monocular Images},
	url = {http://arxiv.org/abs/1909.10854},
	abstract = {Multi-person 3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose {HG}-{RCNN}, a Mask-{RCNN} based network that also leverages the beneﬁts of the Hourglass architecture for multiperson 3D Human Pose Estimation. A two-staged approach is presented that ﬁrst estimates the 2D keypoints in every Region of Interest ({RoI}) and then lifts the estimated keypoints to 3D. Finally, the estimated 3D poses are placed in camera-coordinates using weak-perspective projection assumption and joint optimization of focal length and root translations. The result is a simple and modular network for multi-person 3D human pose estimation that does not require any multi-person 3D pose dataset. Despite its simple formulation, {HG}-{RCNN} achieves the state-of-the-art results on {MuPoTS}-3D while also approximating the 3D pose in the camera-coordinate system.},
	journaltitle = {{arXiv}:1909.10854 [cs]},
	author = {Dabral, Rishabh and Gundavarapu, Nitesh B. and Mitra, Rahul and Sharma, Abhishek and Ramakrishnan, Ganesh and Jain, Arjun},
	urldate = {2019-10-02},
	date = {2019-09-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.10854},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Dabral et al. - 2019 - Multi-Person 3D Human Pose Estimation from Monocul.pdf:/Users/fangqi/Zotero/storage/ZLU3G66R/Dabral et al. - 2019 - Multi-Person 3D Human Pose Estimation from Monocul.pdf:application/pdf},
}

@article{fabbri_compressed_nodate,
	title = {Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation},
	abstract = {In this paper we present a novel approach for bottomup multi-person 3D human pose estimation from monocular {RGB} images. We propose to use high resolution volumetric heatmaps to model joint locations, devising a simple and effective compression method to drastically reduce the size of this representation. At the core of the proposed method lies our Volumetric Heatmap Autoencoder, a fully-convolutional network tasked with the compression of ground-truth heatmaps into a dense intermediate representation. A second model, the Code Predictor, is then trained to predict these codes, which can be decompressed at test time to re-obtain the original representation. Our experimental evaluation shows that our method performs favorably when compared to state of the art on both multi-person and single-person 3D human pose estimation datasets and, thanks to our novel compression strategy, can process {fullHD} images at the constant runtime of 8 fps regardless of the number of subjects in the scene. Code and models are publicly available.},
	pages = {10},
	author = {Fabbri, Matteo and Lanzi, Fabio and Calderara, Simone and Alletto, Stefano and Cucchiara, Rita},
	langid = {english},
	file = {Fabbri 等。 - Compressed Volumetric Heatmaps for Multi-Person 3D.pdf:/Users/fangqi/Zotero/storage/M9KPSQBH/Fabbri 等。 - Compressed Volumetric Heatmaps for Multi-Person 3D.pdf:application/pdf},
}

@article{benzine_pandanet_nodate,
	title = {{PandaNet}: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation},
	pages = {10},
	author = {Benzine, Abdallah and Chabot, Florian and Luvison, Bertrand and Pham, Quoc Cuong and Achard, Catherine},
	langid = {english},
	file = {Benzine 等。 - PandaNet Anchor-Based Single-Shot Multi-Person 3D.pdf:/Users/fangqi/Zotero/storage/HI6VV7EL/Benzine 等。 - PandaNet Anchor-Based Single-Shot Multi-Person 3D.pdf:application/pdf},
}

@article{veges_multi-person_2020,
	title = {Multi-Person Absolute 3D Human Pose Estimation with Weak Depth Supervision},
	url = {http://arxiv.org/abs/2004.03989},
	abstract = {In 3D human pose estimation one of the biggest problems is the lack of large, diverse datasets. This is especially true for multi-person 3D pose estimation, where, to our knowledge, there are only machine generated annotations available for training. To mitigate this issue, we introduce a network that can be trained with additional {RGB}-D images in a weakly supervised fashion. Due to the existence of cheap sensors, videos with depth maps are widely available, and our method can exploit a large, unannotated dataset. Our algorithm is a monocular, multi-person, absolute pose estimator. We evaluate the algorithm on several benchmarks, showing a consistent improvement in error rates. Also, our model achieves state-of-the-art results on the {MuPoTS}-3D dataset by a considerable margin.},
	journaltitle = {{arXiv}:2004.03989 [cs]},
	author = {Veges, Marton and Lorincz, Andras},
	urldate = {2020-06-25},
	date = {2020-04-08},
	eprinttype = {arxiv},
	eprint = {2004.03989},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/fangqi/Zotero/storage/XASJAEBJ/Veges 和 Lorincz - 2020 - Multi-Person Absolute 3D Human Pose Estimation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/fangqi/Zotero/storage/RNG7ICL4/2004.html:text/html},
}

@inproceedings{benzine_deep_2019,
	title = {Deep, Robust and Single Shot 3D Multi-Person Human Pose Estimation from Monocular Images},
	doi = {10.1109/ICIP.2019.8803833},
	abstract = {In this paper, we propose a new single shot method for multi-person 3D pose estimation, from monocular {RGB} images. Our model jointly learns to locate the human joints in the image, to estimate their 3D coordinates and to group these predictions into full human skeletons. Our approach leverages and extends the Stacked Hourglass Network and its multi-scale feature learning to manage multi-person situations. Thus, we exploit the Occlusions Robust Pose Maps ({ORPM}) to fully describe several 3D human poses even in case of strong occlusions or cropping. Then, joint grouping and human pose estimation for an arbitrary number of people are performed using associative embedding. We evaluate our method on the challenging {CMU} Panoptic dataset, and demonstrate that it achieves better results than the state of the art.},
	eventtitle = {2019 {IEEE} International Conference on Image Processing ({ICIP})},
	pages = {584--588},
	booktitle = {2019 {IEEE} International Conference on Image Processing ({ICIP})},
	author = {Benzine, Abdallah and Luvison, Bertrand and Pham, Quoc Cuong and Achard, Catherine},
	date = {2019-09},
	note = {{ISSN}: 2381-8549},
	keywords = {3D, cropping, estimation, feature extraction, Heating systems, human, human joints, human skeletons, image classification, image colour analysis, image motion analysis, image representation, image sequences, joint grouping, learning (artificial intelligence), monocular images, monocular {RGB} images, multi-person, multiperson 3D pose estimation, multiperson situations, multiscale feature, Neck, Occlusions Robust Pose Maps, pose, pose estimation, Pose estimation, single shot method, Skeleton, Solid modeling, Stacked Hourglass Network, strong occlusions, Three-dimensional displays, Two dimensional displays},
	file = {IEEE Xplore Full Text PDF:/Users/fangqi/Zotero/storage/H4QMTM5Q/Benzine 等。 - 2019 - Deep, Robust and Single Shot 3D Multi-Person Human.pdf:application/pdf},
}

@article{jiang_coherent_nodate,
	title = {Coherent Reconstruction of Multiple Humans From a Single Image},
	pages = {10},
	author = {Jiang, Wen and Kolotouros, Nikos and Pavlakos, Georgios and Zhou, Xiaowei and Daniilidis, Kostas},
	langid = {english},
	file = {Jiang 等。 - Coherent Reconstruction of Multiple Humans From a .pdf:/Users/fangqi/Zotero/storage/LZQVXLDB/Jiang 等。 - Coherent Reconstruction of Multiple Humans From a .pdf:application/pdf},
}

@article{fieraru_three-dimensional_nodate,
	title = {Three-Dimensional Reconstruction of Human Interactions},
	abstract = {Understanding 3d human interactions is fundamental for ﬁne grained scene analysis and behavioural modeling. However, most of the existing models focus on analyzing a single person in isolation, and those who process several people focus largely on resolving multi-person data association, rather than inferring interactions. This may lead to incorrect, lifeless 3d estimates, that miss the subtle human contact aspects–the essence of the event–and are of little use for detailed behavioral understanding. This paper addresses such issues and makes several contributions: (1) we introduce models for interaction signature estimation ({ISP}) encompassing contact detection, segmentation, and 3d contact signature prediction; (2) we show how such components can be leveraged in order to produce augmented losses that ensure contact consistency during 3d reconstruction; (3) we construct several large datasets for learning and evaluating 3d contact prediction and reconstruction methods; speciﬁcally, we introduce {CHI}3D, a lab-based accurate 3d motion capture dataset with 631 sequences containing 2, 525 contact events, 728, 664 ground truth 3d poses, as well as {FlickrCI}3D, a dataset of 11, 216 images, with 14, 081 processed pairs of people, and 81, 233 facet-level surface correspondences within 138, 213 selected contact regions. Finally, (4) we present models and baselines to illustrate how contact estimation supports meaningful 3d reconstruction where essential interactions are captured. Models and data are made available for research purposes at http://vision.imar.ro/ci3d.},
	pages = {10},
	author = {Fieraru, Mihai and Zanfir, Mihai and Oneata, Elisabeta and Popa, Alin-Ionut and Olaru, Vlad and Sminchisescu, Cristian},
	langid = {english},
	file = {Fieraru 等。 - Three-Dimensional Reconstruction of Human Interact.pdf:/Users/fangqi/Zotero/storage/AM7Y6VQ5/Fieraru 等。 - Three-Dimensional Reconstruction of Human Interact.pdf:application/pdf},
}

@article{fabbri_learning_2018,
	title = {Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World},
	url = {http://arxiv.org/abs/1803.08319},
	abstract = {Multi-People Tracking in an open-world setting requires a special effort in precise detection. Moreover, temporal continuity in the detection phase gains more importance when scene cluttering introduces the challenging problems of occluded targets. For the purpose, we propose a deep network architecture that jointly extracts people body parts and associates them across short temporal spans. Our model explicitly deals with occluded body parts, by hallucinating plausible solutions of not visible joints. We propose a new end-to-end architecture composed by four branches (visible heatmaps, occluded heatmaps, part affinity fields and temporal affinity fields) fed by a time linker feature extractor. To overcome the lack of surveillance data with tracking, body part and occlusion annotations we created the vastest Computer Graphics dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. Our architecture trained on virtual data exhibits good generalization capabilities also on public real tracking benchmarks, when image resolution and sharpness are high enough, producing reliable tracklets useful for further batch data association or re-id modules.},
	journaltitle = {{arXiv}:1803.08319 [cs]},
	author = {Fabbri, Matteo and Lanzi, Fabio and Calderara, Simone and Palazzi, Andrea and Vezzani, Roberto and Cucchiara, Rita},
	urldate = {2020-06-25},
	date = {2018-09-18},
	eprinttype = {arxiv},
	eprint = {1803.08319},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/fangqi/Zotero/storage/ARB4AJRH/Fabbri 等。 - 2018 - Learning to Detect and Track Visible and Occluded .pdf:application/pdf;arXiv.org Snapshot:/Users/fangqi/Zotero/storage/FE97E3I3/1803.html:text/html},
}

@article{mehta_xnect_2020,
	title = {{XNect}: Real-time Multi-Person 3D Motion Capture with a Single {RGB} Camera},
	url = {http://arxiv.org/abs/1907.00837},
	doi = {10.1145/3386569.3392410},
	shorttitle = {{XNect}},
	abstract = {We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single {RGB} camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network ({CNN}) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals.We contribute a new architecture for this {CNN}, called {SelecSLS} Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fully connected neural network turns the possibly partial (on account of occlusion) 2Dpose and 3Dpose features for each subject into a complete 3Dpose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.},
	journaltitle = {{arXiv}:1907.00837 [cs]},
	author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian},
	urldate = {2020-06-25},
	date = {2020-04-30},
	eprinttype = {arxiv},
	eprint = {1907.00837},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/fangqi/Zotero/storage/65RFGJBD/Mehta 等。 - 2020 - XNect Real-time Multi-Person 3D Motion Capture wi.pdf:application/pdf;arXiv.org Snapshot:/Users/fangqi/Zotero/storage/X6PR35JV/1907.html:text/html},
}

@article{sarandi_metrabs_2020,
	title = {{MeTRAbs}: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human Pose Estimation},
	url = {http://arxiv.org/abs/2007.07227},
	shorttitle = {{MeTRAbs}},
	abstract = {Heatmap representations have formed the basis of human pose estimation systems for many years, and their extension to 3D has been a fruitful line of recent research. This includes 2.5D volumetric heatmaps, whose X and Y axes correspond to image space and Z to metric depth around the subject. To obtain metric-scale predictions, 2.5D methods need a separate post-processing step to resolve scale ambiguity. Further, they cannot localize body joints outside the image boundaries, leading to incomplete estimates for truncated images. To address these limitations, we propose metric-scale truncation-robust ({MeTRo}) volumetric heatmaps, whose dimensions are all defined in metric 3D space, instead of being aligned with image space. This reinterpretation of heatmap dimensions allows us to directly estimate complete, metric-scale poses without test-time knowledge of distance or relying on anthropometric heuristics, such as bone lengths. To further demonstrate the utility our representation, we present a differentiable combination of our 3D metric-scale heatmaps with 2D image-space ones to estimate absolute 3D pose (our {MeTRAbs} architecture). We find that supervision via absolute pose loss is crucial for accurate non-root-relative localization. Using a {ResNet}-50 backbone without further learned layers, we obtain state-of-the-art results on Human3.6M, {MPI}-{INF}-3DHP and {MuPoTS}-3D. Our code will be made publicly available to facilitate further research.},
	journaltitle = {{arXiv}:2007.07227 [cs]},
	author = {Sárándi, István and Linder, Timm and Arras, Kai O. and Leibe, Bastian},
	urldate = {2020-07-15},
	date = {2020-07-12},
	eprinttype = {arxiv},
	eprint = {2007.07227},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.4.8},
	file = {arXiv Fulltext PDF:/Users/fangqi/Zotero/storage/CLD56YSN/Sárándi 等。 - 2020 - MeTRAbs Metric-Scale Truncation-Robust Heatmaps f.pdf:application/pdf;arXiv.org Snapshot:/Users/fangqi/Zotero/storage/2N7YMISD/2007.html:text/html},
}

@article{lin_hdnet_2020,
	title = {{HDNet}: Human Depth Estimation for Multi-Person Camera-Space Localization},
	url = {http://arxiv.org/abs/2007.08943},
	shorttitle = {{HDNet}},
	abstract = {Current works on multi-person 3D pose estimation mainly focus on the estimation of the 3D joint locations relative to the root joint and ignore the absolute locations of each pose. In this paper, we propose the Human Depth Estimation Network ({HDNet}), an end-to-end framework for absolute root joint localization in the camera coordinate space. Our {HDNet} first estimates the 2D human pose with heatmaps of the joints. These estimated heatmaps serve as attention masks for pooling features from image regions corresponding to the target person. A skeleton-based Graph Neural Network ({GNN}) is utilized to propagate features among joints. We formulate the target depth regression as a bin index estimation problem, which can be transformed with a soft-argmax operation from the classification output of our {HDNet}. We evaluate our {HDNet} on the root joint localization and root-relative 3D pose estimation tasks with two benchmark datasets, i.e., Human3.6M and {MuPoTS}-3D. The experimental results show that we outperform the previous state-of-the-art consistently under multiple evaluation metrics. Our source code is available at: https://github.com/{jiahaoLjh}/{HumanDepth}.},
	journaltitle = {{arXiv}:2007.08943 [cs]},
	author = {Lin, Jiahao and Lee, Gim Hee},
	urldate = {2020-07-20},
	date = {2020-07-17},
	eprinttype = {arxiv},
	eprint = {2007.08943},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/fangqi/Zotero/storage/A886WZEI/Lin 和 Lee - 2020 - HDNet Human Depth Estimation for Multi-Person Cam.pdf:application/pdf;arXiv.org Snapshot:/Users/fangqi/Zotero/storage/VAZ2QUBP/2007.html:text/html},
}

@article{li_hmor_2020,
	title = {{HMOR}: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation},
	url = {http://arxiv.org/abs/2008.00206},
	shorttitle = {{HMOR}},
	abstract = {Remarkable progress has been made in 3D human pose estimation from a monocular {RGB} camera. However, only a few studies explored 3D multi-person cases. In this paper, we attempt to address the lack of a global perspective of the top-down approaches by introducing a novel form of supervision - Hierarchical Multi-person Ordinal Relations ({HMOR}). The {HMOR} encodes interaction information as the ordinal relations of depths and angles hierarchically, which captures the body-part and joint level semantic and maintains global consistency at the same time. In our approach, an integrated top-down model is designed to leverage these ordinal relations in the learning process. The integrated model estimates human bounding boxes, human depths, and root-relative 3D poses simultaneously, with a coarse-to-ﬁne architecture to improve the accuracy of depth estimation. The proposed method signiﬁcantly outperforms state-of-the-art methods on publicly available multi-person 3D pose datasets. In addition to superior performance, our method costs lower computation complexity and fewer model parameters.},
	journaltitle = {{arXiv}:2008.00206 [cs]},
	author = {Li, Jiefeng and Wang, Can and Liu, Wentao and Qian, Chen and Lu, Cewu},
	urldate = {2020-10-31},
	date = {2020-08-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.00206},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Li 等。 - 2020 - HMOR Hierarchical Multi-Person Ordinal Relations .pdf:/Users/fangqi/Zotero/storage/PT2XVA7V/Li 等。 - 2020 - HMOR Hierarchical Multi-Person Ordinal Relations .pdf:application/pdf},
}

@article{kundu_unsupervised_2020,
	title = {Unsupervised Cross-Modal Alignment for Multi-Person 3D Pose Estimation},
	url = {http://arxiv.org/abs/2008.01388},
	abstract = {We present a deployment friendly, fast bottom-up framework for multi-person 3D human pose estimation. We adopt a novel neural representation of multi-person 3D pose which uniﬁes the position of person instances with their corresponding 3D pose representation. This is realized by learning a generative pose embedding which not only ensures plausible 3D pose predictions, but also eliminates the usual keypoint grouping operation as employed in prior bottom-up approaches. Further, we propose a practical deployment paradigm where paired 2D or 3D pose annotations are unavailable. In the absence of any paired supervision, we leverage a frozen network, as a teacher model, which is trained on an auxiliary task of multi-person 2D pose estimation. We cast the learning as a cross-modal alignment problem and propose training objectives to realize a shared latent space between two diverse modalities. We aim to enhance the model’s ability to perform beyond the limiting teacher network by enriching the latent-to-3D pose mapping using artiﬁcially synthesized multi-person 3D scene samples. Our approach not only generalizes to in-the-wild images, but also yields a superior trade-oﬀ between speed and performance, compared to prior top-down approaches. Our approach also yields state-of-the-art multi-person 3D pose estimation performance among the bottom-up approaches under consistent supervision levels.},
	journaltitle = {{arXiv}:2008.01388 [cs]},
	author = {Kundu, Jogendra Nath and Revanur, Ambareesh and Waghmare, Govind Vitthal and Venkatesh, Rahul Mysore and Babu, R. Venkatesh},
	urldate = {2021-02-28},
	date = {2020-08-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.01388},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Kundu 等。 - 2020 - Unsupervised Cross-Modal Alignment for Multi-Perso.pdf:/Users/fangqi/Zotero/storage/BB929VPS/Kundu 等。 - 2020 - Unsupervised Cross-Modal Alignment for Multi-Perso.pdf:application/pdf},
}

@article{tian_recovering_2022,
	title = {Recovering 3D Human Mesh from Monocular Images: A Survey},
	url = {http://arxiv.org/abs/2203.01923},
	shorttitle = {Recovering 3D Human Mesh from Monocular Images},
	abstract = {Estimating human pose and shape from monocular images is a long-standing problem in computer vision. Since the release of statistical body models, 3D human mesh recovery has been drawing broader attention. With the same goal of obtaining well-aligned and physically plausible mesh results, two paradigms have been developed to overcome challenges in the 2D-to-3D lifting process: i) an optimization-based paradigm, where different data terms and regularization terms are exploited as optimization objectives; and ii) a regression-based paradigm, where deep learning techniques are embraced to solve the problem in an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving the quality of 3D mesh labels for a wide range of datasets. Though remarkable progress has been achieved in the past decade, the task is still challenging due to flexible body motions, diverse appearances, complex environments, and insufficient in-the-wild annotations. To the best of our knowledge, this is the first survey to focus on the task of monocular 3D human mesh recovery. We start with the introduction of body models, and then introduce recovery frameworks and training objectives by providing in-depth analyses of their strengths and weaknesses. We also summarize datasets, evaluation metrics, and benchmark results. Open issues and future directions are discussed in the end, hoping to motivate researchers and facilitate their research in this area. A regularly updated project page can be found at https://github.com/tinatiansjz/hmr-survey.},
	journaltitle = {{arXiv}:2203.01923 [cs]},
	author = {Tian, Yating and Zhang, Hongwen and Liu, Yebin and Wang, Limin},
	urldate = {2022-03-04},
	date = {2022-03-03},
	eprinttype = {arxiv},
	eprint = {2203.01923},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/fangqi/Zotero/storage/H7JT6DJ6/Tian 等。 - 2022 - Recovering 3D Human Mesh from Monocular Images A .pdf:application/pdf;arXiv.org Snapshot:/Users/fangqi/Zotero/storage/F6J5R47S/2203.html:text/html},
}

@article{cheng_graph_2021,
	title = {Graph and Temporal Convolutional Networks for 3D Multi-person Pose Estimation in Monocular Videos},
	url = {http://arxiv.org/abs/2012.11806},
	abstract = {Despite the recent progress, 3D multi-person pose estimation from monocular videos is still challenging due to the commonly encountered problem of missing information caused by occlusion, partially out-of-frame target persons, and inaccurate person detection. To tackle this problem, we propose a novel framework integrating graph convolutional networks ({GCNs}) and temporal convolutional networks ({TCNs}) to robustly estimate camera-centric multi-person 3D poses that do not require camera parameters. In particular, we introduce a human-joint {GCN}, which, unlike the existing {GCN}, is based on a directed graph that employs the 2D pose estimator's confidence scores to improve the pose estimation results. We also introduce a human-bone {GCN}, which models the bone connections and provides more information beyond human joints. The two {GCNs} work together to estimate the spatial frame-wise 3D poses and can make use of both visible joint and bone information in the target frame to estimate the occluded or missing human-part information. To further refine the 3D pose estimation, we use our temporal convolutional networks ({TCNs}) to enforce the temporal and human-dynamics constraints. We use a joint-{TCN} to estimate person-centric 3D poses across frames, and propose a velocity-{TCN} to estimate the speed of 3D joints to ensure the consistency of the 3D pose estimation in consecutive frames. Finally, to estimate the 3D human poses for multiple persons, we propose a root-{TCN} that estimates camera-centric 3D poses without requiring camera parameters. Quantitative and qualitative evaluations demonstrate the effectiveness of the proposed method.},
	journaltitle = {{arXiv}:2012.11806 [cs]},
	author = {Cheng, Yu and Wang, Bo and Yang, Bo and Tan, Robby T.},
	urldate = {2022-03-28},
	date = {2021-04-07},
	eprinttype = {arxiv},
	eprint = {2012.11806},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/fangqi/Zotero/storage/6MAHHGR7/Cheng 等。 - 2021 - Graph and Temporal Convolutional Networks for 3D M.pdf:application/pdf;arXiv.org Snapshot:/Users/fangqi/Zotero/storage/MRTRZM28/2012.html:text/html},
}
