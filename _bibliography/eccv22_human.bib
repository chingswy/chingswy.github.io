---
---
@inproceedings{GGNerf,
  title={Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering},
  author={Mingfei Chen and Jianfeng Zhang and Xiangyu Xu and Lijuan Liu and Yujun Cai and Jiashi Feng and Shuicheng Yan},
  year={2021},
  tags={human-nerf, accelerate},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2112.04312.pdf},
  preview={assets/paper-reading/GGNerf.png},
  qing={Geometry-guided image feature integration获得density volume，减少采样的点的数量},
  abstract={In this work we develop a generalizable and efficient Neural Radiance Field(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis undersettings with sparse camera views. Though existing NeRF-based methods cansynthesize rather realistic details for human body, they tend to produce poorresults when the input has self-occlusion, especially for unseen humans undersparse views. Moreover, these methods often require a large number of samplingpoints for rendering, which leads to low efficiency and limits their real-worldapplicability. To address these challenges, we propose a Geometry-guidedProgressive NeRF (GP-NeRF). In particular, to better tackle self-occlusion, wedevise a geometry-guided multi-view feature integration approach that utilizesthe estimated geometry prior to integrate the incomplete information from inputviews and construct a complete geometry volume for the target human body.Meanwhile, for achieving higher rendering efficiency, we introduce aprogressive rendering pipeline through geometry guidance, which leverages thegeometric feature volume and the predicted density values to progressivelyreduce the number of sampling points and speed up the rendering process.Experiments on the ZJU-MoCap and THUman datasets show that our methodoutperforms the state-of-the-arts significantly across multiple generalizationsettings, while the time cost is reduced > 70% via applying our efficientprogressive rendering pipeline.},
  bibtex_show={true},
}

@inproceedings{Regularizing_Vector_Embedding,
  title={Regularizing Vector Embedding in Bottom-Up Human Pose Estimation},
  author={},
  year={2022},
  tags={human-pose-estimation, bottom-up},
  booktitle={ECCV},
  html={https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660105.pdf},
  preview={assets/paper-reading/Regularizing_Vector_Embedding.png},
  qing={使用scale来提升embedding},
  bibtex_show={true},
}

@inproceedings{Structural_Triangulation,
  title={Structural Triangulation: A Closed-Form Solution to Constrained 3D Human Pose Estimation},
  author={},
  year={2022},
  tags={mv1p},
  booktitle={ECCV},
  html={https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650685.pdf},
  preview={assets/paper-reading/Structural_Triangulation.png},
  qing={输入相机参数和骨架结构信息，优化的方式获得人体骨架},
  bibtex_show={true},
}

@inproceedings{NDF,
  title={NDF: Neural Deformable Fields for Dynamic Human Modelling},
  author={Ruiqi Zhang and Jie Chen},
  year={2022},
  tags={view-synthesis},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2207.09193.pdf},
  preview={assets/paper-reading/NDF.png},
  abstract={We propose Neural Deformable Fields (NDF), a new representation for dynamichuman digitization from a multi-view video. Recent works proposed to representa dynamic human body with shared canonical neural radiance fields which linksto the observation space with deformation fields estimations. However, thelearned canonical representation is static and the current design of thedeformation fields is not able to represent large movements or detailedgeometry changes. In this paper, we propose to learn a neural deformable fieldwrapped around a fitted parametric body model to represent the dynamic human.The NDF is spatially aligned by the underlying reference surface. A neuralnetwork is then learned to map pose to the dynamics of NDF. The proposed NDFrepresentation can synthesize the digitized performer with novel views andnovel poses with a detailed and reasonable dynamic appearance. Experiments showthat our method significantly outperforms recent human synthesis methods.},
  bibtex_show={true},
}

@inproceedings{Mobius_GCN,
  title={3D Human Pose Estimation Using Möbius Graph Convolutional Networks},
  author={Niloofar Azizi and Horst Possegger and Emanuele Rodolà and Horst Bischof},
  year={2022},
  tags={human-pose-estimation, gcn},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2203.10554.pdf},
  preview={assets/paper-reading/Mobius_GCN.png},
  abstract={3D human pose estimation is fundamental to understanding human behavior.Recently, promising results have been achieved by graph convolutional networks(GCNs), which achieve state-of-the-art performance and provide ratherlight-weight architectures. However, a major limitation of GCNs is theirinability to encode all the transformations between joints explicitly. Toaddress this issue, we propose a novel spectral GCN using the M\"obiustransformation (M\"obiusGCN). In particular, this allows us to directly andexplicitly encode the transformation between joints, resulting in asignificantly more compact representation. Compared to even the lightestarchitectures so far, our novel approach requires 90-98% fewer parameters, i.e.our lightest M\"obiusGCN uses only 0.042M trainable parameters. Besides thedrastic parameter reduction, explicitly encoding the transformation of jointsalso enables us to achieve state-of-the-art results. We evaluate our approachon the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP,demonstrating both state-of-the-art results and the generalization capabilitiesof M\"obiusGCN.},
  bibtex_show={true},
}

@inproceedings{PPT,
  title={PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation},
  author={Haoyu Ma and Zhe Wang and Yifei Chen and Deying Kong and Liangjian Chen and Xingwei Liu and Xiangyi Yan and Hao Tang and Xiaohui Xie},
  year={2022},
  tags={human, mv, 1p, 3dpose},
  html={https://arxiv.org/pdf/2209.08194.pdf},
  preview={assets/paper-reading/PPT.png},
  qing={使用人体区域来做fusion},
  abstract={Recently, the vision transformer and its variants have played an increasinglyimportant role in both monocular and multi-view human pose estimation.Considering image patches as tokens, transformers can model the globaldependencies within the entire image or across images from other views.However, global attention is computationally expensive. As a consequence, it isdifficult to scale up these transformer-based methods to high-resolutionfeatures and many views.  In this paper, we propose the token-Pruned Pose Transformer (PPT) for 2Dhuman pose estimation, which can locate a rough human mask and performsself-attention only within selected tokens. Furthermore, we extend our PPT tomulti-view human pose estimation. Built upon PPT, we propose a new cross-viewfusion strategy, called human area fusion, which considers all human foregroundpixels as corresponding candidates. Experimental results on COCO and MPIIdemonstrate that our PPT can match the accuracy of previous pose transformermethods while reducing the computation. Moreover, experiments on Human 3.6M andSki-Pose demonstrate that our Multi-view PPT can efficiently fuse cues frommultiple views and achieve new state-of-the-art results.},
  bibtex_show={true},
}

@inproceedings{DiffuStereo,
  title={DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras},
  author={Ruizhi Shao and Zerong Zheng and Hongwen Zhang and Jingxiang Sun and Yebin Liu},
  year={2022},
  tags={human-reconstruction},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2207.08000.pdf},
  preview={assets/paper-reading/DiffuStereo.png},
  abstract={We propose DiffuStereo, a novel system using only sparse cameras (8 in thiswork) for high-quality 3D human reconstruction. At its core is a noveldiffusion-based stereo module, which introduces diffusion models, a type ofpowerful generative models, into the iterative stereo matching network. To thisend, we design a new diffusion kernel and additional stereo constraints tofacilitate stereo matching and depth estimation in the network. We furtherpresent a multi-level stereo network architecture to handle high-resolution (upto 4k) inputs without requiring unaffordable memory footprint. Given a set ofsparse-view color images of a human, the proposed multi-level diffusion-basedstereo network can produce highly accurate depth maps, which are then convertedinto a high-quality 3D human model through an efficient multi-view fusionstrategy. Overall, our method enables automatic reconstruction of human modelswith quality on par to high-end dense-view camera rigs, and this is achievedusing a much more light-weight hardware setup. Experiments show that our methodoutperforms state-of-the-art methods by a large margin both qualitatively andquantitatively.},
  bibtex_show={true},
}

@inproceedings{SimCC,
  title={SimCC: a Simple Coordinate Classification Perspective for Human Pose Estimation},
  author={Yanjie Li and Sen Yang and Peidong Liu and Shoukui Zhang and Yunxiao Wang and Zhicheng Wang and Wankou Yang and Shu-Tao Xia},
  year={2021},
  tags={human-pose-estimation},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2107.03332.pdf},
  preview={assets/paper-reading/SimCC.png},
  qing={从坐标分类的角度来看2D人体姿态估计问题},
  abstract={The 2D heatmap-based approaches have dominated Human Pose Estimation (HPE)for years due to high performance. However, the long-standing quantizationerror problem in the 2D heatmap-based methods leads to several well-knowndrawbacks: 1) The performance for the low-resolution inputs is limited; 2) Toimprove the feature map resolution for higher localization precision, multiplecostly upsampling layers are required; 3) Extra post-processing is adopted toreduce the quantization error. To address these issues, we aim to explore abrand new scheme, called \textit{SimCC}, which reformulates HPE as twoclassification tasks for horizontal and vertical coordinates. The proposedSimCC uniformly divides each pixel into several bins, thus achieving\emph{sub-pixel} localization precision and low quantization error. Benefitingfrom that, SimCC can omit additional refinement post-processing and excludeupsampling layers under certain settings, resulting in a more simple andeffective pipeline for HPE. Extensive experiments conducted over COCO,CrowdPose, and MPII datasets show that SimCC outperforms heatmap-basedcounterparts, especially in low-resolution settings by a large margin.},
  bibtex_show={true},
}

@inproceedings{Occulusion_reasoning,
  title={Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation},
  author={Qihao Liu and Yi Zhang and Song Bai and Alan Yuille},
  year={2022},
  tags={human-pose-estimation, 1vmp},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2208.00090.pdf},
  preview={assets/paper-reading/Occulusion_reasoning.png},
  qing={估计被遮挡住的数据然后再进行association},
  abstract={Occlusion poses a great threat to monocular multi-person 3D human poseestimation due to large variability in terms of the shape, appearance, andposition of occluders. While existing methods try to handle occlusion with posepriors/constraints, data augmentation, or implicit reasoning, they still failto generalize to unseen poses or occlusion cases and may make large mistakeswhen multiple people are present. Inspired by the remarkable ability of humansto infer occluded joints from visible cues, we develop a method to explicitlymodel this process that significantly improves bottom-up multi-person humanpose estimation with or without occlusions. First, we split the task into twosubtasks: visible keypoints detection and occluded keypoints reasoning, andpropose a Deeply Supervised Encoder Distillation (DSED) network to solve thesecond one. To train our model, we propose a Skeleton-guided human ShapeFitting (SSF) approach to generate pseudo occlusion labels on the existingdatasets, enabling explicit occlusion reasoning. Experiments show thatexplicitly learning from occlusions improves human pose estimation. Inaddition, exploiting feature-level information of visible joints allows us toreason about occluded joints more accurately. Our method outperforms both thestate-of-the-art top-down and bottom-up methods on several benchmarks.},
  bibtex_show={true},
}

@inproceedings{VisDB,
  title={Learning Visibility for Robust Dense Human Body Estimation},
  author={Chun-Han Yao and Jimei Yang and Duygu Ceylan and Yi Zhou and Yang Zhou and Ming-Hsuan Yang},
  year={2022},
  tags={SMPL, 1v1p},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2208.10652.pdf},
  preview={assets/paper-reading/VisDB.png},
  qing={考虑了遮挡来估计SMPL},
  abstract={Estimating 3D human pose and shape from 2D images is a crucial yetchallenging task. While prior methods with model-based representations canperform reasonably well on whole-body images, they often fail when parts of thebody are occluded or outside the frame. Moreover, these results usually do notfaithfully capture the human silhouettes due to their limited representationpower of deformable models (e.g., representing only the naked body). Analternative approach is to estimate dense vertices of a predefined templatebody in the image space. Such representations are effective in localizingvertices within an image but cannot handle out-of-frame body parts. In thiswork, we learn dense human body estimation that is robust to partialobservations. We explicitly model the visibility of human joints and verticesin the x, y, and z axes separately. The visibility in x and y axes helpdistinguishing out-of-frame cases, and the visibility in depth axis correspondsto occlusions (either self-occlusions or occlusions by other objects). Weobtain pseudo ground-truths of visibility labels from dense UV correspondencesand train a neural network to predict visibility along with 3D coordinates. Weshow that visibility can serve as 1) an additional signal to resolve depthordering ambiguities of self-occluded vertices and 2) a regularization termwhen fitting a human body model to the predictions. Extensive experiments onmultiple 3D human datasets demonstrate that visibility modeling significantlyimproves the accuracy of human body estimation, especially for partial-bodycases. Our project page with code is at: https://github.com/chhankyao/visdb.},
  bibtex_show={true},
}

@inproceedings{FastMETRO,
  title={Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers},
  author={Junhyeong Cho and Kim Youwang and Tae-Hyun Oh},
  year={2022},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2207.13820.pdf},
  code={https://github.com/postech-ami/FastMETRO},
  qing={为了高效的学习关系，mask掉了没有连接的vertices},
  abstract={Transformer encoder architectures have recently achieved state-of-the-artresults on monocular 3D human mesh reconstruction, but they require asubstantial number of parameters and expensive computations. Due to the largememory overhead and slow inference speed, it is difficult to deploy such modelsfor practical use. In this paper, we propose a novel transformerencoder-decoder architecture for 3D human mesh reconstruction from a singleimage, called FastMETRO. We identify the performance bottleneck in theencoder-based transformers is caused by the token design which introduces highcomplexity interactions among input tokens. We disentangle the interactions viaan encoder-decoder architecture, which allows our model to demand much fewerparameters and shorter inference time. In addition, we impose the priorknowledge of human body's morphological relationship via attention masking andmesh upsampling operations, which leads to faster convergence with higheraccuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency,and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore,we validate its generalizability on FreiHAND.},
  bibtex_show={true},
}

@inproceedings{SmoothNet,
  title={SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos},
  author={Ailing Zeng and Lei Yang and Xuan Ju and Jiefeng Li and Jianyi Wang and Qiang Xu},
  year={2021},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2112.13715.pdf},
  abstract={When analyzing human motion videos, the output jitters from existing poseestimators are highly-unbalanced with varied estimation errors across frames.Most frames in a video are relatively easy to estimate and only suffer fromslight jitters. In contrast, for rarely seen or occluded actions, the estimatedpositions of multiple joints largely deviate from the ground truth values for aconsecutive sequence of frames, rendering significant jitters on them. Totackle this problem, we propose to attach a dedicated temporal-only refinementnetwork to existing pose estimators for jitter mitigation, named SmoothNet.Unlike existing learning-based solutions that employ spatio-temporal models toco-optimize per-frame precision and temporal smoothness at all the joints,SmoothNet models the natural smoothness characteristics in body movements bylearning the long-range temporal relations of every joint without consideringthe noisy correlations among joints. With a simple yet effective motion-awarefully-connected network, SmoothNet improves the temporal smoothness of existingpose estimators significantly and enhances the estimation accuracy of thosechallenging frames as a side-effect. Moreover, as a temporal-only model, aunique advantage of SmoothNet is its strong transferability across varioustypes of estimators and datasets. Comprehensive experiments on five datasetswith eleven popular backbone networks across 2D and 3D pose estimation and bodyrecovery tasks demonstrate the efficacy of the proposed solution. Code isavailable at https://github.com/cure-lab/SmoothNet.},
  bibtex_show={true},
}

@inproceedings{SCIO,
  title={Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation},
  author={Zhehan Kan and Shuoshuo Chen and Zeng Li and Zhihai He},
  year={2022},
  tags={2dpose, top-down},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2207.02425.pdf},
  abstract={We observe that human poses exhibit strong group-wise structural correlationand spatial coupling between keypoints due to the biological constraints ofdifferent body parts. This group-wise structural correlation can be explored toimprove the accuracy and robustness of human pose estimation. In this work, wedevelop a self-constrained prediction-verification network to characterize andlearn the structural correlation between keypoints during training. During theinference stage, the feedback information from the verification network allowsus to perform further optimization of pose prediction, which significantlyimproves the performance of human pose estimation. Specifically, we partitionthe keypoints into groups according to the biological structure of human body.Within each group, the keypoints are further partitioned into two subsets,high-confidence base keypoints and low-confidence terminal keypoints. Wedevelop a self-constrained prediction-verification network to perform forwardand backward predictions between these keypoint subsets. One fundamentalchallenge in pose estimation, as well as in generic prediction tasks, is thatthere is no mechanism for us to verify if the obtained pose estimation orprediction results are accurate or not, since the ground truth is notavailable. Once successfully learned, the verification network serves as anaccuracy verification module for the forward pose prediction. During theinference stage, it can be used to guide the local optimization of the poseestimation results of low-confidence keypoints with the self-constrained losson high-confidence keypoints as the objective function. Our extensiveexperimental results on benchmark MS COCO and CrowdPose datasets demonstratethat the proposed method can significantly improve the pose estimation results.},
  bibtex_show={true},
}

@inproceedings{KAPAO,
  title={Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation},
  author={William McNally and Kanav Vats and Alexander Wong and John McPhee},
  year={2021},
  tags={2dpose, bottom-up},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2111.08557.pdf},
  qing={把关键点当做物体来直接用yolo回归},
  abstract={In keypoint estimation tasks such as human pose estimation, heatmap-basedregression is the dominant approach despite possessing notable drawbacks:heatmaps intrinsically suffer from quantization error and require excessivecomputation to generate and post-process. Motivated to find a more efficientsolution, we propose to model individual keypoints and sets of spatiallyrelated keypoints (i.e., poses) as objects within a dense single-stageanchor-based detection framework. Hence, we call our method KAPAO (pronounced"Ka-Pow"), for Keypoints And Poses As Objects. KAPAO is applied to the problemof single-stage multi-person human pose estimation by simultaneously detectinghuman pose and keypoint objects and fusing the detections to exploit thestrengths of both object representations. In experiments, we observe that KAPAOis faster and more accurate than previous methods, which suffer greatly fromheatmap post-processing. The accuracy-speed trade-off is especially favourablein the practical setting when not using test-time augmentation. Source code:https://github.com/wmcnally/kapao.},
  bibtex_show={true},
}

@inproceedings{Poseur,
  title={Poseur: Direct Human Pose Regression with Transformers},
  author={Weian Mao and Yongtao Ge and Chunhua Shen and Zhi Tian and Xinlong Wang and Zhibin Wang and Anton van den Hengel},
  year={2022},
  tags={2dpose, top-down, transformer},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2201.07412.pdf},
  abstract={We propose a direct, regression-based approach to 2D human pose estimationfrom single images. We formulate the problem as a sequence prediction task,which we solve using a Transformer network. This network directly learns aregression mapping from images to the keypoint coordinates, without resortingto intermediate representations such as heatmaps. This approach avoids much ofthe complexity associated with heatmap-based approaches. To overcome thefeature misalignment issues of previous regression-based methods, we propose anattention mechanism that adaptively attends to the features that are mostrelevant to the target keypoints, considerably improving the accuracy.Importantly, our framework is end-to-end differentiable, and naturally learnsto exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,two predominant pose-estimation datasets, demonstrate that our methodsignificantly improves upon the state-of-the-art in regression-based poseestimation. More notably, ours is the first regression-based approach toperform favorably compared to the best heatmap-based pose estimation methods.},
  bibtex_show={true},
}

@inproceedings{VirtualPose,
  title={VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data},
  author={Jiajun Su and Chunyu Wang and Xiaoxuan Ma and Wenjun Zeng and Yizhou Wang},
  year={2022},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2207.09949.pdf},
  code={https://github.com/wkom/VirtualPose},
  qing={使用root depth估计人体三维的位置，使用3DCNN恢复关键点位置},
  abstract={While monocular 3D pose estimation seems to have achieved very accurateresults on the public datasets, their generalization ability is largelyoverlooked. In this work, we perform a systematic evaluation of the existingmethods and find that they get notably larger errors when tested on differentcameras, human poses and appearance. To address the problem, we introduceVirtualPose, a two-stage learning framework to exploit the hidden "free lunch"specific to this task, i.e. generating infinite number of poses and cameras fortraining models at no cost. To that end, the first stage transforms images toabstract geometry representations (AGR), and then the second maps them to 3Dposes. It addresses the generalization issue from two aspects: (1) the firststage can be trained on diverse 2D datasets to reduce the risk of over-fittingto limited appearance; (2) the second stage can be trained on diverse AGRsynthesized from a large number of virtual cameras and poses. It outperformsthe SOTA methods without using any paired images and 3D poses from thebenchmarks, which paves the way for practical applications. Code is availableat https://github.com/wkom/VirtualPose.},
  bibtex_show={true},
}

@inproceedings{HULC,
  title={HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance},
  author={Soshi Shimada and Vladislav Golyanik and Zhi Li and Patrick Pérez and Weipeng Xu and Christian Theobalt},
  year={2022},
  tags={contact},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2205.05677.pdf},
  abstract={Marker-less monocular 3D human motion capture (MoCap) with scene interactionsis a challenging research topic relevant for extended reality, robotics andvirtual avatar generation. Due to the inherent depth ambiguity of monocularsettings, 3D motions captured with existing methods often contain severeartefacts such as incorrect body-scene inter-penetrations, jitter and bodyfloating. To tackle these issues, we propose HULC, a new approach for 3D humanMoCap which is aware of the scene geometry. HULC estimates 3D poses and densebody-environment surface contacts for improved 3D localisations, as well as theabsolute scale of the subject. Furthermore, we introduce a 3D pose trajectoryoptimisation based on a novel pose manifold sampling that resolves erroneousbody-environment inter-penetrations. Although the proposed method requires lessstructured inputs compared to existing scene-aware monocular MoCap algorithms,it produces more physically-plausible poses: HULC significantly andconsistently outperforms the existing approaches in various experiments and ondifferent metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.},
  bibtex_show={true},
}

@inproceedings{FLEX,
  title={FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction},
  author={Brian Gordon and Sigal Raab and Guy Azov and Raja Giryes and Daniel Cohen-Or},
  year={2021},
  tags={3dpose, mv1p},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2105.01937.pdf},
  code={https://briang13.github.io/FLEX/},
  qing={输入多视角的序列的2D估计，估计脚步接触标签以及骨长、3D旋转，可以不给定相机},
  abstract={The increasing availability of video recordings made by multiple cameras hasoffered new means for mitigating occlusion and depth ambiguities in pose andmotion reconstruction methods. Yet, multi-view algorithms strongly depend oncamera parameters; particularly, the relative transformations between thecameras. Such a dependency becomes a hurdle once shifting to dynamic capture inuncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), anend-to-end extrinsic parameter-free multi-view model. FLEX is extrinsicparameter-free (dubbed ep-free) in the sense that it does not require extrinsiccamera parameters. Our key idea is that the 3D angles between skeletal parts,as well as bone lengths, are invariant to the camera position. Hence, learning3D rotations and bone lengths rather than locations allows predicting commonvalues for all camera views. Our network takes multiple video streams, learnsfused deep features through a novel multi-view fusion layer, and reconstructs asingle consistent skeleton with temporally coherent joint rotations. Wedemonstrate quantitative and qualitative results on three public datasets, andon synthetic multi-person video streams captured by dynamic cameras. We compareour model to state-of-the-art methods that are not ep-free and show that in theabsence of camera parameters, we outperform them by a large margin whileobtaining comparable results when camera parameters are available. Code,trained models, and other materials are available on our project page.},
  bibtex_show={true},
}

@inproceedings{neural_capture,
  title={Neural Capture of Animatable 3D Human from Monocular Video},
  author={Gusi Te and Xiu Li and Xiao Li and Jinglu Wang and Wei Hu and Yan Lu},
  year={2022},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2208.08728.pdf},
  qing={使用query embedding包含了距离、方向},
  abstract={We present a novel paradigm of building an animatable 3D human representationfrom a monocular video input, such that it can be rendered in any unseen posesand views. Our method is based on a dynamic Neural Radiance Field (NeRF) riggedby a mesh-based parametric 3D human model serving as a geometry proxy. Previousmethods usually rely on multi-view videos or accurate 3D geometry informationas additional inputs; besides, most methods suffer from degraded quality whengeneralized to unseen poses. We identify that the key to generalization is agood input embedding for querying dynamic NeRF: A good input embedding shoulddefine an injective mapping in the full volumetric space, guided by surfacemesh deformation under pose variation. Based on this observation, we propose toembed the input query with its relationship to local surface regions spanned bya set of geodesic nearest neighbors on mesh vertices. By including bothposition and relative distance information, our embedding defines adistance-preserved deformation mapping and generalizes well to unseen poses. Toreduce the dependency on additional inputs, we first initialize per-frame 3Dmeshes using off-the-shelf tools and then propose a pipeline to jointlyoptimize NeRF and refine the initial mesh. Extensive experiments show ourmethod can synthesize plausible human rendering results under unseen poses andviews.},
  bibtex_show={true},
}

@inproceedings{BodySLAM,
  title={BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking},
  author={Dorian F. Henning and Tristan Laidlow and Stefan Leutenegger},
  year={2022},
  booktitle={ECCV},
  html={https://arxiv.org/pdf/2205.02301.pdf},
  qing={同时完成相机定位和人体跟踪},
  abstract={Estimating human motion from video is an active research area due to its manypotential applications. Most state-of-the-art methods predict human shape andposture estimates for individual images and do not leverage the temporalinformation available in video. Many "in the wild" sequences of human motionare captured by a moving camera, which adds the complication of conflatedcamera and human motion to the estimation. We therefore present BodySLAM, amonocular SLAM system that jointly estimates the position, shape, and postureof human bodies, as well as the camera trajectory. We also introduce a novelhuman motion model to constrain sequential body postures and observe the scaleof the scene. Through a series of experiments on video sequences of humanmotion captured by a moving monocular camera, we demonstrate that BodySLAMimproves estimates of all human body parameters and camera poses when comparedto estimating these separately.},
  bibtex_show={true},
}

