<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="chingswy.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="chingswy.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-28T12:26:05+00:00</updated><id>chingswy.github.io/feed.xml</id><title type="html">Qing Shuai | 帅青</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">常用工具安装使用</title><link href="chingswy.github.io/blog/2022/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/" rel="alternate" type="text/html" title="常用工具安装使用"/><published>2022-11-16T15:59:00+00:00</published><updated>2022-11-16T15:59:00+00:00</updated><id>chingswy.github.io/blog/2022/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8</id><content type="html" xml:base="chingswy.github.io/blog/2022/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/"><![CDATA[<h1 id="文字">文字</h1> <h2 id="galai">galai</h2> <p>安装，依赖<code class="language-plaintext highlighter-rouge">torch</code>，有许多依赖库，安装比较久。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install galai
</code></pre></div></div> <p>使用：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import galai as gal

model = gal.load_model("huge")
model.generate("The Transformer architecture [START_REF]")
# The Transformer architecture [START_REF] Attention is All you Need, Vaswani[END_REF] has been widely used in natural language processing.
</code></pre></div></div> <h1 id="blender">Blender</h1> <p>Blender安装easymocap:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> &lt;path/to/EasyMocap&gt;
&lt;path/to/blender&gt;/blender-2.93.5-linux-x64/2.93/python/bin/python3.9 setup.py develop
</code></pre></div></div> <h1 id="视频编辑">视频编辑</h1> <h2 id="tts工具">tts工具</h2> <p><a href="https://azure.microsoft.com/zh-cn/products/cognitive-services/text-to-speech/#features">azure</a></p> <p>https://greasyfork.org/en/scripts/444347-azure-speech-download</p> <h2 id="字幕">字幕</h2> <p>字幕编辑工具 <a href="https://github.com/mli/autocut">autocut</a></p>]]></content><author><name></name></author><category term="tools"/><category term="tools"/><summary type="html"><![CDATA[文字]]></summary></entry><entry><title type="html">多视角多人相关工作阅读</title><link href="chingswy.github.io/blog/2022/mvmp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="alternate" type="text/html" title="多视角多人相关工作阅读"/><published>2022-11-06T15:59:00+00:00</published><updated>2022-11-06T15:59:00+00:00</updated><id>chingswy.github.io/blog/2022/mvmp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB</id><content type="html" xml:base="chingswy.github.io/blog/2022/mvmp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><![CDATA[<h3 id="1-优化的方法">1. 优化的方法</h3> <p>Panoptic Studio, Pictorial structure, mvpose, 4D association</p> <h3 id="2-深度学习的方法">2. 深度学习的方法</h3> <p>lrtri, voxel pose</p> <h3 id="3-其他">3. 其他</h3> <p><a class="citation" href="#SmartMocap">(Saini et al., 2022)</a>.</p> <h2 id="references">References</h2> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="SmartMocap" class="col-sm-8"> <div class="title">SmartMocap: Joint Estimation of Human and Camera Motion using Uncalibrated RGB Cameras</div> <div class="author"> Nitin Saini,&nbsp;Chun-hao P. Huang,&nbsp;<a href="https://ps.is.tuebingen.mpg.de/person/black">Michael J. Black</a>,&nbsp;and&nbsp;Aamir Ahmad</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2209.13906.pdf" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Markerless human motion capture (mocap) from multiple RGB cameras is a widelystudied problem. Existing methods either need calibrated cameras or calibratethem relative to a static camera, which acts as the reference frame for themocap system. The calibration step has to be done a priori for every capturesession, which is a tedious process, and re-calibration is required whenevercameras are intentionally or accidentally moved. In this paper, we propose amocap method which uses multiple static and moving extrinsically uncalibratedRGB cameras. The key components of our method are as follows. First, since thecameras and the subject can move freely, we select the ground plane as a commonreference to represent both the body and the camera motions unlike existingmethods which represent bodies in the camera coordinate. Second, we learn aprobability distribution of short human motion sequences (\sim1sec) relativeto the ground plane and leverage it to disambiguate between the camera andhuman motion. Third, we use this distribution as a motion prior in a novelmulti-stage optimization approach to fit the SMPL human body model and thecamera poses to the human body keypoints on the images. Finally, we show thatour method can work on a variety of datasets ranging from aerial cameras tosmartphones. It also gives more accurate results compared to thestate-of-the-art on the task of monocular human mocap with a static camera. Ourcode is available for research purposes onhttps://github.com/robot-perception-group/SmartMocap.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SmartMocap</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SmartMocap: Joint Estimation of Human and Camera Motion using Uncalibrated RGB Cameras}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saini, Nitin and Huang, Chun-hao P. and Black, Michael J. and Ahmad, Aamir}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{mocap, pose-estimation, uncalibrated}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/mocap"> <i class="fas fa-hashtag fa-sm"></i> mocap</a> &nbsp;<a href="/tags/pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> pose-estimation</a> &nbsp;<a href="/tags/uncalibrated"> <i class="fas fa-hashtag fa-sm"></i> uncalibrated</a> &nbsp;</div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="human"/><category term="mv1p,"/><category term="human,"/><category term="review"/><summary type="html"><![CDATA[1. 优化的方法]]></summary></entry><entry><title type="html">SMPLify论文阅读</title><link href="chingswy.github.io/blog/2019/SMPLify%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="alternate" type="text/html" title="SMPLify论文阅读"/><published>2019-12-19T15:59:00+00:00</published><updated>2019-12-19T15:59:00+00:00</updated><id>chingswy.github.io/blog/2019/SMPLify%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB</id><content type="html" xml:base="chingswy.github.io/blog/2019/SMPLify%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><![CDATA[<h2 id="keep-it-smpl-automatic-estimation-of-3d-human-pose-and-shape-from-a-single-image">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</h2> <p>这篇文章从单张图像估计一个SMPL模型的pose参数和shape参数，将这个问题分为两步进行。首先，使用DeepCut来预测2D身体关节坐标；其次用SMPL模型拟合到这些2D关节坐标上。</p> <p><img src="assets/1534408669771.png" alt="1534408669771"/></p> <h3 id="实现方法">实现方法</h3> <p>这篇论文将这个估计问题转化为了参数优化问题，而不是使用端对端的方法来实现。这样做的最大的坏处就是慢。它的目标函数定义比较复杂，通过最小化这个目标函数，来找到最适合的模型参数。目标函数总共有5项，分别为</p> <ul> <li> <p>$E_J(\beta,\theta;K,J_{est}) $ \(E_J(\beta,\theta;K,J_{est}) = \sum_{joint~i}w_i\rho(\Pi_K(R_\theta(J(\beta)_i))-J{est,i})\) 这一项表示模型投影后的关节位置与前面的关节位置的距离的加权和。$\Pi_K$ 表示使用相机参数$K$投影到平面上，每个关节位置的权重是根据其位置估计的置信度来的，由前一步给出。对于被挡住的关节，这个值一般比较小，然后这个时候的参数就主要受到pose priors来控制。为了抑制测量的噪声，使用的一个鲁棒的可微的Geman-McClure惩罚函数$\rho$ 来自一篇1987年的文章，不知道作者怎么弄到的</p> </li> <li> <p>$E_a(\theta)$ \(E_a(\theta) = \sum_i \exp(\theta_i)\) 这一项用来惩罚膝盖和手腕的不正常的弯曲，就是反关节这种。关节没有弯的时候，$\theta_i$ 是0，如果是负的时候，就是正常的弯曲。正的时候就是不正常的，需要进行惩罚。（这一项考虑得也太细致了一点）</p> </li> <li> <p>$E_\theta(\theta)$ \(E_\theta(\theta)\equiv -\log\sum_j(g_j\mathcal{N}(\theta;\mu_{\theta,j},\Sigma_{\theta,j})) \approx -\log\max_j(cg_j\mathcal{N}(\theta;\mu_{\theta,j},\Sigma_{\theta,j})) \\=\min_j-\log(g_j\mathcal{N}(\theta;\mu_{\theta,j},\Sigma_{\theta,j}))\) 这一项就是pose prior，文章根据CMU数据集来训练，用来表示可能出现的姿态。由于计算求和太费劲了，因此使用最值来进行替代。尽管这一项不是可微的，但是他在每一处都用雅克比矩阵来近似。</p> <ul> <li>$g_j$ 代表混合模型的权重</li> <li>$c$,正的常数</li> </ul> </li> <li> <p>$E_{sp}(\theta;\beta)$</p> <p>这一项是为了约束身体避免碰撞，然后由于直接计算是否碰撞太困难了，因此作者使用了一些capsule来对各个部位进行近似，然后判断这些圆柱的碰撞关系，来进行惩罚。</p> <p><img src="assets/1534407306445.png" alt="1534407306445"/></p> <p>误差项是与胶囊交叉部分的体积有关联的，但是这样仍然难以计算，因此再次进行简化。将胶囊简化为一个球，球心为胶囊轴上的$C(\theta,\beta)$，半径为$r(\beta)$ ，与胶囊半径一样。 \(E_{sp}(\theta;\beta) = \sum_i\sum_{j\in I(i)} \exp \left(\dfrac{||C_i(\theta,\beta) - C_j(\theta,\beta) ||^2}{\sigma_i^2(\beta) + \sigma_j^2(\beta)}\right)\\ \sigma(\beta) = r(\beta)/3\) 在优化shape的时候没有使用这一项。</p> </li> <li> <p>$E_{\beta}(\beta) = \beta_T\Sigma_\beta^{-1}\beta$</p> <p>这一项是定义的shape prior，中间的矩阵是通过主成分分析得来的。</p> </li> </ul> <h3 id="优化">优化</h3> <p>首先假设相机的位置和身体的朝向是位置的。然后假设人是平行站立于图片平面的，初始化相机的位置，接着通过模型的躯干的平均长度，和我们预测得到的关节计算得来的躯干长度，利用相似三角形来估计人站立的深度。这样只是初步估计，将来会通过$E_J$来优化这个参数。接着会固定shape参数，来优化身体姿势。</p> <p>开始的时候，prior项的系数取大一点，然后再逐渐减小这两个权重。这样据说可以避免局部最优。</p> <p>如果看到的是人的侧面的话，人朝向哪个方向比较容易引起混淆。因此判断人的肩膀的距离小于阈值的时候，就给他两个方向都进行求解。然后选择损失函数最小的哪个。</p> <h3 id="代码实现">代码实现</h3> <p>作者使用的是Powell’s dogleg来进行优化求解，投影是基于OpenDR的，模型主要是基于Chumpy的。作者优化求解一张图需要一分钟左右。我在i5-5500,8G上跑大概需要两分钟的样子。反正就是很慢。要做到实时是相当难的。</p>]]></content><author><name></name></author><category term="human"/><summary type="html"><![CDATA[Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image]]></summary></entry><entry><title type="html">【论文阅读】videoavatars</title><link href="chingswy.github.io/blog/2019/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvideoavatar/" rel="alternate" type="text/html" title="【论文阅读】videoavatars"/><published>2019-12-19T15:59:00+00:00</published><updated>2019-12-19T15:59:00+00:00</updated><id>chingswy.github.io/blog/2019/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvideoavatar</id><content type="html" xml:base="chingswy.github.io/blog/2019/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvideoavatar/"><![CDATA[<h1 id="基于视频的3d人体模型重建">基于视频的3D人体模型重建</h1> <blockquote> <p>论文地址： [CVPR, 2018] <a href="https://virtualhumans.mpi-inf.mpg.de/papers/alldieck2018video/alldieck2018videoshapes.pdf">Video Based Reconstruction of 3D People Models</a></p> <p><a href="https://github.com/thmoa/videoavatars">代码地址</a></p> </blockquote> <p><img src="video3D.assets/1576494195075.png" alt="1576494195075"/></p> <ul> <li> <p>输入：单目单人人体动的视频</p> </li> <li> <p>输出：精确的人体形状及纹理</p> </li> <li> <p>核心：将动态变化的每一帧转化到参考帧上，获得visual hull。</p> </li> <li> <p>思想来源：visual hull 方法</p> <p><img src="videoavatars.assets/1576573351850.png" alt="1576573351850"/></p> <ul> <li>传统的visual hull ：从多视角的图片去估计一个静止的形状</li> <li>改造的visual hull：将各帧的人体动作undo到一个标准帧下。</li> </ul> </li> <li> <p>步骤：</p> <ol> <li>对每一帧拟合SMPL人体模型到2D关键点上</li> <li>将轮廓上的点反投影到空间中，并逆变换到标准帧下</li> <li>优化SMPL shape参数与每个点的位移，最小化点到线的距离</li> </ol> </li> </ul> <p>详细内容</p> <ol> <li>带偏置(offsets)的SMPL模型</li> </ol> <p><img src="https://www.zhihu.com/equation?tex=M(\beta, \theta) = W(T(\beta, \theta), J(\beta), \theta, W) \\ T(\beta, \theta) = T_\mu + B_s(\beta) + B_P(\theta) + \mathbf{D}" alt="M(\beta, \theta) = W(T(\beta, \theta), J(\beta), \theta, W) \\ T(\beta, \theta) = T_\mu + B_s(\beta) + B_P(\theta) + \mathbf{D}" class="ee_img tr_noresize" eeimg="1"/></p> <ol> <li><strong>姿态估计</strong>：增加拟合到轮廓的loss</li> </ol> <p><img src="https://www.zhihu.com/equation?tex= E_{silh}(\theta) = G(w_oI_{rn}(\theta))C + w_i(1-I_{rn}(\theta)\bar{C}) " alt=" E_{silh}(\theta) = G(w_oI_{rn}(\theta))C + w_i(1-I_{rn}(\theta)\bar{C}) " class="ee_img tr_noresize" eeimg="1"/></p> <p>其中 <img src="https://www.zhihu.com/equation?tex=I_{rn}" alt="I_{rn}" class="ee_img tr_noresize" eeimg="1"/> 为SMPL模型渲染到图像上的轮廓； <img src="https://www.zhihu.com/equation?tex=C" alt="C" class="ee_img tr_noresize" eeimg="1"/> 为图片中得到的轮廓， <img src="https://www.zhihu.com/equation?tex=\bar{C}" alt="\bar{C}" class="ee_img tr_noresize" eeimg="1"/> 为其逆。w为权重。对这个loss优化四个不同分辨率的，提高鲁棒性。</p> <p>对于SMPLify文中的姿态的先验，这篇文章考虑的是人体是A-pose的，改成A-pose的先验。</p> <p>再考虑时序平滑。</p> <ol> <li> <p><strong>一致的形状</strong>:使用上一步估计的pose，将每一帧的轮廓unpose到标准帧。</p> <ul> <li>数据项：</li> </ul> </li> </ol> <p><img src="https://www.zhihu.com/equation?tex= E_{data} = \sum_{(v,r) \in \mathcal{M}} \rho (v\times r_n - r_m) " alt=" E_{data} = \sum_{(v,r) \in \mathcal{M}} \rho (v\times r_n - r_m) " class="ee_img tr_noresize" eeimg="1"/></p> <ul> <li>拉普拉斯项：对mesh变形进行smooth</li> </ul> <p><img src="https://www.zhihu.com/equation?tex= E_{lp} = \sum_{i=1}^N \tau_{l,i} || L(v_i) - \delta_i || ^2 " alt=" E_{lp} = \sum_{i=1}^N \tau_{l,i} || L(v_i) - \delta_i || ^2 " class="ee_img tr_noresize" eeimg="1"/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 其中的 &lt;img src="https://www.zhihu.com/equation?tex=\delta" alt="\delta" class="ee_img tr_noresize" eeimg="1"&gt;
</code></pre></div></div> <p>为没有offset项的时候的拉普拉斯项</p> <ul> <li>身体模型项：约束offset的变形不要太大</li> <li>对称项：让身体处于对称位置的点的变形尽量一致</li> </ul> <ol> <li><strong>纹理生成</strong>: 把优化后的标准帧的模型重新投影到各个帧中，然后将图片反投影到纹理上。</li> </ol> <h2 id="附录知乎贴公式方案">附录：<a href="https://zhuanlan.zhihu.com/p/69142198">知乎贴公式方案</a></h2> <p>查找目标</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="se">\$\$\n</span><span class="k">*</span><span class="o">(</span>.<span class="k">*</span>?<span class="se">\n</span>.<span class="k">*</span><span class="o">)</span><span class="se">\n\$\$</span>
</code></pre></div></div> <p>替换为</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\n<span class="nt">&lt;img</span> <span class="na">src=</span><span class="s">"https://www.zhihu.com/equation?tex=$1"</span> <span class="na">alt=</span><span class="s">"$1"</span> <span class="na">class=</span><span class="s">"ee_img tr_noresize"</span> <span class="na">eeimg=</span><span class="s">"1"</span><span class="nt">&gt;</span>\n
</code></pre></div></div> <p>行内公式： 查找目标：</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\$\n*(.*?)\n*\$
</code></pre></div></div> <p>替换为</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\n<span class="nt">&lt;img</span> <span class="na">src=</span><span class="s">"https://www.zhihu.com/equation?tex=$1"</span> <span class="na">alt=</span><span class="s">"$1"</span> <span class="na">class=</span><span class="s">"ee_img tr_noresize"</span> <span class="na">eeimg=</span><span class="s">"1"</span><span class="nt">&gt;</span>\n
</code></pre></div></div>]]></content><author><name></name></author><category term="human"/><summary type="html"><![CDATA[基于视频的3D人体模型重建]]></summary></entry><entry><title type="html">SMPL论文阅读</title><link href="chingswy.github.io/blog/2019/SMPL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="alternate" type="text/html" title="SMPL论文阅读"/><published>2019-12-09T15:59:00+00:00</published><updated>2019-12-09T15:59:00+00:00</updated><id>chingswy.github.io/blog/2019/SMPL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB</id><content type="html" xml:base="chingswy.github.io/blog/2019/SMPL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><![CDATA[<h2 id="smpl-a-skinned-multi-person-linear-model">SMPL: A Skinned Multi-Person Linear Model</h2> <p><img src="assets/1539784197807.png" alt="1539784197807"/></p> <p><strong>摘要</strong>：这篇文章提出了一个使用<code class="language-plaintext highlighter-rouge">pose</code>和<code class="language-plaintext highlighter-rouge">shape</code>参数驱动的线性的人体模型，模型的主要参数有：<code class="language-plaintext highlighter-rouge">rest pose template</code>,<code class="language-plaintext highlighter-rouge">blend weights</code>,<code class="language-plaintext highlighter-rouge">pose-dependent blend shapes</code>,<code class="language-plaintext highlighter-rouge">identity-dependent blend shapes</code>,和一个从vertices到joint的<code class="language-plaintext highlighter-rouge">regressor</code>，这些参数都是是从训练数据中学习得来的。与之前的工作不同的是，<code class="language-plaintext highlighter-rouge">pose-dependent blend shapes</code>这一项是pose旋转矩阵的线性函数。这样使得从一个大型数据集里面训练这个模型成为可能的。</p> <p><strong>关键词</strong>：Body shape, skinning（不重要）,blendshapes,soft-tissue(不重要)</p> <p><strong>项目地址</strong>：<a href="http://smpl.is.tue.mpg.de/">SMPL</a></p> <h3 id="导论">导论</h3> <p>他们的目标是创造一个可以表示不同形状的身体的，可以随着动作自然的变形的，并且软组织在运动过程中还能发生形变的 人体模型。一般商业上的操作手法是手动操作mesh，来修改使用传统模型时出的问题。人的工作量就比较大。也有人从扫描的人体数据集中学习一个统计的身体模型，但是与商用软件不兼容，没法使用。</p> <p>因此SMPL模型的目标就是，既能使用，又能表示大范围的人体，还要能通过pose来自然的形变，还要有软组织的动力学，做动画的效率高，并且和现有的渲染引擎兼容。</p> <p>现有的LBS模型是使用得最广泛的，他是建立vertices和骨架之间的关系。但是这个模型会出现一些问题。</p> <h3 id="模型定义">模型定义</h3> <p>模型与SCAPE类似，将身体形状分解为identity-dependent shape和non-rigid pose dependent shape。这个人体模型包含了 \(N=6890\) 个点，与 \(K=23\) 个关节。男女的大部分参数都是通用的。</p> <p>模型的输入参数为形状参数 \(\beta\) ，和动作参数 \(\theta\) ,模型中包含以下几项：</p> <ul> <li> <p>\(\bar{\textbf{T}} \in \mathbb{R}^{3N}\) ,平均的模板形状 (mean template shape)</p> <p>这个时候的pose是zero pose,( \(\vec{\theta^*}\) )</p> </li> <li> <p>\(\mathcal{W}\in \mathbb{R}^{N\times K}\) ,各个关节的混合权重</p> </li> <li> <table> <tbody> <tr> <td>$$ B_S(\vec{\beta}):\mathbb{R}^{</td> <td>\vec{\beta}</td> <td>} \mapsto \mathbb{R}^{3N} $$ ,blend shape函数，将shape参数映射到每一个点上</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$$ J(\vec{\beta}):\mathbb{R}^{</td> <td>\vec{\beta}</td> <td>} \mapsto \mathbb{R}^{3K} $$ ，将shape参数映射到每个joint的位置上</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$$ B_P(\vec{\theta}):\mathbb{R}^{</td> <td>\vec{\theta}</td> <td>} \mapsto \mathbb{R}^{3N} $$ ,将pose参数映射到每个点上</td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>最终得到的结果就是 $$ M(\vec{\beta},\vec{\theta};\Phi):\mathbb{R}^{</td> <td>\vec{\theta}</td> <td>\times</td> <td>\vec{\beta}</td> <td>} \mapsto \mathbb{R}^{3N} $$ ,将shape和pose参数映射到每个点上。这里的$\Phi$ 指的是学习的模型的参数。</td> </tr> </tbody> </table> <h3 id="模型参数">模型参数</h3> <p>pose参数是使用axis-angle来定义的，对于每一个joint，都有一个$\vec{\omega}_k\in \mathbb{R}^3$，然后加上原点处的，总共24个关节，就有72个参数。旋转矩阵是使用Rodrigues formula计算得到</p> <table> <tbody> <tr> <td>$$ W(\bar{\mathbf{T}},\mathbf{J},\vec{\theta},\mathcal{W}):\mathbb{R}^{2N\times 3K\times</td> <td>\vec{\theta}</td> <td>\times</td> <td>\mathcal{W}</td> <td>} \mapsto \mathbb{R}^{3N} $$ ,将rest pose、joint location、pose参数、blend weights权重转化成每个点的坐标量。</td> </tr> </tbody> </table> <p><img src="assets/1539784149245.png" alt="1539784149245"/></p> <h3 id="代码实现">代码实现</h3> <p>原始的代码是基于 <a href="https://github.com/mattloper/chumpy">chumpy</a> 实现的，这个库似乎已经没有人维护了。而且也没法进行GPU计算。</p> <p>模型参数：</p> <table> <thead> <tr> <th>name</th> <th>type</th> <th>size</th> </tr> </thead> <tbody> <tr> <td>J_regressor_prior</td> <td>&lt;class ‘scipy.sparse.csc.csc_matrix’&gt;</td> <td>(24, 6890)</td> </tr> <tr> <td>pose</td> <td>&lt;class ‘chumpy.ch.Ch’&gt;</td> <td>(72,)</td> </tr> <tr> <td>f</td> <td>&lt;type ‘numpy.ndarray’&gt;</td> <td>(13776, 3)</td> </tr> <tr> <td>J_regressor</td> <td>&lt;class ‘scipy.sparse.csc.csc_matrix’&gt;</td> <td>(24, 6890)</td> </tr> <tr> <td>betas</td> <td>&lt;class ‘chumpy.ch.Ch’&gt;</td> <td>(10,)</td> </tr> <tr> <td>kintree_table</td> <td>&lt;type ‘numpy.ndarray’&gt;</td> <td>(2, 24)</td> </tr> <tr> <td>J</td> <td>&lt;class ‘chumpy.reordering.transpose’&gt;</td> <td>(24, 3)</td> </tr> <tr> <td>v_shaped</td> <td>&lt;class ‘chumpy.ch_ops.add’&gt;</td> <td>(6890, 3)</td> </tr> <tr> <td>weights_prior</td> <td>&lt;type ‘numpy.ndarray’&gt;</td> <td>(6890, 24)</td> </tr> <tr> <td>trans</td> <td>&lt;class ‘chumpy.ch.Ch’&gt;</td> <td>(3,)</td> </tr> <tr> <td>v_posed</td> <td>&lt;class ‘chumpy.ch_ops.add’&gt;</td> <td>(6890, 3)</td> </tr> <tr> <td>weights</td> <td>&lt;class ‘chumpy.ch.Ch’&gt;</td> <td>(6890, 24)</td> </tr> <tr> <td>vert_sym_idxs</td> <td>&lt;type ‘numpy.ndarray’&gt;</td> <td>(6890,)</td> </tr> <tr> <td>posedirs</td> <td>&lt;class ‘chumpy.ch.Ch’&gt;</td> <td>(6890, 3, 207)</td> </tr> <tr> <td>pose_training_info</td> <td>&lt;type ‘dict’&gt;</td> <td>6</td> </tr> <tr> <td>bs_style</td> <td>&lt;type ‘str’&gt;</td> <td>3</td> </tr> <tr> <td>v_template</td> <td>&lt;class ‘chumpy.ch.Ch’&gt;</td> <td>(6890, 3)</td> </tr> <tr> <td>shapedirs</td> <td>&lt;class ‘chumpy.ch.Ch’&gt;</td> <td>(6890, 3, 10)</td> </tr> <tr> <td>bs_type</td> <td>&lt;type ‘str’&gt;</td> <td>7</td> </tr> <tr> <td>r</td> <td>&lt;type ‘numpy.ndarray’&gt;</td> <td>(6890, 3)</td> </tr> </tbody> </table> <p><a href="https://github.com/CalciferZh/SMPL">SMPL的numpy及TensorFlow实现</a></p> <p><a href="https://github.com/blzq/tf_smpl">hmr论文中SMPL的tf实现</a></p> <p>##</p>]]></content><author><name></name></author><category term="human"/><summary type="html"><![CDATA[SMPL: A Skinned Multi-Person Linear Model]]></summary></entry></feed>