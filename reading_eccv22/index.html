<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>ECCV22/human | Qing Shuai | 帅青</title> <meta name="author" content="Qing Shuai"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="chingswy.github.io/reading_eccv22/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Qing Shuai | 帅青</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">ECCV22/human</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Regularizing_Vector_Embedding.png"> </div> <div id="Regularizing_Vector_Embedding" class="col-sm-8"> <div class="title">Regularizing Vector Embedding in Bottom-Up Human Pose Estimation</div> <div class="author"> </div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660105.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="qing hidden"> <p>使用scale来提升embedding</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Regularizing_Vector_Embedding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Regularizing Vector Embedding in Bottom-Up Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-pose-estimation, bottom-up}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> human-pose-estimation</a>  <a href="/tags/bottom-up"> <i class="fas fa-hashtag fa-sm"></i> bottom-up</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Structural_Triangulation.png"> </div> <div id="Structural_Triangulation" class="col-sm-8"> <div class="title">Structural Triangulation: A Closed-Form Solution to Constrained 3D Human Pose Estimation</div> <div class="author"> </div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650685.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="qing hidden"> <p>输入相机参数和骨架结构信息，优化的方式获得人体骨架</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Structural_Triangulation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Structural Triangulation: A Closed-Form Solution to Constrained 3D Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{mv1p}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/mv1p"> <i class="fas fa-hashtag fa-sm"></i> mv1p</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/NDF.png"> </div> <div id="NDF" class="col-sm-8"> <div class="title">NDF: Neural Deformable Fields for Dynamic Human Modelling</div> <div class="author"> Ruiqi Zhang, and Jie Chen</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2207.09193.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose Neural Deformable Fields (NDF), a new representation for dynamichuman digitization from a multi-view video. Recent works proposed to representa dynamic human body with shared canonical neural radiance fields which linksto the observation space with deformation fields estimations. However, thelearned canonical representation is static and the current design of thedeformation fields is not able to represent large movements or detailedgeometry changes. In this paper, we propose to learn a neural deformable fieldwrapped around a fitted parametric body model to represent the dynamic human.The NDF is spatially aligned by the underlying reference surface. A neuralnetwork is then learned to map pose to the dynamics of NDF. The proposed NDFrepresentation can synthesize the digitized performer with novel views andnovel poses with a detailed and reasonable dynamic appearance. Experiments showthat our method significantly outperforms recent human synthesis methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NDF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NDF: Neural Deformable Fields for Dynamic Human Modelling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Ruiqi and Chen, Jie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{view-synthesis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/view-synthesis"> <i class="fas fa-hashtag fa-sm"></i> view-synthesis</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Mobius_GCN.png"> </div> <div id="Mobius_GCN" class="col-sm-8"> <div class="title">3D Human Pose Estimation Using Möbius Graph Convolutional Networks</div> <div class="author"> Niloofar Azizi, Horst Possegger, Emanuele Rodolà, and Horst Bischof</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2203.10554.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>3D human pose estimation is fundamental to understanding human behavior.Recently, promising results have been achieved by graph convolutional networks(GCNs), which achieve state-of-the-art performance and provide ratherlight-weight architectures. However, a major limitation of GCNs is theirinability to encode all the transformations between joints explicitly. Toaddress this issue, we propose a novel spectral GCN using the Möbiustransformation (MöbiusGCN). In particular, this allows us to directly andexplicitly encode the transformation between joints, resulting in asignificantly more compact representation. Compared to even the lightestarchitectures so far, our novel approach requires 90-98% fewer parameters, i.e.our lightest MöbiusGCN uses only 0.042M trainable parameters. Besides thedrastic parameter reduction, explicitly encoding the transformation of jointsalso enables us to achieve state-of-the-art results. We evaluate our approachon the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP,demonstrating both state-of-the-art results and the generalization capabilitiesof MöbiusGCN.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Mobius_GCN</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{3D Human Pose Estimation Using Möbius Graph Convolutional Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azizi, Niloofar and Possegger, Horst and Rodolà, Emanuele and Bischof, Horst}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-pose-estimation, gcn}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> human-pose-estimation</a>  <a href="/tags/gcn"> <i class="fas fa-hashtag fa-sm"></i> gcn</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/PPT.png"> </div> <div id="PPT" class="col-sm-8"> <div class="title">PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation</div> <div class="author"> Haoyu Ma, Zhe Wang, Yifei Chen, Deying Kong, Liangjian Chen, Xingwei Liu, Xiangyi Yan, Hao Tang, and Xiaohui Xie</div> <div class="periodical"> <em>In </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2209.08194.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Recently, the vision transformer and its variants have played an increasinglyimportant role in both monocular and multi-view human pose estimation.Considering image patches as tokens, transformers can model the globaldependencies within the entire image or across images from other views.However, global attention is computationally expensive. As a consequence, it isdifficult to scale up these transformer-based methods to high-resolutionfeatures and many views. In this paper, we propose the token-Pruned Pose Transformer (PPT) for 2Dhuman pose estimation, which can locate a rough human mask and performsself-attention only within selected tokens. Furthermore, we extend our PPT tomulti-view human pose estimation. Built upon PPT, we propose a new cross-viewfusion strategy, called human area fusion, which considers all human foregroundpixels as corresponding candidates. Experimental results on COCO and MPIIdemonstrate that our PPT can match the accuracy of previous pose transformermethods while reducing the computation. Moreover, experiments on Human 3.6M andSki-Pose demonstrate that our Multi-view PPT can efficiently fuse cues frommultiple views and achieve new state-of-the-art results.</p> </div> <div class="qing hidden"> <p>使用人体区域来做fusion</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PPT</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Haoyu and Wang, Zhe and Chen, Yifei and Kong, Deying and Chen, Liangjian and Liu, Xingwei and Yan, Xiangyi and Tang, Hao and Xie, Xiaohui}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human, mv, 1p, 3dpose}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human"> <i class="fas fa-hashtag fa-sm"></i> human</a>  <a href="/tags/mv"> <i class="fas fa-hashtag fa-sm"></i> mv</a>  <a href="/tags/1p"> <i class="fas fa-hashtag fa-sm"></i> 1p</a>  <a href="/tags/3dpose"> <i class="fas fa-hashtag fa-sm"></i> 3dpose</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/DiffuStereo.png"> </div> <div id="DiffuStereo" class="col-sm-8"> <div class="title">DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras</div> <div class="author"> Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang Sun, and Yebin Liu</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2207.08000.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose DiffuStereo, a novel system using only sparse cameras (8 in thiswork) for high-quality 3D human reconstruction. At its core is a noveldiffusion-based stereo module, which introduces diffusion models, a type ofpowerful generative models, into the iterative stereo matching network. To thisend, we design a new diffusion kernel and additional stereo constraints tofacilitate stereo matching and depth estimation in the network. We furtherpresent a multi-level stereo network architecture to handle high-resolution (upto 4k) inputs without requiring unaffordable memory footprint. Given a set ofsparse-view color images of a human, the proposed multi-level diffusion-basedstereo network can produce highly accurate depth maps, which are then convertedinto a high-quality 3D human model through an efficient multi-view fusionstrategy. Overall, our method enables automatic reconstruction of human modelswith quality on par to high-end dense-view camera rigs, and this is achievedusing a much more light-weight hardware setup. Experiments show that our methodoutperforms state-of-the-art methods by a large margin both qualitatively andquantitatively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DiffuStereo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shao, Ruizhi and Zheng, Zerong and Zhang, Hongwen and Sun, Jingxiang and Liu, Yebin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-reconstruction}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-reconstruction"> <i class="fas fa-hashtag fa-sm"></i> human-reconstruction</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/Occulusion_reasoning.png"> </div> <div id="Occulusion_reasoning" class="col-sm-8"> <div class="title">Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation</div> <div class="author"> Qihao Liu, Yi Zhang, Song Bai, and Alan Yuille</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2208.00090.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Occlusion poses a great threat to monocular multi-person 3D human poseestimation due to large variability in terms of the shape, appearance, andposition of occluders. While existing methods try to handle occlusion with posepriors/constraints, data augmentation, or implicit reasoning, they still failto generalize to unseen poses or occlusion cases and may make large mistakeswhen multiple people are present. Inspired by the remarkable ability of humansto infer occluded joints from visible cues, we develop a method to explicitlymodel this process that significantly improves bottom-up multi-person humanpose estimation with or without occlusions. First, we split the task into twosubtasks: visible keypoints detection and occluded keypoints reasoning, andpropose a Deeply Supervised Encoder Distillation (DSED) network to solve thesecond one. To train our model, we propose a Skeleton-guided human ShapeFitting (SSF) approach to generate pseudo occlusion labels on the existingdatasets, enabling explicit occlusion reasoning. Experiments show thatexplicitly learning from occlusions improves human pose estimation. Inaddition, exploiting feature-level information of visible joints allows us toreason about occluded joints more accurately. Our method outperforms both thestate-of-the-art top-down and bottom-up methods on several benchmarks.</p> </div> <div class="qing hidden"> <p>估计被遮挡住的数据然后再进行association</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Occulusion_reasoning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Qihao and Zhang, Yi and Bai, Song and Yuille, Alan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{human-pose-estimation, 1vmp}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/human-pose-estimation"> <i class="fas fa-hashtag fa-sm"></i> human-pose-estimation</a>  <a href="/tags/1vmp"> <i class="fas fa-hashtag fa-sm"></i> 1vmp</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/paper-reading/VisDB.png"> </div> <div id="VisDB" class="col-sm-8"> <div class="title">Learning Visibility for Robust Dense Human Body Estimation</div> <div class="author"> Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, Yang Zhou, and Ming-Hsuan Yang</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2208.10652.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Estimating 3D human pose and shape from 2D images is a crucial yetchallenging task. While prior methods with model-based representations canperform reasonably well on whole-body images, they often fail when parts of thebody are occluded or outside the frame. Moreover, these results usually do notfaithfully capture the human silhouettes due to their limited representationpower of deformable models (e.g., representing only the naked body). Analternative approach is to estimate dense vertices of a predefined templatebody in the image space. Such representations are effective in localizingvertices within an image but cannot handle out-of-frame body parts. In thiswork, we learn dense human body estimation that is robust to partialobservations. We explicitly model the visibility of human joints and verticesin the x, y, and z axes separately. The visibility in x and y axes helpdistinguishing out-of-frame cases, and the visibility in depth axis correspondsto occlusions (either self-occlusions or occlusions by other objects). Weobtain pseudo ground-truths of visibility labels from dense UV correspondencesand train a neural network to predict visibility along with 3D coordinates. Weshow that visibility can serve as 1) an additional signal to resolve depthordering ambiguities of self-occluded vertices and 2) a regularization termwhen fitting a human body model to the predictions. Extensive experiments onmultiple 3D human datasets demonstrate that visibility modeling significantlyimproves the accuracy of human body estimation, especially for partial-bodycases. Our project page with code is at: https://github.com/chhankyao/visdb.</p> </div> <div class="qing hidden"> <p>考虑了遮挡来估计SMPL</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">VisDB</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Visibility for Robust Dense Human Body Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yao, Chun-Han and Yang, Jimei and Ceylan, Duygu and Zhou, Yi and Zhou, Yang and Yang, Ming-Hsuan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{SMPL, 1v1p}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/smpl"> <i class="fas fa-hashtag fa-sm"></i> SMPL</a>  <a href="/tags/1v1p"> <i class="fas fa-hashtag fa-sm"></i> 1v1p</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="FastMETRO" class="col-sm-8"> <div class="title">Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers</div> <div class="author"> Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2207.13820.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/postech-ami/FastMETRO" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Transformer encoder architectures have recently achieved state-of-the-artresults on monocular 3D human mesh reconstruction, but they require asubstantial number of parameters and expensive computations. Due to the largememory overhead and slow inference speed, it is difficult to deploy such modelsfor practical use. In this paper, we propose a novel transformerencoder-decoder architecture for 3D human mesh reconstruction from a singleimage, called FastMETRO. We identify the performance bottleneck in theencoder-based transformers is caused by the token design which introduces highcomplexity interactions among input tokens. We disentangle the interactions viaan encoder-decoder architecture, which allows our model to demand much fewerparameters and shorter inference time. In addition, we impose the priorknowledge of human body’s morphological relationship via attention masking andmesh upsampling operations, which leads to faster convergence with higheraccuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency,and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore,we validate its generalizability on FreiHAND.</p> </div> <div class="qing hidden"> <p>为了高效的学习关系，mask掉了没有连接的vertices</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">FastMETRO</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cho, Junhyeong and Youwang, Kim and Oh, Tae-Hyun}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="SCIO" class="col-sm-8"> <div class="title">Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation</div> <div class="author"> Zhehan Kan, Shuoshuo Chen, Zeng Li, and Zhihai He</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2207.02425.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We observe that human poses exhibit strong group-wise structural correlationand spatial coupling between keypoints due to the biological constraints ofdifferent body parts. This group-wise structural correlation can be explored toimprove the accuracy and robustness of human pose estimation. In this work, wedevelop a self-constrained prediction-verification network to characterize andlearn the structural correlation between keypoints during training. During theinference stage, the feedback information from the verification network allowsus to perform further optimization of pose prediction, which significantlyimproves the performance of human pose estimation. Specifically, we partitionthe keypoints into groups according to the biological structure of human body.Within each group, the keypoints are further partitioned into two subsets,high-confidence base keypoints and low-confidence terminal keypoints. Wedevelop a self-constrained prediction-verification network to perform forwardand backward predictions between these keypoint subsets. One fundamentalchallenge in pose estimation, as well as in generic prediction tasks, is thatthere is no mechanism for us to verify if the obtained pose estimation orprediction results are accurate or not, since the ground truth is notavailable. Once successfully learned, the verification network serves as anaccuracy verification module for the forward pose prediction. During theinference stage, it can be used to guide the local optimization of the poseestimation results of low-confidence keypoints with the self-constrained losson high-confidence keypoints as the objective function. Our extensiveexperimental results on benchmark MS COCO and CrowdPose datasets demonstratethat the proposed method can significantly improve the pose estimation results.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SCIO</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kan, Zhehan and Chen, Shuoshuo and Li, Zeng and He, Zhihai}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{2dpose, top-down}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/2dpose"> <i class="fas fa-hashtag fa-sm"></i> 2dpose</a>  <a href="/tags/top-down"> <i class="fas fa-hashtag fa-sm"></i> top-down</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="Poseur" class="col-sm-8"> <div class="title">Poseur: Direct Human Pose Regression with Transformers</div> <div class="author"> Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang, and Anton Hengel</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2201.07412.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We propose a direct, regression-based approach to 2D human pose estimationfrom single images. We formulate the problem as a sequence prediction task,which we solve using a Transformer network. This network directly learns aregression mapping from images to the keypoint coordinates, without resortingto intermediate representations such as heatmaps. This approach avoids much ofthe complexity associated with heatmap-based approaches. To overcome thefeature misalignment issues of previous regression-based methods, we propose anattention mechanism that adaptively attends to the features that are mostrelevant to the target keypoints, considerably improving the accuracy.Importantly, our framework is end-to-end differentiable, and naturally learnsto exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,two predominant pose-estimation datasets, demonstrate that our methodsignificantly improves upon the state-of-the-art in regression-based poseestimation. More notably, ours is the first regression-based approach toperform favorably compared to the best heatmap-based pose estimation methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Poseur</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Poseur: Direct Human Pose Regression with Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Weian and Ge, Yongtao and Shen, Chunhua and Tian, Zhi and Wang, Xinlong and Wang, Zhibin and van den Hengel, Anton}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{2dpose, top-down, transformer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/2dpose"> <i class="fas fa-hashtag fa-sm"></i> 2dpose</a>  <a href="/tags/top-down"> <i class="fas fa-hashtag fa-sm"></i> top-down</a>  <a href="/tags/transformer"> <i class="fas fa-hashtag fa-sm"></i> transformer</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="VirtualPose" class="col-sm-8"> <div class="title">VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data</div> <div class="author"> Jiajun Su, Chunyu Wang, Xiaoxuan Ma, Wenjun Zeng, and Yizhou Wang</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2207.09949.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/wkom/VirtualPose" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>While monocular 3D pose estimation seems to have achieved very accurateresults on the public datasets, their generalization ability is largelyoverlooked. In this work, we perform a systematic evaluation of the existingmethods and find that they get notably larger errors when tested on differentcameras, human poses and appearance. To address the problem, we introduceVirtualPose, a two-stage learning framework to exploit the hidden "free lunch"specific to this task, i.e. generating infinite number of poses and cameras fortraining models at no cost. To that end, the first stage transforms images toabstract geometry representations (AGR), and then the second maps them to 3Dposes. It addresses the generalization issue from two aspects: (1) the firststage can be trained on diverse 2D datasets to reduce the risk of over-fittingto limited appearance; (2) the second stage can be trained on diverse AGRsynthesized from a large number of virtual cameras and poses. It outperformsthe SOTA methods without using any paired images and 3D poses from thebenchmarks, which paves the way for practical applications. Code is availableat https://github.com/wkom/VirtualPose.</p> </div> <div class="qing hidden"> <p>使用root depth估计人体三维的位置，使用3DCNN恢复关键点位置</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">VirtualPose</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Su, Jiajun and Wang, Chunyu and Ma, Xiaoxuan and Zeng, Wenjun and Wang, Yizhou}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="HULC" class="col-sm-8"> <div class="title">HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance</div> <div class="author"> Soshi Shimada, Vladislav Golyanik, Zhi Li, Patrick Pérez, Weipeng Xu, and <a href="https://vcai.mpi-inf.mpg.de/VCAI_Projects.html" target="_blank" rel="noopener noreferrer">Christian Theobalt</a> </div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.05677.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Marker-less monocular 3D human motion capture (MoCap) with scene interactionsis a challenging research topic relevant for extended reality, robotics andvirtual avatar generation. Due to the inherent depth ambiguity of monocularsettings, 3D motions captured with existing methods often contain severeartefacts such as incorrect body-scene inter-penetrations, jitter and bodyfloating. To tackle these issues, we propose HULC, a new approach for 3D humanMoCap which is aware of the scene geometry. HULC estimates 3D poses and densebody-environment surface contacts for improved 3D localisations, as well as theabsolute scale of the subject. Furthermore, we introduce a 3D pose trajectoryoptimisation based on a novel pose manifold sampling that resolves erroneousbody-environment inter-penetrations. Although the proposed method requires lessstructured inputs compared to existing scene-aware monocular MoCap algorithms,it produces more physically-plausible poses: HULC significantly andconsistently outperforms the existing approaches in various experiments and ondifferent metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HULC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shimada, Soshi and Golyanik, Vladislav and Li, Zhi and Pérez, Patrick and Xu, Weipeng and Theobalt, Christian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{contact}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <a href="/tags/contact"> <i class="fas fa-hashtag fa-sm"></i> contact</a>  </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="neural_capture" class="col-sm-8"> <div class="title">Neural Capture of Animatable 3D Human from Monocular Video</div> <div class="author"> Gusi Te, Xiu Li, Xiao Li, Jinglu Wang, Wei Hu, and Yan Lu</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2208.08728.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present a novel paradigm of building an animatable 3D human representationfrom a monocular video input, such that it can be rendered in any unseen posesand views. Our method is based on a dynamic Neural Radiance Field (NeRF) riggedby a mesh-based parametric 3D human model serving as a geometry proxy. Previousmethods usually rely on multi-view videos or accurate 3D geometry informationas additional inputs; besides, most methods suffer from degraded quality whengeneralized to unseen poses. We identify that the key to generalization is agood input embedding for querying dynamic NeRF: A good input embedding shoulddefine an injective mapping in the full volumetric space, guided by surfacemesh deformation under pose variation. Based on this observation, we propose toembed the input query with its relationship to local surface regions spanned bya set of geodesic nearest neighbors on mesh vertices. By including bothposition and relative distance information, our embedding defines adistance-preserved deformation mapping and generalizes well to unseen poses. Toreduce the dependency on additional inputs, we first initialize per-frame 3Dmeshes using off-the-shelf tools and then propose a pipeline to jointlyoptimize NeRF and refine the initial mesh. Extensive experiments show ourmethod can synthesize plausible human rendering results under unseen poses andviews.</p> </div> <div class="qing hidden"> <p>使用query embedding包含了距离、方向</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">neural_capture</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Capture of Animatable 3D Human from Monocular Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Te, Gusi and Li, Xiu and Li, Xiao and Wang, Jinglu and Hu, Wei and Lu, Yan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="BodySLAM" class="col-sm-8"> <div class="title">BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking</div> <div class="author"> Dorian F. Henning, Tristan Laidlow, and Stefan Leutenegger</div> <div class="periodical"> <em>In ECCV</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="qing btn btn-sm z-depth-0" role="button">Qing</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.02301.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Estimating human motion from video is an active research area due to its manypotential applications. Most state-of-the-art methods predict human shape andposture estimates for individual images and do not leverage the temporalinformation available in video. Many "in the wild" sequences of human motionare captured by a moving camera, which adds the complication of conflatedcamera and human motion to the estimation. We therefore present BodySLAM, amonocular SLAM system that jointly estimates the position, shape, and postureof human bodies, as well as the camera trajectory. We also introduce a novelhuman motion model to constrain sequential body postures and observe the scaleof the scene. Through a series of experiments on video sequences of humanmotion captured by a moving monocular camera, we demonstrate that BodySLAMimproves estimates of all human body parameters and camera poses when comparedto estimating these separately.</p> </div> <div class="qing hidden"> <p>同时完成相机定位和人体跟踪</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">BodySLAM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Henning, Dorian F. and Laidlow, Tristan and Leutenegger, Stefan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Qing Shuai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>