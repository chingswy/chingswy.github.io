<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Qing Shuai | 帅青</title> <meta name="author" content="Qing Shuai"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="chingswy.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Qing Shuai | 帅青 </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <div class="img-fluid z-depth-1"> <iframe title="QingShuai@2022" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="200" height="200" src="https://sketchfab.com/models/b0fb28cf0fed4a1d8a696b5152ca775c/embed"> </iframe> </div> </div> <div class="clearfix"> <p>I am currently working at Tencent (2024.7-), where my focus is on human motion capture and generation under multimodal inputs. Prior to this, I was a Ph.D. student in Computer Science at <a href="http://www.zju.edu.cn/english/" target="_blank" rel="noopener noreferrer">Zhejiang University</a> from 2019 to 2024, under the supervision of <a href="http://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a>. My research interests lie at the intersection of computer vision and computer graphics, with a particular emphasis on 3D human pose estimation and generation, 3D reconstruction, and novel view synthesis.</p> <p>During my past career, my main focus was on the <a href="https://github.com/zju3dv/EasyMocap/" target="_blank" rel="noopener noreferrer">EasyMoCap</a> repository. The goal of this repository is to <strong>make human motion capture more accessible and straightforward</strong>. It encompasses a collection of code from my work over the past few years and includes essential tools for the field of human motion capture, such as camera calibration, interactive keypoint annotation, visualization, and more.</p> <div class="repositories" align="center"> <div class="repo p-2 text-center"> <a href="https://github.com/zju3dv/EasyMocap" target="_blank" rel="noopener noreferrer"> <img class="repo-img-light w-100" alt="zju3dv/EasyMocap" src="https://github-readme-stats.vercel.app/api/pin/?username=zju3dv&amp;repo=EasyMocap&amp;theme=default&amp;show_owner=true"> <img class="repo-img-dark w-100" alt="zju3dv/EasyMocap" src="https://github-readme-stats.vercel.app/api/pin/?username=zju3dv&amp;repo=EasyMocap&amp;theme=dark&amp;show_owner=true"> </a> </div> <div class="repo p-2 text-center"> <a href="https://github.com/zju3dv/EasyVolcap" target="_blank" rel="noopener noreferrer"> <img class="repo-img-light w-100" alt="zju3dv/EasyVolcap" src="https://github-readme-stats.vercel.app/api/pin/?username=zju3dv&amp;repo=EasyVolcap&amp;theme=default&amp;show_owner=true"> <img class="repo-img-dark w-100" alt="zju3dv/EasyVolcap" src="https://github-readme-stats.vercel.app/api/pin/?username=zju3dv&amp;repo=EasyVolcap&amp;theme=dark&amp;show_owner=true"> </a> </div> <div class="repo p-2 text-center"> <a href="https://github.com/zju3dv/LoG" target="_blank" rel="noopener noreferrer"> <img class="repo-img-light w-100" alt="zju3dv/LoG" src="https://github-readme-stats.vercel.app/api/pin/?username=zju3dv&amp;repo=LoG&amp;theme=default&amp;show_owner=true"> <img class="repo-img-dark w-100" alt="zju3dv/LoG" src="https://github-readme-stats.vercel.app/api/pin/?username=zju3dv&amp;repo=LoG&amp;theme=dark&amp;show_owner=true"> </a> </div> </div> <h2 id="demos">Demos</h2> <h4 id="professional-motion-capture-with-multi-camera-systems">Professional Motion Capture with Multi-Camera Systems</h4> <div class="row"> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-left"> <a href="assets/mocap/02_fitsmpl_output.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/mocap/02_fitsmpl_output.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-right"> <a href="assets/mocap/04_ballet.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/mocap/04_ballet.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> </div> <h4 id="simple-motion-capture-from-complex-internet-videos">Simple Motion Capture from Complex Internet Videos</h4> <div class="row"> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-left"> <a href="assets/mocap/1v1p-test-yusheng.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/mocap/1v1p-test-yusheng.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container"> <a href="assets/mocap/1v1p-test-cxk.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/mocap/1v1p-test-cxk.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-right"> <a href="assets/mocap/03_fitmono_mano.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/mocap/03_fitmono_mano.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> </div> <h4 id="novel-view-synthesis">Novel View Synthesis</h4> <div class="row"> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-left"> <a href="assets/multinb/demo_soccer1-6.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/multinb/demo_soccer1-6.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container"> <a href="assets/multinb/demo_soccer1-beijia.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/multinb/demo_soccer1-beijia.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-right"> <a href="assets/multinb/demo_soccer1-yuang.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/multinb/demo_soccer1-yuang.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> </div> <h4 id="4d-scene-reconstruction-and-editing">4D Scene Reconstruction and Editing</h4> <div class="row"> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-left"> <a href="assets/multinb/demo_boxing2.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/multinb/demo_boxing2.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container"> <a href="assets/multinb/demo_basketball_disappear.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/multinb/demo_basketball_disappear.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container"> <a href="assets/multinb/demo_handstand.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/multinb/demo_handstand.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> <div class="col-sm mt-0 mt-md-0"> <figure> <div class="video-container video-container-right"> <a href="assets/multinb/demo_juggle.mp4" target="_blank" title="click to play"> <video class="myvideo rounded z-depth-1" width="100%" playsinline="" loop="loop" preload="" onmouseover="this.play()" onmouseout="this.pause();" muted=""><source src="assets/multinb/demo_juggle.mp4" type="video/mp4"></source></video> </a> </div> </figure> </div> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <h2 class="year">2024</h2> <ol class="bibliography"></ol> <h2 class="year">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="assets/mocap/closemocap.mp4" type="video/mp4"></source></video></div> <div id="shuai2023reconstructing" class="col-sm-8"> <div class="title">Reconstructing Close Human Interactions from Multiple Views</div> <div class="author"> Qing Shuai, Zhiyuan Yu, Zhize Zhou, Lixin Fan, Haijun Yang, Can Yang, and <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a> </div> <div class="periodical"> <em>ACM Transactions on Graphics (TOG)</em> 2023 </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/multinb.mp4" type="video/mp4"></source></video></div> <div id="shuai2022multinb" class="col-sm-8"> <div class="title">Novel View Synthesis of Human Interactions from Sparse Multi-view Videos</div> <div class="author"> <em>Shuai Qing</em>, Geng Chen, Fang Qi, Peng Sida, Shen Wenhao, Zhou Xiaowei, and Bao Hujun</div> <div class="periodical"> <em>In SIGGRAPH Conference Proceedings</em> 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2022_SIGGRAPH_MultiNeuralBody.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shuai2022multinb</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Novel View Synthesis of Human Interactions from Sparse
  Multi-view Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qing, Shuai and Chen, Geng and Qi, Fang and Sida, Peng and Wenhao, Shen and Xiaowei, Zhou and Hujun, Bao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Conference Proceedings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/mirror.mp4" type="video/mp4"></source></video></div> <div id="fang2021mirrored" class="col-sm-8"> <div class="title">Reconstructing 3D Human Pose by Watching Humans in the Mirror</div> <div class="author"> Fang Qi, <em>Shuai* Qing</em>, Dong Junting, Bao Hujun, and Zhou Xiaowei</div> <div class="periodical"> <em>In CVPR</em> 2021 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fang2021mirrored</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reconstructing 3D Human Pose by Watching Humans in the Mirror}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qi, Fang and Qing, Shuai* and Junting, Dong and Hujun, Bao and Xiaowei, Zhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/imocap.mp4" type="video/mp4"></source></video></div> <div id="junting2020" class="col-sm-8"> <div class="title">Motion Capture from Internet Videos</div> <div class="author"> Dong* Junting, <em>Shuai* Qing</em>, Zhang Yuanqing, Liu Xian, Zhou Xiaowei, and Bao Hujun</div> <div class="periodical"> <em>In ECCV</em> 2020 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://zju3dv.github.io/iMoCap/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">junting2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Motion Capture from Internet Videos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">cover</span> <span class="p">=</span> <span class="s">{mocap.png}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="publications"> <h2>All Publications</h2> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="peng2024animatable" class="col-sm-8"> <div class="title">Animatable implicit neural representations for creating realistic avatars from videos</div> <div class="author"> Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 2024 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="xu2024anchorcrafteranimatecyberanchorssaling" class="col-sm-8"> <div class="title">AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation</div> <div class="author"> Ziyi Xu, Ziyao Huang, Juan Cao, Yong Zhang, Xiaodong Cun, Qing Shuai, Yuchen Wang, Linchao Bao, Jintao Li, and Fan Tang</div> <div class="periodical"> 2024 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="pi2024motion2to3leveraging2dmotion" class="col-sm-8"> <div class="title">Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation</div> <div class="author"> Huaijin Pi, Ruoxi Guo, Zehong Shen, Qing Shuai, Zechen Hu, Zhumei Wang, Yajiao Dong, Ruizhen Hu, Taku Komura, Sida Peng, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Xiaowei Zhou' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2024 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="zhuang2024idolinstantphotorealistic3d" class="col-sm-8"> <div class="title">IDOL: Instant Photorealistic 3D Human Creation from a Single Image</div> <div class="author"> Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, and Wei Liu</div> <div class="periodical"> 2024 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="http://pengsida.net/images/neuralbody_pami.png"></div> <div id="peng2023implicit" class="col-sm-8"> <div class="title">Implicit Neural Representations with Structured Latent Codes for Human Body Modeling</div> <div class="author"> Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a>, and Hujun Bao</div> <div class="periodical"> <em>TPAMI</em> 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">peng2023implicit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Implicit Neural Representations with Structured Latent Codes for Human Body Modeling}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{TPAMI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="http://pengsida.net/images/mlp_maps.png"></div> <div id="Peng_2023_CVPR" class="col-sm-8"> <div class="title">Representing Volumetric Videos As Dynamic MLP Maps</div> <div class="author"> Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a> </div> <div class="periodical"> <em>In CVPR</em> Jun 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Peng_2023_CVPR</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Sida and Yan, Yunzhi and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Representing Volumetric Videos As Dynamic MLP Maps}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4252-4262}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" onmouseover="this.play()" onmouseout="this.pause();" loop="loop" preload="" muted=""><source src="https://netease-gameai.github.io/ProPose/static/assets/hmr.mp4" type="video/mp4"></source></video></div> <div id="Fang_2023_CVPR" class="col-sm-8"> <div class="title">Learning Analytical Posterior Probability for Human Mesh Recovery</div> <div class="author"> Qi Fang, Kang Chen, Yinghui Fan, Qing Shuai, Jiefeng Li, and Weidong Zhang</div> <div class="periodical"> <em>In CVPR</em> Jun 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://netease-gameai.github.io/ProPose/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Fang_2023_CVPR</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fang, Qi and Chen, Kang and Fan, Yinghui and Shuai, Qing and Li, Jiefeng and Zhang, Weidong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Analytical Posterior Probability for Human Mesh Recovery}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8781-8791}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="assets/mocap/closemocap.mp4" type="video/mp4"></source></video></div> <div id="shuai2023reconstructing" class="col-sm-8"> <div class="title">Reconstructing Close Human Interactions from Multiple Views</div> <div class="author"> Qing Shuai, Zhiyuan Yu, Zhize Zhou, Lixin Fan, Haijun Yang, Can Yang, and <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a> </div> <div class="periodical"> <em>ACM Transactions on Graphics (TOG)</em> Jun 2023 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="bao2023motion" class="col-sm-8"> <div class="title">Motion capture method based on unsynchorized videos</div> <div class="author"> Hujun Bao, <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a>, DONG Junting, and Qing Shuai</div> <div class="periodical"> Aug 2023 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="dong2023ivs" class="col-sm-8"> <div class="title">iVS-Net: Learning Human View Synthesis from Internet Videos</div> <div class="author"> Junting Dong, Qi Fang, Tianshuo Yang, Qing Shuai, Chengyu Qiao, and Sida Peng</div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em> Aug 2023 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="shen2023learning" class="col-sm-8"> <div class="title">Learning human mesh recovery in 3D scenes</div> <div class="author"> Zehong Shen, Zhi Cen, Sida Peng, Qing Shuai, Hujun Bao, and <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> Aug 2023 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/multinb.mp4" type="video/mp4"></source></video></div> <div id="shuai2022multinb" class="col-sm-8"> <div class="title">Novel View Synthesis of Human Interactions from Sparse Multi-view Videos</div> <div class="author"> <em>Shuai Qing</em>, Geng Chen, Fang Qi, Peng Sida, Shen Wenhao, Zhou Xiaowei, and Bao Hujun</div> <div class="periodical"> <em>In SIGGRAPH Conference Proceedings</em> Aug 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2022_SIGGRAPH_MultiNeuralBody.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shuai2022multinb</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Novel View Synthesis of Human Interactions from Sparse
  Multi-view Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qing, Shuai and Chen, Geng and Qi, Fang and Sida, Peng and Wenhao, Shen and Xiaowei, Zhou and Hujun, Bao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Conference Proceedings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="quickpose" class="col-sm-8"> <div class="title">QuickPose: Real-Time Multi-View Multi-Person Pose Estimation in Crowded Scenes</div> <div class="author"> Zhou Zhize, <em>Shuai Qing</em>, Wang Yize, Fang Qi, Ji Xiaopeng, Li Fashuai, Bao Hujun, and Zhou Xiaowei</div> <div class="periodical"> <em>In SIGGRAPH Conference Proceedings</em> Aug 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This work proposes a real-time algorithm for reconstructing 3D human poses in crowded scenes from multiple calibrated views. The key challenge of this problem is to efficiently match 2D observations across multiple views. Previous methods perform multi-view matching either at the full-body level, which is sensitive to 2D pose estimation error, or at the part level, which ignores 2D constraints between different types of body parts in the same view. Instead, our approach reasons about all plausible skeleton proposals during multi-view matching, where each skeleton may consist of an arbitrary number of parts instead of being a whole body or a single part. To this end, we formulate the multi-view matching problem as mode seeking in the space of skeleton proposals and develop an efficient algorithm named QuickPose to solve the problem, which enables real-time motion capture in crowded scenes. Experiments show that the proposed algorithm achieves the state-of-the-art performance in terms of both speed and accuracy on public datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">quickpose</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhize, Zhou and Qing, Shuai and Yize, Wang and Qi, Fang and Xiaopeng, Ji and Fashuai, Li and Hujun, Bao and Xiaowei, Zhou}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{QuickPose: Real-Time Multi-View Multi-Person Pose Estimation in Crowded Scenes}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450393379}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3528233.3530746}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3528233.3530746}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Conference Proceedings}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{45}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{human pose estimation, quickshift, multi-person, multi-view}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Vancouver, BC, Canada}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SIGGRAPH '22}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="https://xzhou.me/videos/enerf.png"></div> <div id="lin2022efficient" class="col-sm-8"> <div class="title">Efficient Neural Radiance Fields for Interactive Free-viewpoint Video</div> <div class="author"> Lin Haotong, Peng Sida, Xu Zhen, Yan Yunzhi, <em>Shuai Qing</em>, Bao Hujun, and Zhou Xiaowei</div> <div class="periodical"> <em>In SIGGRAPH Asia Conference Proceedings</em> Aug 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lin2022efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Neural Radiance Fields for Interactive Free-viewpoint Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Haotong, Lin and Sida, Peng and Zhen, Xu and Yunzhi, Yan and Qing, Shuai and Hujun, Bao and Xiaowei, Zhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia Conference Proceedings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="https://xzhou.me/videos/handscanner.png"></div> <div id="huang2022hhor" class="col-sm-8"> <div class="title">Reconstructing Hand-Held Objects from Monocular Video</div> <div class="author"> Di Huang, Xiaopeng Ji, Xingyi He, Jiaming Sun, Tong He, <em>Shuai Qing</em>, Wanli Ouyang, and <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a> </div> <div class="periodical"> <em>In SIGGRAPH Asia Conference Proceedings</em> Aug 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dihuangdh.github.io/hhor/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huang2022hhor</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reconstructing Hand-Held Objects from Monocular Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Di and Ji, Xiaopeng and He, Xingyi and Sun, Jiaming and He, Tong and Qing, Shuai and Ouyang, Wanli and Zhou, Xiaowei}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia Conference Proceedings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="9419782" class="col-sm-8"> <div class="title">Shape Prior Guided Instance Disparity Estimation for 3D Object Detection</div> <div class="author"> Chen Linghao, Sun Jiaming, Xie Yiming, Zhang Siyu, <em>Shuai Qing</em>, Jiang Qinhong, Zhang Guofeng, Bao Hujun, and Zhou Xiaowei</div> <div class="periodical"> <em>TPAMI</em> Aug 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9419782</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Linghao, Chen and Jiaming, Sun and Yiming, Xie and Siyu, Zhang and Qing, Shuai and Qinhong, Jiang and Guofeng, Zhang and Hujun, Bao and Xiaowei, Zhou}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{TPAMI}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Shape Prior Guided Instance Disparity Estimation for 3D Object Detection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{44}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5529-5540}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TPAMI.2021.3076678}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"></div> <div id="NEURIPS2022_589c5bd0" class="col-sm-8"> <div class="title">TotalSelfScan: Learning Full-body Avatars from Self-Portrait Videos of Faces, Hands, and Bodies</div> <div class="author"> Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, <a href="https://xzhou.me" target="_blank" rel="noopener noreferrer">Xiaowei Zhou</a>, and Hujun Bao</div> <div class="periodical"> <em>In NeurIPS</em> Aug 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NEURIPS2022_589c5bd0</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dong, Junting and Fang, Qi and Guo, Yudong and Peng, Sida and Shuai, Qing and Zhou, Xiaowei and Bao, Hujun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13654--13667}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TotalSelfScan: Learning Full-body Avatars from Self-Portrait Videos of Faces, Hands, and Bodies}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" onmouseover="this.play()" onmouseout="this.pause();" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/neuralbody.mp4" type="video/mp4"></source></video></div> <div id="peng2021neural" class="col-sm-8"> <div class="title">Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans</div> <div class="author"> Peng Sida, Zhang Yuanqing, Xu Yinghao, Wang Qianqian, <em>Shuai Qing</em>, Bao Hujun, and Zhou Xiaowei</div> <div class="periodical"> <em>In CVPR</em> Aug 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://zju3dv.github.io/neuralbody/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>xxx</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">peng2021neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">cover</span> <span class="p">=</span> <span class="s">{neural_body.png}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/mirror.mp4" type="video/mp4"></source></video></div> <div id="fang2021mirrored" class="col-sm-8"> <div class="title">Reconstructing 3D Human Pose by Watching Humans in the Mirror</div> <div class="author"> Fang Qi, <em>Shuai* Qing</em>, Dong Junting, Bao Hujun, and Zhou Xiaowei</div> <div class="periodical"> <em>In CVPR</em> Aug 2021 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fang2021mirrored</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reconstructing 3D Human Pose by Watching Humans in the Mirror}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qi, Fang and Qing, Shuai* and Junting, Dong and Hujun, Bao and Xiaowei, Zhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" onmouseover="this.play()" onmouseout="this.pause();" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/animatable_nerf.mp4" type="video/mp4"></source></video></div> <div id="peng2021animatable" class="col-sm-8"> <div class="title">Animatable neural radiance fields for modeling dynamic human bodies</div> <div class="author"> Peng Sida, Dong Junting, Wang Qianqian, Zhang Shangzhan, <em>Shuai Qing</em>, Zhou Xiaowei, and Bao Hujun</div> <div class="periodical"> <em>In ICCV</em> Aug 2021 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">peng2021animatable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Animatable neural radiance fields for modeling dynamic human bodies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sida, Peng and Junting, Dong and Qianqian, Wang and Shangzhan, Zhang and Qing, Shuai and Xiaowei, Zhou and Hujun, Bao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{14314--14323}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"><video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""><source src="https://xzhou.me/videos/imocap.mp4" type="video/mp4"></source></video></div> <div id="junting2020" class="col-sm-8"> <div class="title">Motion Capture from Internet Videos</div> <div class="author"> Dong* Junting, <em>Shuai* Qing</em>, Zhang Yuanqing, Liu Xian, Zhou Xiaowei, and Bao Hujun</div> <div class="periodical"> <em>In ECCV</em> Aug 2020 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://zju3dv.github.io/iMoCap/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">junting2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Motion Capture from Internet Videos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">cover</span> <span class="p">=</span> <span class="s">{mocap.png}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73_%71@%7A%6A%75.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=51lY6vIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/chingswy" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Qing Shuai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>